id;source_file;version_tag;snippet
0;README.md;beta;"# Aligned

Aligned makes it easier to reduce visibility debt, and data depencency debts, as described in [Sculley et al. [2015]](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf).

Want to look at examples of how to use `aligned`? 
View the [`MatsMoll/aligned-example` repo](https://github.com/MatsMoll/aligned-example).

This is done by providing an new innovative way of describing feature transformations, and data flow in ML systems. While also collecting dependency metadata that would otherwise be too inconvenient and error prone to manually type out.

Therefore, you get the following:
- [Feature Store](https://matsmoll.github.io/posts/understanding-the-chaotic-landscape-of-mlops#feature-store)
- [Feature Server](#feature-server)
- [Stream Processing](#stream-worker)
- Model Performance Monitoring - Documentation coming soon
- Data Catalog - Documentation coming soon
- Data Lineage - Documentation coming soon
- [Data Quality Assurance](#data-quality)
- [Easy Data Loading](#access-data)
- [Load Form Multiple Sources](#fast-development)


All from the simple API of defining
- [Data Sources](#data-sources)
- [Feature Views](#feature-views)
- [Models](#describe-models)

As a result, loading model features is as easy as:

```python
entities = {""passenger_id"": [1, 2, 3, 4]}
await store.model(""titanic"").features_for(entities).as_pandas()
```

Aligned is still in active development, so changes are likely.
"
1;README.md;beta;"# Feature Views

Write features as the should be, as data models.
Then get code completion and typesafety by referencing them in other features.

This makes the features light weight, data source indipendent, and flexible.

```python
class TitanicPassenger(FeatureView):

    metadata = FeatureView.metadata_with(
        name=""passenger"",
        description=""Some features from the titanic dataset"",
        batch_source=FileSource.csv_at(""titanic.csv""),
        stream_source=HttpStreamSource(topic_name=""titanic"")
    )

    passenger_id = Int32().as_entity()

    # Input values
    age = (
        Float()
            .description(""A float as some have decimals"")
            .is_required()
            .lower_bound(0)
            .upper_bound(110)
    )

    name = String()
    sex = String().accepted_values([""male"", ""female""])
    survived = Bool().description(""If the passenger survived"")
    sibsp = Int32().lower_bound(0, is_inclusive=True).description(""Number of siblings on titanic"")
    cabin = String()

    # Creates two one hot encoded values
    is_male, is_female = sex.one_hot_encode(['male', 'female'])
```
"
2;README.md;beta;"# Data sources

Alinged makes handling data sources easy, as you do not have to think about how it is done.
Only define where the data is, and we handle the dirty work.

```python
my_db = PostgreSQLConfig(env_var=""DATABASE_URL"")
redis = RedisConfig(env_var=""REDIS_URL"")

class TitanicPassenger(FeatureView):

    metadata = FeatureView.metadata_with(
        name=""passenger"",
        description=""Some features from the titanic dataset"",
        batch_source=my_db.table(
            ""passenger"",
            mapping_keys={
                ""Passenger_Id"": ""passenger_id""
            }
        ),
        stream_source=redis.stream(topic=""titanic"")
    )

    passenger_id = Int32().as_entity()
```
"
3;README.md;beta;"## Fast development

Making iterativ and fast exploration in ML is important. This is why Aligned also makes it super easy to combine, and test multiple sources.

```python
my_db = PostgreSQLConfig.localhost()

aws_bucket = AwsS3Config(...)

class SomeFeatures(FeatureView):

    metadata = FeatureViewMetadata(
        name=""some_features"",
        description=""..."",
        batch_source=my_db.table(""local_features"")
    )

    # Some features
    ...

class AwsFeatures(FeatureView):

    metadata = FeatureViewMetadata(
        name=""aws"",
        description=""..."",
        batch_source=aws_bucket.file_at(""path/to/file.parquet"")
    )

    # Some features
    ...
```
"
4;README.md;beta;"# Describe Models

Usually will you need to combine multiple features for each model.
This is where a `Model` comes in.
Here can you define which features should be exposed.

```python
class Titanic(Model):

    passenger = TitanicPassenger()
    location = LocationFeatures()

    metadata = Model.metadata_with(
        name=""titanic"",
        features=[
            passenger.constant_filled_age,
            passenger.ordinal_sex,
            passenger.sibsp,

            location.distance_to_shore,
            location.distance_to_closest_boat
        ]
    )

    # Referencing the passenger's survived feature as the target
    did_survive = passenger.survived.as_classification_target()
```

"
5;README.md;beta;"# Data Enrichers

In manny cases will extra data be needed in order to generate some features.
We therefore need some way of enriching the data.
This can easily be done with Alinged's `DataEnricher`s.

```python
my_db = PostgreSQLConfig.localhost()
redis = RedisConfig.localhost()

user_location = my_db.data_enricher( # Fetch all user locations
    sql=""SELECT * FROM user_location""
).cache( # Cache them for one day
    ttl=timedelta(days=1),
    cache_key=""user_location_cache""
).lock( # Make sure only one processer fetches the data at a time
    lock_name=""user_location_lock"",
    redis_config=redis
)


async def distance_to_users(df: DataFrame) -> Series:
    user_location_df = await user_location.load()
    ...
    return distances

class SomeFeatures(FeatureView):

    metadata = FeatureViewMetadata(...)

    latitude = Float()
    longitude = Float()

    distance_to_users = Float().transformed_using_features_pandas(
        [latitude, longitude],
        distance_to_users
    )
```

"
6;README.md;beta;"# Access Data

You can easily create a feature store that contains all your feature definitions.
This can then be used to genreate data sets, setup an instce to serve features, DAG's etc.

```python
store = await FileSource.json_at(""./feature-store.json"").feature_store()
"
7;README.md;beta;" Select all features from a single feature view
df = await store.all_for(""passenger"", limit=100).to_pandas()
```
"
8;README.md;beta;"## Centraliced Feature Store Definition
You would often share the features with other coworkers, or split them into different stages, like `staging`, `shadow`, or `production`.
One option is therefore to reference the storage you use, and load the `FeatureStore` from there.

```python
aws_bucket = AwsS3Config(...)
store = await aws_bucket.json_at(""production.json"").feature_store()
"
9;README.md;beta; This switches from the production online store to the offline store
10;README.md;beta;" Aka. the batch sources defined on the feature views
experimental_store = store.offline_store()
```
This json file can be generated by running `aligned apply`.
"
11;README.md;beta;"## Select multiple feature views

```python
df = await store.features_for({
    ""passenger_id"": [1, 50, 110]
}, features=[
    ""passenger:scaled_age"",
    ""passenger:is_male"",
    ""passenger:sibsp""

    ""other_features:distance_to_closest_boat"",
]).to_polars()
```
"
12;README.md;beta;"## Model Service

Selecting features for a model is super simple.


```python
df = await store.model(""titanic_model"").features_for({
    ""passenger_id"": [1, 50, 110]
}).to_pandas()
```
"
13;README.md;beta;"## Feature View

If you want to only select features for a specific feature view, then this is also possible.

```python
prev_30_days = await store.feature_view(""match"").previous(days=30).to_pandas()
sample_of_20 = await store.feature_view(""match"").all(limit=20).to_pandas()
```
"
14;README.md;beta;"# Data quality
Alinged will make sure all the different features gets formatted as the correct datatype.
In addition will aligned also make sure that the returend features aligne with defined constraints.

```python
class TitanicPassenger(FeatureView):

    ...

    age = (
        Float()
            .is_required()
            .lower_bound(0)
            .upper_bound(110)
    )
    sibsp = Int32().lower_bound(0, is_inclusive=True)
```

Then since our feature view have a `is_required` and a `lower_bound`, will the `.validate(...)` command filter out the entites that do not follow that behavior.

```python
from aligned.validation.pandera import PanderaValidator

df = await store.model(""titanic_model"").features_for({
    ""passenger_id"": [1, 50, 110]
}).validate(
    PanderaValidator()  # Validates all features
).to_pandas()
```
"
15;README.md;beta;"# Feature Server

You can define how to serve your features with the `FeatureServer`. Here can you define where you want to load, and potentially write your features to.

By default will it `aligned` look for a file called `server.py`, and a `FeatureServer` object called `server`. However, this can be defined manually as well.

```python
from aligned import RedisConfig, FileSource
from aligned.schemas.repo_definition import FeatureServer

store = FileSource.json_at(""feature-store.json"")

server = FeatureServer.from_reference(
    store,
    RedisConfig.localhost()
)
```

Then run `aligned serve`, and a FastAPI server will start. Here can you push new features, which then transforms and stores the features, or just fetch them.
"
16;README.md;beta;"# Stream Worker

You can also setup stream processing with a similar structure. However, here will a `StreamWorker` be used.

by default will `aligned` look for a `worker.py` file with an object called `worker`. An example would be the following.

```python
from aligned import RedisConfig, FileSource
from aligned.schemas.repo_definition import FeatureServer

store = FileSource.json_at(""feature-store.json"")

server = FeatureServer.from_reference(
    store,
    RedisConfig.localhost()
)
```
"
17;aligned/active_learning/job.py;beta;"class ActiveLearningJob(RetrivalJob):

    job: RetrivalJob
    model: Model
    metric: ActiveLearningMetric
    selection: ActiveLearningSelection
    write_policy: ActiveLearningWritePolicy

    async def to_polars(self) -> pl.LazyFrame:
        if not self.model.predictions_view.classification_targets:
            logger.info('Found no target. Therefore, no data will be written to an active learning dataset.')
            return await self.job.to_polars()

        data = await self.job.to_polars()
        active_learning_set = self.selection.select(self.model, data, self.metric)
        await self.write_policy.write(active_learning_set, self.model)
        return data

    async def to_pandas(self) -> pd.DataFrame:
        raise NotImplementedError()"
18;aligned/active_learning/selection.py;beta;"class ActiveLearningMetric:
    def metric(self, model: Model) -> pl.Expr:
        raise NotImplementedError()

    @staticmethod
    def max_probability() -> ActiveLearningMetric:
        return ActivLearningPolarsExprMetric(
            lambda model: pl.concat_list(
                [prob.feature.name for prob in model.predictions_view.probabilities]
            ).arr.max()
        )"
19;aligned/active_learning/selection.py;beta;"class ActivLearningPolarsExprMetric(ActiveLearningMetric):

    factory: Callable[[Model], pl.Expr]

    def metric(self, model: Model) -> pl.Expr:
        return self.factory(model)"
20;aligned/active_learning/selection.py;beta;"class ActiveLearningSelection:
    def select(self, model: Model, data: pl.LazyFrame, metric: ActiveLearningMetric) -> pl.LazyFrame:
        raise NotImplementedError()

    @staticmethod
    def under_threshold(threshold: float) -> ActiveLearningSelection:
        return ActivLearningPolarsExprSelection(lambda model, metric: metric.metric(model) < threshold)"
21;aligned/active_learning/selection.py;beta;"class ActivLearningPolarsExprSelection(ActiveLearningSelection):

    factory: Callable[[Model, ActiveLearningMetric], pl.Expr]

    def select(self, model: Model, data: pl.LazyFrame, metric: ActiveLearningMetric) -> pl.LazyFrame:
        return data.filter(self.factory(model, metric))"
22;aligned/active_learning/write_policy.py;beta;"class ActiveLearningWritePolicy:
    async def write(self, data: pl.LazyFrame, model: Model):
        raise NotImplementedError()

    @staticmethod
    def sample_size(write_size: int, ideal_size: int) -> ActiveLearningWritePolicy:
        return ActiveLearningSampleSizePolicy(write_size, ideal_size)"
23;aligned/active_learning/write_policy.py;beta;"class ActiveLearningSampleSizePolicy(ActiveLearningWritePolicy):

    write_size: int
    ideal_size: int

    dataset_folder_name: str = field(default='active_learning')
    dataset_file_name: str = field(default='data.csv')

    unsaved_size: float = field(default=0)
    write_timestamp: float = field(default_factory=lambda: datetime.utcnow().timestamp())
    current_frame: pl.DataFrame = field(default_factory=lambda: pl.DataFrame())

    async def write(self, data: pl.LazyFrame, model: Model):

        if not model.dataset_folder:
            logger.info(
                'Found no dataset folder. Therefore, no data will be written to an active learning dataset.'
            )
            return

        collected_data = data.collect()

        if self.current_frame.shape[0] == 0:
            self.current_frame = collected_data
        else:
            self.current_frame = self.current_frame.extend(collected_data)

        self.unsaved_size += collected_data.shape[0]

        if self.unsaved_size >= self.write_size or self.current_frame.shape[0] >= self.ideal_size:
            dataset_subfolder = Path(self.dataset_folder_name) / str(self.write_timestamp)
            logger.info(f'Writing active learning data to {dataset_subfolder}')

            dataset = model.dataset_folder.file_at(dataset_subfolder / self.dataset_file_name)
            await dataset.write(self.current_frame.write_csv().encode('utf-8'))
            self.unsaved_size = 0

        if self.current_frame.shape[0] >= self.ideal_size:
            self.write_timestamp = datetime.utcnow().timestamp()
            self.current_frame = pl.DataFrame()"
24;aligned/cli.py;beta;"class CategoricalFeatureSummary(Codable):
    missing_percentage: float
    unique_values: int
    values: list[str]
    value_count: list[int]"
25;aligned/cli.py;beta;"class NumericFeatureSummary(Codable):
    missing_percentage: float
    mean: float | None
    median: float | None
    std: float | None
    lowest: float | None
    highests: float | None
    histogram_count: list[int]
    histogram_splits: list[float]"
26;aligned/cli.py;beta;"class ProfilingResult(Codable):
    numeric_features: dict[str, NumericFeatureSummary]
    categorical_features: dict[str, CategoricalFeatureSummary]


# Should add some way of profiling models, not feature views.
# Or maybe both
@cli.command('profile')
@coro
@click.option(
    '--repo-path',
    default='.',
    help='The path to the repo',
)
@click.option(
    '--reference-file',
    default='feature_store_location.py',
    help='The file defining where to read the feature store from',
)
@click.option('--output', default='profiling-result.json')
@click.option('--dataset-size', default=10000)
@click.option(
    '--env-file',
    default='.env',
    help='The path to env variables',
)
async def profile(repo_path: str, reference_file: str, env_file: str, output: str, dataset_size: int) -> None:
    import numpy as np
    from pandas import DataFrame

    from aligned import FeatureStore

    # Make sure modules can be read, and that the env is set
    dir = Path.cwd() if repo_path == '.' else Path(repo_path).absolute()
    sys.path.append(str(dir))
    env_file_path = dir / env_file
    load_envs(env_file_path)

    online_store: FeatureStore = await FeatureStore.from_reference_at_path(repo_path, reference_file)
    feature_store = online_store.offline_store()

    results = ProfilingResult(numeric_features={}, categorical_features={})

    for feature_view_name in sorted(feature_store.feature_views.keys()):
        click.echo(f'Profiling: {feature_view_name}')
        feature_view = feature_store.feature_view(feature_view_name)
        data_set: DataFrame = feature_view.all(limit=dataset_size).to_pandas()

        all_features: list[Feature] = list(feature_view.view.features) + list(
            feature_view.view.derived_features
        )
        for feature in all_features:

            data_slice = data_set[feature.name]

            reference = f'{feature_view_name}:{feature.name}'

            if (not feature.dtype.is_numeric) or feature.dtype.name == 'bool':
                unique_values = data_slice.unique()
                filter_unique_nan_values = [
                    value
                    for value in unique_values
                    if not (
                        str(value).lower() == 'nan' or str(value).lower() == 'nat' or str(value) == '<NA>'
                    )
                ]

                results.categorical_features[reference] = CategoricalFeatureSummary(
                    missing_percentage=(data_slice.isna() | data_slice.isnull()).sum() / data_slice.shape[0],
                    unique_values=unique_values.shape[0],
                    values=[str(value) for value in filter_unique_nan_values],
                    value_count=data_slice.value_counts()[filter_unique_nan_values].tolist(),
                )
            else:
                description = data_slice.describe()
                n_bins = np.min([50, len(data_slice.unique())])
                max_value = description['max']
                min_value = description['min']

                if np.isnan(max_value):
                    continue

                width = (max_value - min_value) / n_bins

                if width <= 0:
                    histogram = [description['count']]
                    cuts = []
                else:
                    cuts = np.arange(start=min_value, stop=max_value + width, step=width)
                    histogram, _ = np.histogram(data_slice.loc[~data_slice.isna()].values, cuts)

                results.numeric_features[reference] = NumericFeatureSummary(
                    missing_percentage=(data_slice.isna() | data_slice.isnull()).sum() / data_slice.shape[0],
                    mean=description['mean'] if not np.isnan(description['mean']) else None,
                    median=description['50%'] if not np.isnan(description['50%']) else None,
                    std=description['std'] if not np.isnan(description['std']) else None,
                    lowest=description['min'] if not np.isnan(description['min']) else None,
                    highests=description['max'] if not np.isnan(description['max']) else None,
                    histogram_count=list(histogram),
                    histogram_splits=list(cuts),
                )

    Path(output).write_bytes(results.to_json().encode('utf-8'))


@cli.command('create-indexes')
@coro
@click.option(
    '--repo-path',
    default='.',
    help='The path to the repo',
)
@click.option(
    '--reference-file',
    default='feature_store_location.py:source',
    help='The path to a feature store reference file. Defining where to read and write the feature store',
)
@click.option(
    '--env-file',
    default='.env',
    help='The path to env variables',
)
async def create_indexes(repo_path: str, reference_file: str, env_file: str) -> None:
    from aligned import FeatureStore, FileSource

    setup_logger()

    # Make sure modules can be read, and that the env is set
    path, obj = reference_file.split(':')
    dir = Path.cwd() if repo_path == '.' else Path(repo_path).absolute()
    reference_file_path = Path(path)

    sys.path.append(str(dir))
    env_file_path = dir / env_file
    load_envs(env_file_path)

    repo_ref = RepoReference('const', {'const': FileSource.json_at('./feature-store.json')})
    with suppress(ValueError):
        repo_ref = RepoReference.reference_object(dir, reference_file_path, obj)

    if file := repo_ref.selected_file:
        click.echo(f'Updating file at: {file}')

        repo_def = await RepoReader.definition_from_path(dir)
    else:
        click.echo(f'No repo file found at {dir}. Returning without creating indexes')
        return

    feature_store = FeatureStore.from_definition(repo_def)

    for feature_view_name in sorted(feature_store.feature_views.keys()):
        view = feature_store.feature_views[feature_view_name]
        if view.indexes is None:
            continue

        for index in view.indexes:
            click.echo(f'Creating indexes for: {feature_view_name}')
            await index.storage.create_index(index)


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, format='{asctime} {message}', style='{')
    cli()"
27;aligned/compiler/aggregation_factory.py;beta;"class ConcatStringsAggrigationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: String
    group_by: list[FeatureReferance]
    separator: str | None = None
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import ConcatStringAggregation

        return ConcatStringAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
            separator=self.separator,
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
28;aligned/compiler/aggregation_factory.py;beta;"class SumAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import SumAggregation

        return SumAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
29;aligned/compiler/aggregation_factory.py;beta;"class MeanAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import MeanAggregation

        return MeanAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
30;aligned/compiler/aggregation_factory.py;beta;"class MinAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import MinAggregation

        return MinAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
31;aligned/compiler/aggregation_factory.py;beta;"class MaxAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import MaxAggregation

        return MaxAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
32;aligned/compiler/aggregation_factory.py;beta;"class CountAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import CountAggregation

        return CountAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
33;aligned/compiler/aggregation_factory.py;beta;"class CountDistinctAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import CountDistinctAggregation

        return CountDistinctAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
34;aligned/compiler/aggregation_factory.py;beta;"class StdAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import StdAggregation

        return StdAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
35;aligned/compiler/aggregation_factory.py;beta;"class VarianceAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import VarianceAggregation

        return VarianceAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
36;aligned/compiler/aggregation_factory.py;beta;"class MedianAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import MedianAggregation

        return MedianAggregation(
            key=self.feature.feature_referance().name,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
37;aligned/compiler/aggregation_factory.py;beta;"class PercentileAggregationFactory(TransformationFactory, AggregationTransformationFactory):

    feature: FeatureFactory
    percentile: float
    group_by: list[FeatureReferance]
    time_window: timedelta | None = None

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import PercentileAggregation

        return PercentileAggregation(
            key=self.feature.feature_referance().name,
            percentile=self.percentile,
            group_keys=[feature.name for feature in self.group_by],
        )

    def with_group_by(self, values: list[FeatureReferance]) -> TransformationFactory:
        self.group_by = values
        return self"
38;aligned/compiler/constraint_factory.py;beta;"class ConstraintFactory:
    async def compile(self, view: CompiledFeatureView) -> Constraint:
        pass"
39;aligned/compiler/constraint_factory.py;beta;"class LiteralFactory(ConstraintFactory):

    constraint: Constraint

    async def compile(self, view: CompiledFeatureView) -> Constraint:
        return self.constraint"
40;aligned/compiler/feature_factory.py;beta;"class TransformationFactory:
    """"""
    A"
41;aligned/compiler/feature_factory.py;beta;"class that can compute the transformation logic.

    For most classes will there be no need for a factory,
    However for more advanced transformations will this be needed.

    e.g:

    StandaredScalerFactory(
        time_window=timedelta(days=30)
    )

    The batch data source will be provided when the compile method is run.
    leading to fetching a small sample, and compute the metrics needed in order to generate a
    """"""

    def compile(self) -> Transformation:
        pass

    @property
    def using_features(self) -> list[FeatureFactory]:
        pass"
42;aligned/compiler/feature_factory.py;beta;"class AggregationTransformationFactory:
    @property
    def time_window(self) -> timedelta | None:
        pass

    def with_group_by(self, entities: list[FeatureReferance]) -> TransformationFactory:
        pass


T = TypeVar('T')"
43;aligned/compiler/feature_factory.py;beta;"class EventTrigger:
    condition: FeatureFactory
    event: StreamDataSource"
44;aligned/compiler/feature_factory.py;beta;"class TargetProbability:
    of_value: Any
    target: ClassificationTarget
    _name: str | None = None

    def __hash__(self) -> int:
        return self._name.__hash__()

    def __set_name__(self, owner, name):
        self._name = name

    def compile(self) -> ClassTargetProbability:
        return ClassTargetProbability(
            outcome=LiteralValue.from_value(self.of_value),
            feature=Feature(self._name, dtype=FeatureType('').float),
        )"
45;aligned/compiler/feature_factory.py;beta;"class FeatureReferencable:
    def feature_referance(self) -> FeatureReferance:
        pass"
46;aligned/compiler/feature_factory.py;beta;"class RegressionTarget(FeatureReferencable):
    feature: FeatureFactory
    event_trigger: EventTrigger | None = field(default=None)
    ground_truth_event: StreamDataSource | None = field(default=None)
    _name: str | None = field(default=None)
    _location: FeatureLocation | None = field(default=None)

    def __set_name__(self, owner, name):
        self._name = name

    def feature_referance(self) -> FeatureReferance:
        if not self._name:
            raise ValueError('Missing name, can not create reference')
        if not self._location:
            raise ValueError('Missing location, can not create reference')
        return FeatureReferance(self._name, self._location, self.feature.dtype)

    def listen_to_ground_truth_event(self, stream: StreamDataSource) -> RegressionTarget:
        return RegressionTarget(
            feature=self.feature,
            event_trigger=self.event_trigger,
            ground_truth_event=stream,
        )

    def send_ground_truth_event(self, when: Bool, sink_to: StreamDataSource) -> RegressionTarget:
        assert when.dtype == FeatureType('').bool, 'A trigger needs a boolean condition'

        return RegressionTarget(
            self.feature, EventTrigger(when, sink_to), ground_truth_event=self.ground_truth_event
        )

    def compile(self) -> RegressionTargetSchemas:
        on_ground_truth_event = self.ground_truth_event
        trigger = self.event_trigger

        if self.event_trigger:
            event = self.event_trigger
            if not event.condition._name:
                event.condition._name = '0'

            trigger = EventTriggerSchema(
                event.condition.compile(), event=event.event, payload={self.feature.feature()}
            )
            if not on_ground_truth_event:
                on_ground_truth_event = event.event

        return RegressionTargetSchemas(
            self.feature.feature_referance(),
            feature=Feature(self._name, self.feature.dtype),
            on_ground_truth_event=on_ground_truth_event,
            event_trigger=trigger,
        )"
47;aligned/compiler/feature_factory.py;beta;"class ClassificationTarget(FeatureReferencable):
    feature: FeatureFactory
    event_trigger: EventTrigger | None = field(default=None)
    ground_truth_event: StreamDataSource | None = field(default=None)
    _name: str | None = field(default=None)
    _location: FeatureLocation | None = field(default=None)

    def __set_name__(self, owner, name):
        self._name = name

    def feature_referance(self) -> FeatureReferance:
        if not self._name:
            raise ValueError('Missing name, can not create reference')
        if not self._location:
            raise ValueError('Missing location, can not create reference')
        return FeatureReferance(self._name, self._location, self.feature.dtype)

    def listen_to_ground_truth_event(self, stream: StreamDataSource) -> ClassificationTarget:
        return ClassificationTarget(
            feature=self.feature,
            event_trigger=self.event_trigger,
            ground_truth_event=stream,
        )

    def send_ground_truth_event(self, when: Bool, sink_to: StreamDataSource) -> ClassificationTarget:
        assert when.dtype == FeatureType('').bool, 'A trigger needs a boolean condition'

        return ClassificationTarget(self.feature, EventTrigger(when, sink_to))

    def probability_of(self, value: Any) -> TargetProbability:
        """"""Define a value that will be the probability of a certain target class.

        This is mainly intended to be used for classification problems with low cardinality.

        For example, if the target is a binary classification, then the probability of the target.
        being 1 can be defined by:

        >>> target.probability_of(1)

        For cases where the target have a high cardinality can `probabilities` be used instead.

        Args:
            value (Any): The"
48;aligned/compiler/feature_factory.py;beta;"class that will be the probability of the target.

        Returns:
            TargetProbability: A feature that contains the probability of the target class.
        """"""

        if not isinstance(value, self.feature.dtype.python_type):
            raise ValueError(
                (
                    'Probability of target is of incorrect data type. ',
                    f'Target is {self.feature.dtype}, but value is {type(value)}.',
                )
            )

        return TargetProbability(value, self)

    def compile(self) -> ClassificationTargetSchemas:
        on_ground_truth_event = self.ground_truth_event
        trigger = self.event_trigger

        if self.event_trigger:
            event = self.event_trigger
            if not event.condition._name:
                event.condition._name = '0'

            trigger = EventTriggerSchema(
                event.condition.compile(), event=event.event, payload={self.feature.feature()}
            )
            if not on_ground_truth_event:
                on_ground_truth_event = event.event

        return ClassificationTargetSchemas(
            self.feature.feature_referance(),
            feature=Feature(self._name, self.feature.dtype),
            on_ground_truth_event=on_ground_truth_event,
            event_trigger=trigger,
        )"
49;aligned/compiler/feature_factory.py;beta;"class FeatureFactory(FeatureReferencable):
    """"""
    Represents the information needed to generate a feature definition

    This may contain lazely loaded information, such as then name.
    Which will be added when the feature view is compiled, as we can get the attribute name at runtime.

    The feature can still have no name, but this means that it is an unstored feature.
    It will threfore need a transformation field.

    The feature_dependencies is the features graph for the given feature.

    aka
                            x <- standard scaler <- age: Float
    x_and_y_is_equal <-
                            y: Float
    """"""

    _name: str | None = None
    _location: FeatureLocation | None = None
    _description: str | None = None

    transformation: TransformationFactory | None = None
    constraints: set[ConstraintFactory] | None = None

    def __set_name__(self, owner, name):
        self._name = name

    @property
    def dtype(self) -> FeatureType:
        raise NotImplementedError()

    @property
    def name(self) -> str:
        if not self._name:
            raise ValueError('Have not been given a name yet')
        return self._name

    @property
    def depending_on_names(self) -> list[str]:
        if not self.transformation:
            return []
        return [feat._name for feat in self.transformation.using_features if feat._name]

    def feature_referance(self) -> FeatureReferance:
        return FeatureReferance(self.name, self._location, self.dtype)

    def feature(self) -> Feature:
        return Feature(
            name=self.name,
            dtype=self.dtype,
            description=self._description,
            tags=None,
            constraints=self.constraints,
        )

    def as_classification_target(self) -> ClassificationTarget:
        return ClassificationTarget(self)

    def as_regression_target(self) -> RegressionTarget:
        return RegressionTarget(self)

    def compile(self) -> DerivedFeature:

        if not self.transformation:
            raise ValueError(f'Trying to create a derived feature with no transformation, {self.name}')

        return DerivedFeature(
            name=self.name,
            dtype=self.dtype,
            depending_on=[feat.feature_referance() for feat in self.transformation.using_features],
            transformation=self.transformation.compile(),
            depth=self.depth(),
            description=self._description,
            tags=None,
            constraints=self.constraints,
        )

    def depth(self) -> int:
        value = 0
        if not self.transformation:
            return value
        for feature in self.transformation.using_features:
            value = max(feature.depth(), value)
        return value + 1

    def description(self: T, description: str) -> T:
        self._description = description  # type: ignore [attr-defined]
        return self

    def feature_dependencies(self) -> list[FeatureFactory]:
        values = []

        if not self.transformation:
            return []

        def add_values(feature: FeatureFactory) -> None:
            values.append(feature)
            if not feature.transformation:
                return
            for sub_feature in feature.transformation.using_features:
                add_values(sub_feature)

        for sub_feature in self.transformation.using_features:
            add_values(sub_feature)

        return values

    def copy_type(self: T) -> T:
        raise NotImplementedError()

    def fill_na(self: T, value: FillNaStrategy | Any) -> T:

        from aligned.compiler.transformation_factory import (
            ConstantFillNaStrategy,
            FillMissingFactory,
            FillNaStrategy,
        )

        instance: FeatureFactory = self.copy_type()  # type: ignore [attr-defined]
        if isinstance(value, FillNaStrategy):
            instance.transformation = FillMissingFactory(self, value)
        else:
            instance.transformation = FillMissingFactory(self, ConstantFillNaStrategy(value))
        return instance  # type: ignore [return-value]

    def transformed_using_features_pandas(
        self: T, using_features: list[FeatureFactory], transformation: Callable[[pd.DataFrame, pd.Series]]
    ) -> T:
        from aligned.compiler.transformation_factory import PandasTransformationFactory

        dtype: FeatureFactory = self.copy_type()  # type: ignore [assignment]

        dtype.transformation = PandasTransformationFactory(dtype, transformation, using_features or [self])
        return dtype  # type: ignore [return-value]

    def transform_pandas(self, transformation: Callable[[pd.DataFrame], pd.Series], as_dtype: T) -> T:
        from aligned.compiler.transformation_factory import PandasTransformationFactory

        dtype: FeatureFactory = as_dtype  # type: ignore [assignment]

        dtype.transformation = PandasTransformationFactory(dtype, transformation, [self])
        return dtype  # type: ignore [return-value]

    def transformed_using_features_polars(
        self: T,
        using_features: list[FeatureFactory],
        transformation: Callable[[pl.LazyFrame, str], pl.LazyFrame],
    ) -> T:
        from aligned.compiler.transformation_factory import PolarsTransformationFactory

        dtype: FeatureFactory = self.copy_type()  # type: ignore [assignment]
        dtype.transformation = PolarsTransformationFactory(dtype, transformation, using_features or [self])
        return dtype  # type: ignore [return-value]

    def transform_polars(
        self,
        expression: pl.Expr,
        using_features: list[FeatureFactory] | None = None,
        as_dtype: T | None = None,
    ) -> T:
        from aligned.compiler.transformation_factory import PolarsTransformationFactory

        dtype: FeatureFactory = as_dtype or self.copy_type()  # type: ignore [assignment]
        dtype.transformation = PolarsTransformationFactory(dtype, expression, using_features or [self])
        return dtype  # type: ignore [return-value]

    def is_required(self: T) -> T:
        from aligned.schemas.constraints import Required

        self._add_constraint(Required())  # type: ignore[attr-defined]
        return self

    def _add_constraint(self, constraint: ConstraintFactory | Constraint) -> None:
        # The constraint should be a lazy evaluated constraint
        # Aka, a factory, as with the features.
        # Therefore making it possible to add distribution checks
        if not self.constraints:
            self.constraints = set()
        if isinstance(constraint, Constraint):
            self.constraints.add(constraint)
        else:
            self.constraints.add(LiteralFactory(constraint))

    def is_not_null(self) -> Bool:
        from aligned.compiler.transformation_factory import NotNullFactory

        instance = Bool()
        instance.transformation = NotNullFactory(self)
        return instance"
50;aligned/compiler/feature_factory.py;beta;"class CouldBeEntityFeature:
    def as_entity(self) -> Entity:
        if isinstance(self, FeatureFactory):
            return Entity(self)

        raise ValueError(f'{self} is not a feature factory, and can therefore not be an entity')"
51;aligned/compiler/feature_factory.py;beta;"class EquatableFeature(FeatureFactory):

    # Comparable operators
    def __eq__(self, right: FeatureFactory | Any) -> Bool:  # type: ignore[override]
        from aligned.compiler.transformation_factory import EqualsFactory

        instance = Bool()
        instance.transformation = EqualsFactory(self, right)
        return instance

    def equals(self, right: object) -> Bool:
        return self == right

    def __ne__(self, right: FeatureFactory | Any) -> Bool:  # type: ignore[override]
        from aligned.compiler.transformation_factory import NotEqualsFactory

        instance = Bool()
        instance.transformation = NotEqualsFactory(right, self)
        return instance

    def not_equals(self, right: object) -> Bool:
        return self != right

    def is_in(self, values: list[Any]) -> Bool:
        from aligned.compiler.transformation_factory import IsInFactory

        instance = Bool()
        instance.transformation = IsInFactory(self, values)
        return instance"
52;aligned/compiler/feature_factory.py;beta;"class ComparableFeature(EquatableFeature):
    def __lt__(self, right: object) -> Bool:
        from aligned.compiler.transformation_factory import LowerThenFactory

        instance = Bool()
        instance.transformation = LowerThenFactory(right, self)
        return instance

    def __le__(self, right: float) -> Bool:
        from aligned.compiler.transformation_factory import LowerThenOrEqualFactory

        instance = Bool()
        instance.transformation = LowerThenOrEqualFactory(right, self)
        return instance

    def __gt__(self, right: object) -> Bool:
        from aligned.compiler.transformation_factory import GreaterThenFactory

        instance = Bool()
        instance.transformation = GreaterThenFactory(self, right)
        return instance

    def __ge__(self, right: object) -> Bool:
        from aligned.compiler.transformation_factory import GreaterThenOrEqualFactory

        instance = Bool()
        instance.transformation = GreaterThenOrEqualFactory(right, self)
        return instance

    def lower_bound(self: T, value: float, is_inclusive: bool | None = None) -> T:

        if is_inclusive:
            self._add_constraint(LowerBoundInclusive(value))  # type: ignore[attr-defined]
        else:
            self._add_constraint(LowerBound(value))  # type: ignore[attr-defined]
        return self

    def upper_bound(self: T, value: float, is_inclusive: bool | None = None) -> T:

        if is_inclusive:
            self._add_constraint(UpperBoundInclusive(value))  # type: ignore[attr-defined]
        else:
            self._add_constraint(UpperBound(value))  # type: ignore[attr-defined]
        return self"
53;aligned/compiler/feature_factory.py;beta;"class ArithmeticFeature(ComparableFeature):
    def __sub__(self, other: FeatureFactory) -> Float:
        from aligned.compiler.transformation_factory import DifferanceBetweenFactory, TimeDifferanceFactory

        feature = Float()
        if self.dtype == FeatureType('').datetime:
            feature.transformation = TimeDifferanceFactory(self, other)
        else:
            feature.transformation = DifferanceBetweenFactory(self, other)
        return feature

    def __add__(self, other: FeatureFactory) -> Float:
        from aligned.compiler.transformation_factory import AdditionBetweenFactory

        feature = Float()
        feature.transformation = AdditionBetweenFactory(self, other)
        return feature

    def __truediv__(self, other: FeatureFactory | Any) -> Float:
        from aligned.compiler.transformation_factory import RatioFactory

        feature = Float()
        if isinstance(other, FeatureFactory):
            feature.transformation = RatioFactory(self, other)
        else:
            feature.transformation = RatioFactory(self, LiteralValue.from_value(other))
        return feature

    def __floordiv__(self, other: FeatureFactory) -> Float:
        from aligned.compiler.transformation_factory import RatioFactory

        feature = Float()
        feature.transformation = RatioFactory(self, other)
        return feature

    def __abs__(self) -> Float:
        from aligned.compiler.transformation_factory import AbsoluteFactory

        feature = Float()
        feature.transformation = AbsoluteFactory(self)
        return feature

    def __mul__(self, other: FeatureFactory | Any) -> Float:
        from aligned.compiler.transformation_factory import MultiplyFactory

        feature = Float()
        if isinstance(other, FeatureFactory):
            feature.transformation = MultiplyFactory(self, other)
        else:
            feature.transformation = MultiplyFactory(self, LiteralValue.from_value(other))
        return feature

    def __rmul__(self, other: FeatureFactory | Any) -> Float:
        from aligned.compiler.transformation_factory import MultiplyFactory

        feature = Float()
        if isinstance(other, FeatureFactory):
            feature.transformation = MultiplyFactory(self, other)
        else:
            feature.transformation = MultiplyFactory(self, LiteralValue.from_value(other))
        return feature

    def __pow__(self, other: FeatureFactory | Any) -> Float:
        from aligned.compiler.transformation_factory import PowerFactory

        feature = Float()
        feature.transformation = PowerFactory(self, other)
        return feature

    def log1p(self) -> Float:
        from aligned.compiler.transformation_factory import LogTransformFactory

        feature = Float()
        feature.transformation = LogTransformFactory(self)
        return feature

    def clip(self: T, lower_bound: float, upper_bound: float) -> T:
        from aligned.compiler.transformation_factory import ClipFactory

        feature = Float()
        feature.transformation = ClipFactory(self, lower_bound, upper_bound)
        return feature"
54;aligned/compiler/feature_factory.py;beta;"class DecimalOperations(FeatureFactory):
    def __round__(self) -> Int64:
        from aligned.compiler.transformation_factory import RoundFactory

        feature = Int64()
        feature.transformation = RoundFactory(self)
        return feature

    def round(self) -> Int64:
        return self.__round__()

    def __ceil__(self) -> Int64:
        from aligned.compiler.transformation_factory import CeilFactory

        feature = Int64()
        feature.transformation = CeilFactory(self)
        return feature

    def cail(self) -> Int64:
        return self.__ceil__()

    def __floor__(self) -> Int64:
        from aligned.compiler.transformation_factory import FloorFactory

        feature = Int64()
        feature.transformation = FloorFactory(self)
        return feature

    def floor(self) -> Int64:
        return self.__floor__()"
55;aligned/compiler/feature_factory.py;beta;"class TruncatableFeature(FeatureFactory):
    def __trunc__(self: T) -> T:
        raise NotImplementedError()"
56;aligned/compiler/feature_factory.py;beta;"class NumberConvertableFeature(FeatureFactory):
    def as_float(self) -> Float:
        from aligned.compiler.transformation_factory import ToNumericalFactory

        feature = Float()
        feature.transformation = ToNumericalFactory(self)
        return feature

    def __int__(self) -> Int64:
        raise NotImplementedError()

    def __float__(self) -> Float:
        raise NotImplementedError()"
57;aligned/compiler/feature_factory.py;beta;"class InvertableFeature(FeatureFactory):
    def __invert__(self) -> Bool:
        from aligned.compiler.transformation_factory import InverseFactory

        feature = Bool()
        feature.transformation = InverseFactory(self)
        return feature"
58;aligned/compiler/feature_factory.py;beta;"class LogicalOperatableFeature(InvertableFeature):
    def __and__(self, other: Bool) -> Bool:
        from aligned.compiler.transformation_factory import AndFactory

        feature = Bool()
        feature.transformation = AndFactory(self, other)
        return feature

    def logical_and(self, other: Bool) -> Bool:
        return self & other

    def __or__(self, other: Bool) -> Bool:
        from aligned.compiler.transformation_factory import OrFactory

        feature = Bool()
        feature.transformation = OrFactory(self, other)
        return feature

    def logical_or(self, other: Bool) -> Bool:
        return self | other"
59;aligned/compiler/feature_factory.py;beta;"class CategoricalEncodableFeature(EquatableFeature):
    def one_hot_encode(self, labels: list[str]) -> list[Bool]:
        return [self == label for label in labels]

    def ordinal_categories(self, orders: list[str]) -> Int32:
        from aligned.compiler.transformation_factory import OrdinalFactory

        feature = Int32()
        feature.transformation = OrdinalFactory(orders, self)
        return feature

    def accepted_values(self: T, values: list[str]) -> T:
        self._add_constraint(InDomain(values))  # type: ignore[attr-defined]
        return self"
60;aligned/compiler/feature_factory.py;beta;"class DateFeature(FeatureFactory):
    def date_component(self, component: str) -> Int32:
        from aligned.compiler.transformation_factory import DateComponentFactory

        feature = Int32()
        feature.transformation = DateComponentFactory(component, self)
        return feature"
61;aligned/compiler/feature_factory.py;beta;"class Bool(EquatableFeature, LogicalOperatableFeature):
    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').bool

    def copy_type(self) -> Bool:
        return Bool()"
62;aligned/compiler/feature_factory.py;beta;"class Float(ArithmeticFeature, DecimalOperations):
    def copy_type(self) -> Float:
        return Float()

    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').float

    def aggregate(self) -> ArithmeticAggregation:
        return ArithmeticAggregation(self)"
63;aligned/compiler/feature_factory.py;beta;"class Int32(ArithmeticFeature, CouldBeEntityFeature):
    def copy_type(self) -> Int32:
        return Int32()

    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').int32

    def aggregate(self) -> ArithmeticAggregation:
        return ArithmeticAggregation(self)"
64;aligned/compiler/feature_factory.py;beta;"class Int64(ArithmeticFeature, CouldBeEntityFeature):
    def copy_type(self) -> Int64:
        return Int64()

    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').int64

    def aggregate(self) -> ArithmeticAggregation:
        return ArithmeticAggregation(self)"
65;aligned/compiler/feature_factory.py;beta;"class UUID(FeatureFactory, CouldBeEntityFeature):
    def copy_type(self) -> UUID:
        return UUID()

    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').uuid"
66;aligned/compiler/feature_factory.py;beta;"class LengthValidatable(FeatureFactory):
    def min_length(self: T, length: int) -> T:
        self._add_constraint(MinLength(length))
        return self

    def max_length(self: T, length: int) -> T:
        self._add_constraint(MaxLength(length))
        return self"
67;aligned/compiler/feature_factory.py;beta;"class StringValidatable(FeatureFactory):
    def validate_regex(self: T, regex: str) -> T:
        self._add_constraint(Regex(regex))
        return self

    def validate_endswith(self: T, suffix: str) -> T:
        self._add_constraint(EndsWith(suffix))
        return self

    def validate_startswith(self: T, prefix: str) -> T:
        self._add_constraint(StartsWith(prefix))
        return self"
68;aligned/compiler/feature_factory.py;beta;"class String(
    CategoricalEncodableFeature,
    NumberConvertableFeature,
    CouldBeEntityFeature,
    LengthValidatable,
    StringValidatable,
):
    def copy_type(self) -> String:
        return String()

    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').string

    def aggregate(self) -> StringAggregation:
        return StringAggregation(self)

    def split(self, pattern: str, max_splits: int | None = None) -> String:
        raise NotImplementedError()

    def replace(self, values: dict[str, str]) -> String:
        from aligned.compiler.transformation_factory import ReplaceFactory

        feature = String()
        feature.transformation = ReplaceFactory(values, self)
        return feature

    def contains(self, value: str) -> Bool:
        from aligned.compiler.transformation_factory import ContainsFactory

        feature = Bool()
        feature.transformation = ContainsFactory(value, self)
        return feature

    def sentence_vector(self, model: TextVectoriserModel) -> Embedding:
        from aligned.compiler.transformation_factory import WordVectoriserFactory

        feature = Embedding()
        feature.transformation = WordVectoriserFactory(self, model)
        feature.embedding_size = model.embedding_size
        return feature

    def embedding(self, model: TextVectoriserModel) -> Embedding:
        return self.sentence_vector(model)

    def append(self, other: FeatureFactory | str) -> String:
        from aligned.compiler.transformation_factory import AppendStrings

        feature = String()
        if isinstance(other, FeatureFactory):
            feature.transformation = AppendStrings(self, other)
        else:
            feature.transformation = AppendStrings(self, LiteralValue.from_value(other))
        return feature"
69;aligned/compiler/feature_factory.py;beta;"class Entity(FeatureFactory):

    _dtype: FeatureFactory

    @property
    def dtype(self) -> FeatureType:
        return self._dtype.dtype

    def __init__(self, dtype: FeatureFactory):
        self._dtype = dtype

    def aggregate(self) -> CategoricalAggregation:
        return CategoricalAggregation(self)"
70;aligned/compiler/feature_factory.py;beta;"class Timestamp(DateFeature, ArithmeticFeature):
    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').datetime"
71;aligned/compiler/feature_factory.py;beta;"class EventTimestamp(DateFeature, ArithmeticFeature):

    ttl: timedelta | None

    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').datetime

    def __init__(self, ttl: timedelta | None = None):
        self.ttl = ttl

    def event_timestamp(self) -> EventTimestampFeature:
        return EventTimestampFeature(
            name=self.name, ttl=self.ttl.total_seconds() if self.ttl else None, description=self._description
        )"
72;aligned/compiler/feature_factory.py;beta;"class Embedding(FeatureFactory):

    sub_type: FeatureFactory
    embedding_size: int | None = None
    indexes: list[VectorIndexFactory] | None = None

    def copy_type(self) -> Embedding:
        return Embedding()

    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').embedding

    def indexed(
        self,
        storage: VectorStorage,
        metadata: list[FeatureFactory] | None = None,
        embedding_size: int | None = None,
    ) -> Embedding:
        if self.indexes is None:
            self.indexes = []

        if not self.embedding_size:
            assert embedding_size, 'An embedding size is needed in order to create a vector index'

        self.indexes.append(
            VectorIndexFactory(
                vector_dim=self.embedding_size or embedding_size,
                metadata=metadata or [],
                storage=storage,
            )
        )
        return self"
73;aligned/compiler/feature_factory.py;beta;"class ImageUrl(StringValidatable):
    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').string

    def copy_type(self) -> ImageUrl:
        return ImageUrl()

    def load_image(self) -> Image:
        from aligned.compiler.transformation_factory import LoadImageFactory

        image = Image()
        image.transformation = LoadImageFactory(self)
        return image"
74;aligned/compiler/feature_factory.py;beta;"class Image(FeatureFactory):
    @property
    def dtype(self) -> FeatureType:
        return FeatureType('').array

    def copy_type(self) -> Image:
        return Image()

    def to_grayscale(self) -> Image:
        from aligned.compiler.transformation_factory import GrayscaleImageFactory

        image = Image()
        image.transformation = GrayscaleImageFactory(self)
        return image"
75;aligned/compiler/feature_factory.py;beta;"class Coordinate:

    x: ArithmeticFeature
    y: ArithmeticFeature

    def eucledian_distance(self, to: Coordinate) -> Float:
        sub = self.x - to.x
        sub.hidden = True
        return (sub**2 + (self.y - to.y) ** 2) ** 0.5"
76;aligned/compiler/feature_factory.py;beta;"class StringAggregation:

    feature: String
    time_window: timedelta | None = None

    def over(self, time_window: timedelta) -> StringAggregation:
        self.time_window = time_window
        return self

    def concat(self, separator: str | None = None) -> String:
        from aligned.compiler.aggregation_factory import ConcatStringsAggrigationFactory

        feature = String()
        feature.transformation = ConcatStringsAggrigationFactory(
            self.feature, group_by=[], separator=separator, time_window=self.time_window
        )
        return feature"
77;aligned/compiler/feature_factory.py;beta;"class CategoricalAggregation:

    feature: FeatureFactory
    time_window: timedelta | None = None

    def over(
        self,
        weeks: float | None = None,
        days: float | None = None,
        hours: float | None = None,
        minutes: float | None = None,
        seconds: float | None = None,
    ) -> ArithmeticAggregation:
        self.time_window = timedelta(
            weeks=weeks or 0, days=days or 0, hours=hours or 0, minutes=minutes or 0, seconds=seconds or 0
        )
        return self

    def count(self) -> Int64:
        from aligned.compiler.aggregation_factory import CountAggregationFactory

        feat = Float()
        feat.transformation = CountAggregationFactory(self.feature, group_by=[], time_window=self.time_window)
        return feat"
78;aligned/compiler/feature_factory.py;beta;"class ArithmeticAggregation:

    feature: ArithmeticFeature
    time_window: timedelta | None = None

    def over(
        self,
        weeks: float | None = None,
        days: float | None = None,
        hours: float | None = None,
        minutes: float | None = None,
        seconds: float | None = None,
    ) -> ArithmeticAggregation:
        self.time_window = timedelta(
            weeks=weeks or 0, days=days or 0, hours=hours or 0, minutes=minutes or 0, seconds=seconds or 0
        )
        return self

    def sum(self) -> Float:
        from aligned.compiler.aggregation_factory import SumAggregationFactory

        feat = Float()
        feat.transformation = SumAggregationFactory(self.feature, group_by=[], time_window=self.time_window)
        return feat

    def mean(self) -> Float:
        from aligned.compiler.aggregation_factory import MeanAggregationFactory

        feat = Float()
        feat.transformation = MeanAggregationFactory(self.feature, group_by=[], time_window=self.time_window)
        return feat

    def min(self) -> Float:
        from aligned.compiler.aggregation_factory import MinAggregationFactory

        feat = Float()
        feat.transformation = MinAggregationFactory(self.feature, group_by=[], time_window=self.time_window)
        return feat

    def max(self) -> Float:
        from aligned.compiler.aggregation_factory import MaxAggregationFactory

        feat = Float()
        feat.transformation = MaxAggregationFactory(self.feature, group_by=[], time_window=self.time_window)
        return feat

    def count(self) -> Int64:
        from aligned.compiler.aggregation_factory import CountAggregationFactory

        feat = Int64()
        feat.transformation = CountAggregationFactory(self.feature, group_by=[], time_window=self.time_window)
        return feat

    def count_distinct(self) -> Int64:
        from aligned.compiler.aggregation_factory import CountDistinctAggregationFactory

        feat = Int64()
        feat.transformation = CountDistinctAggregationFactory(
            self.feature, group_by=[], time_window=self.time_window
        )
        return feat

    def std(self) -> Float:
        from aligned.compiler.aggregation_factory import StdAggregationFactory

        feat = Float()
        feat.transformation = StdAggregationFactory(self.feature, group_by=[], time_window=self.time_window)
        return feat

    def variance(self) -> Float:
        from aligned.compiler.aggregation_factory import VarianceAggregationFactory

        feat = Float()
        feat.transformation = VarianceAggregationFactory(
            self.feature, group_by=[], time_window=self.time_window
        )
        return feat

    def median(self) -> Float:
        from aligned.compiler.aggregation_factory import MedianAggregationFactory

        feat = Float()
        feat.transformation = MedianAggregationFactory(
            self.feature, group_by=[], time_window=self.time_window
        )
        return feat

    def percentile(self, percentile: float) -> Float:
        from aligned.compiler.aggregation_factory import PercentileAggregationFactory

        feat = Float()
        feat.transformation = PercentileAggregationFactory(
            self.feature, percentile=percentile, group_by=[], time_window=self.time_window
        )
        return feat"
79;aligned/compiler/model.py;beta;"class SqlEntityDataSource(EntityDataSource):

    url: str
    timestamp_column: str

    def __init__(self, sql: Callable[[str], str], url: str, timestamp_column: str) -> None:
        self.sql = sql
        self.url = url
        self.timestamp_column = timestamp_column

    async def all_in_range(self, start_date: datetime, end_date: datetime) -> pl.DataFrame:
        import os

        start = start_date.strftime('%Y-%m-%d %H:%M:%S')
        end = end_date.strftime('%Y-%m-%d %H:%M:%S')

        query = self.sql(f'{self.timestamp_column} BETWEEN \'{start}\' AND \'{end}\'')
        return pl.read_sql(query, os.environ[self.url])

    async def last(self, days: int, hours: int, seconds: int) -> pl.DataFrame:
        now = datetime.utcnow()
        return await self.all_in_range(now - timedelta(days=days, hours=hours, seconds=seconds), now)"
80;aligned/compiler/model.py;beta;"class ModelMetedata:
    name: str
    features: list[FeatureReferencable]
    # Will log the feature inputs to a model. Therefore, enabling log and wait etc.
    # feature_logger: WritableBatchSource | None = field(default=None)
    contacts: list[str] | None = field(default=None)
    tags: dict[str, str] | None = field(default=None)
    description: str | None = field(default=None)
    predictions_source: BatchDataSource | None = field(default=None)
    predictions_stream: StreamDataSource | None = field(default=None)
    dataset_folder: Folder | None = field(default=None)"
81;aligned/compiler/model.py;beta;"class Model(ABC):
    @staticmethod
    def metadata_with(
        name: str,
        description: str,
        features: list[FeatureReferencable],
        contacts: list[str] | None = None,
        tags: dict[str, str] | None = None,
        predictions_source: BatchDataSource | None = None,
        predictions_stream: StreamDataSource | None = None,
        dataset_folder: Folder | None = None,
    ) -> ModelMetedata:
        return ModelMetedata(
            name,
            features,
            contacts,
            tags,
            description,
            predictions_source,
            predictions_stream,
            dataset_folder,
        )

    @abstractproperty
    def metadata(self) -> ModelMetedata:
        pass

    @classmethod
    def compile(cls) -> ModelSchema:
        var_names = [name for name in cls().__dir__() if not name.startswith('_')]
        metadata = cls().metadata

        inference_view: PredictionsView = PredictionsView(
            entities=set(),
            features=set(),
            derived_features=set(),
            source=metadata.predictions_source,
            stream_source=metadata.predictions_stream,
            classification_targets=set(),
            regression_targets=set(),
        )
        probability_features: dict[str, set[TargetProbability]] = {}

        classification_targets: dict[str, ClassificationTargetSchema] = {}
        regression_targets: dict[str, RegressionTargetSchema] = {}

        for var_name in var_names:
            feature = getattr(cls, var_name)

            if isinstance(feature, FeatureFactory):
                feature._location = FeatureLocation.model(metadata.name)

            if isinstance(feature, FeatureView):
                compiled = feature.compile()
                inference_view.entities.update(compiled.entities)
            elif isinstance(feature, Model):
                compiled = feature.compile()
                inference_view.entities.update(compiled.predictions_view.entities)
            elif isinstance(feature, ClassificationTarget):
                assert feature._name
                feature._location = FeatureLocation.model(metadata.name)
                target_feature = feature.compile()

                classification_targets[var_name] = target_feature
                inference_view.classification_targets.add(target_feature)
            elif isinstance(feature, RegressionTarget):
                assert feature._name
                feature._location = FeatureLocation.model(metadata.name)
                target_feature = feature.compile()

                regression_targets[var_name] = target_feature
                inference_view.regression_targets.add(target_feature)
            elif isinstance(feature, EventTimestamp):
                inference_view.event_timestamp = feature.event_timestamp()

            elif isinstance(feature, TargetProbability):
                feature_name = feature.target._name
                assert feature._name
                assert (
                    feature.target._name in classification_targets
                ), 'Target must be a classification target.'

                target = classification_targets[feature.target._name]
                target.class_probabilities.add(feature.compile())

                inference_view.features.add(
                    Feature(
                        var_name,
                        FeatureType('').float,
                        f""The probability of target named {feature_name} being '{feature.of_value}'."",
                    )
                )
                probability_features[feature_name] = probability_features.get(feature_name, set()).union(
                    {feature}
                )
            elif isinstance(feature, FeatureFactory):
                inference_view.features.add(feature.feature())

        # Needs to run after the feature views have compiled
        features: set[FeatureReferance] = {feature.feature_referance() for feature in metadata.features}

        for target, probabilities in probability_features.items():
            from aligned.schemas.transformation import MapArgMax

            transformation = MapArgMax(
                {probs._name: LiteralValue.from_value(probs.of_value) for probs in probabilities}
            )

            arg_max_feature = DerivedFeature(
                name=target,
                dtype=transformation.dtype,
                transformation=transformation,
                depending_on={
                    FeatureReferance(feat, FeatureLocation.model(metadata.name), dtype=FeatureType('').float)
                    for feat in transformation.column_mappings.keys()
                },
                depth=1,
            )
            inference_view.derived_features.add(arg_max_feature)
        if not probability_features:
            inference_view.features.update(
                {target.feature for target in inference_view.classification_targets}
            )

        return ModelSchema(
            name=metadata.name,
            features=features,
            predictions_view=inference_view,
            contacts=metadata.contacts,
            tags=metadata.tags,
            description=metadata.description,
            dataset_folder=metadata.dataset_folder,
        )"
82;aligned/compiler/repo_reader.py;beta;"class name. Only store the superclasses
    # Otherwise it might init a abstract"
83;aligned/compiler/repo_reader.py;beta;"class and crash
    s = str(obj).replace(""'"", '').replace('>', '')
    super_class_names.remove(s.split('.')[-1])
    return super_class_names


def find_files(repo_path: Path, ignore_path: Path | None = None, file_extension: str = ""py"") -> list[Path]:
    files = {
        path.resolve()
        for path in repo_path.resolve().glob(f'**/*.{file_extension}')
        if path.is_file() and '__init__.py' != path.name and not any(part.startswith(""."") for part in path.parts)
    }
    if ignore_path:
        ignore_files = {
            path.resolve()
            for path in ignore_path.glob(f'**/*.{file_extension}')
            if path.is_file() and '__init__.py' != path.name and not any(part.startswith(""."") for part in path.parts)
        }
        files -= ignore_files
    return sorted(files)


def path_to_py_module(path: Path, repo_root: Path) -> str:
    return str(path.relative_to(repo_root))[: -len('.py')].replace('./', '').replace('/', '.')"
84;aligned/compiler/repo_reader.py;beta;"class RepoReader:
    """"""
    A"
85;aligned/compiler/repo_reader.py;beta;"class reading a repo, and generates a repo config
    """"""

    @staticmethod
    async def definition_from_path(repo_path: Path) -> RepoDefinition:

        metadata = RepoMetadata(created_at=datetime.now(), name=repo_path.name, github_url=None)
        repo = RepoDefinition(
            metadata=metadata,
            feature_views=set(),
            combined_feature_views=set(),
            models=set(),
            enrichers=[],
        )

        feature_view_names: dict[str, str] = {}

        for py_file in find_files(repo_path):
            imports = imports_for(py_file)

            module_path = path_to_py_module(py_file, repo_path)
            if (
                module_path.startswith('aladdin')
                or module_path.startswith('.')
                or module_path.startswith('heroku')
                or module_path.endswith('__')
            ):
                # Skip aladdin modules
                continue

            module = import_module(module_path)

            for attribute in dir(module):
                if attribute in imports:
                    continue

                obj = getattr(module, attribute)

                if isinstance(obj, Enricher):
                    repo.enrichers.append(
                        EnricherReference(module=module_path, attribute_name=attribute, enricher=obj)
                    )
                else:
                    classes = super_classes_in(obj)
                    if 'Model' in classes:
                        repo.models.add(obj.compile())
                    elif 'FeatureView' in classes:
                        fv = obj.compile()
                        if fv.name in feature_view_names:
                            raise Exception(
                                (
                                    f'Duplicate feature view names: {fv.name},',
                                    f' in {py_file}, and {feature_view_names[fv.name]}',
                                )
                            )
                        feature_view_names[fv.name] = py_file.as_posix()
                        repo.feature_views.add(fv)
                    elif 'CombinedFeatureView' in classes:
                        fv = obj.compile()
                        if fv.name in feature_view_names:
                            raise Exception(
                                (
                                    f'Duplicate feature view names: {fv.name},',
                                    f' in {py_file}, and {feature_view_names[fv.name]}',
                                )
                            )
                        feature_view_names[fv.name] = py_file.as_posix()
                        repo.combined_feature_views.add(fv)
        return repo

    @staticmethod
    def reference_from_path(repo_path: Path, file: Path) -> RepoReference:

        imports = imports_for(file)

        module_path = path_to_py_module(file, repo_path)
        module = import_module(module_path)

        for attribute in dir(module):
            if attribute in imports:
                continue

            obj = getattr(module, attribute)

            if isinstance(obj, RepoReference):
                return obj
        raise ValueError('No reference found')"
86;aligned/compiler/transformation_factory.py;beta;"class EqualsFactory(TransformationFactory):

    left: FeatureFactory
    right: FeatureFactory | Any

    @property
    def using_features(self) -> list[FeatureFactory]:
        if isinstance(self.right, FeatureFactory):
            return [self.left, self.right]
        else:
            return [self.left]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Equals

        if isinstance(self.right, FeatureFactory):
            raise NotImplementedError()
        else:
            return Equals(self.left.name, LiteralValue.from_value(self.right))"
87;aligned/compiler/transformation_factory.py;beta;"class NotNullFactory(TransformationFactory):

    value: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.value]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import NotNull

        return NotNull(self.value.name)"
88;aligned/compiler/transformation_factory.py;beta;"class RatioFactory(TransformationFactory):

    numerator: FeatureFactory
    denumerator: FeatureFactory | LiteralValue

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.numerator, self.denumerator]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import DivideDenumeratorValue, Ratio

        if isinstance(self.denumerator, LiteralValue):
            return DivideDenumeratorValue(self.numerator.name, self.denumerator)
        return Ratio(self.numerator.name, self.denumerator.name)"
89;aligned/compiler/transformation_factory.py;beta;"class OrdinalFactory(TransformationFactory):

    orders: list[str]
    feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Ordinal

        return Ordinal(self.feature.name, self.orders)"
90;aligned/compiler/transformation_factory.py;beta;"class ContainsFactory(TransformationFactory):

    text: str
    in_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.in_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Contains as ContainsTransformation

        return ContainsTransformation(self.in_feature.name, self.text)"
91;aligned/compiler/transformation_factory.py;beta;"class NotEqualsFactory(TransformationFactory):

    value: Any
    in_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.in_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import NotEquals as NotEqualsTransformation

        return NotEqualsTransformation(self.in_feature.name, LiteralValue.from_value(self.value))"
92;aligned/compiler/transformation_factory.py;beta;"class GreaterThenFactory(TransformationFactory):

    left_feature: FeatureFactory
    right: Any

    @property
    def using_features(self) -> list[FeatureFactory]:
        if isinstance(self.right, FeatureFactory):
            return [self.left_feature, self.right]
        else:
            return [self.left_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import GreaterThen, GreaterThenValue

        if isinstance(self.right, FeatureFactory):
            return GreaterThen(self.left_feature.name, self.right.name)
        else:
            return GreaterThenValue(self.left_feature.name, self.right)"
93;aligned/compiler/transformation_factory.py;beta;"class GreaterThenOrEqualFactory(TransformationFactory):

    value: Any
    in_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.in_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import GreaterThenOrEqual as GTETransformation

        return GTETransformation(self.in_feature.name, self.value)"
94;aligned/compiler/transformation_factory.py;beta;"class LowerThenFactory(TransformationFactory):

    value: float
    in_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.in_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import LowerThen as LTTransformation

        return LTTransformation(self.in_feature.name, self.value)"
95;aligned/compiler/transformation_factory.py;beta;"class LowerThenOrEqualFactory(TransformationFactory):

    value: float
    in_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.in_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import LowerThenOrEqual as LTETransformation

        return LTETransformation(self.in_feature.name, self.value)


# @dataclass
#"
96;aligned/compiler/transformation_factory.py;beta;"class Split(TransformationFactory):

#     pattern: str
#     from_feature: FeatureFactory
#     max_splits: int | None

#     @property
#     def method(self) -> Callable[[pd.DataFrame], pd.Series]:
#         async def met(df: pd.DataFrame) -> pd.Series:
#             return df[self.from_feature.name].str.split(pat=self.pattern, n=self.max_splits)

#         return met

#     def index(self, index: int) -> 'ArrayIndex':
#         return ArrayIndex(index, self)


#"
97;aligned/compiler/transformation_factory.py;beta;"class ArrayIndex(DillTransformationFactory):

#     index: int
#     from_feature: FeatureFactory

#     def __init__(self, index: int, feature: FeatureFactory) -> None:
#         self.using_features = [feature]
#         self.feature = Bool()
#         self.index = index
#         self.from_feature = feature

#     @property
#     def method(self) -> Callable[[pd.DataFrame], pd.Series]:
#         async def met(df: pd.DataFrame) -> pd.Series:
#             return df[self.from_feature.name].str[self.index]

#         return met"
98;aligned/compiler/transformation_factory.py;beta;"class DateComponentFactory(TransformationFactory):

    component: str
    feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import DateComponent as DCTransformation

        return DCTransformation(self.feature.name, self.component)"
99;aligned/compiler/transformation_factory.py;beta;"class DifferanceBetweenFactory(TransformationFactory):

    first_feature: FeatureFactory
    second_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.first_feature, self.second_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Subtraction

        return Subtraction(self.first_feature.name, self.second_feature.name)"
100;aligned/compiler/transformation_factory.py;beta;"class AdditionBetweenFactory(TransformationFactory):

    first_feature: FeatureFactory
    second_feature: FeatureFactory | Any

    @property
    def using_features(self) -> list[FeatureFactory]:
        if isinstance(self.second_feature, FeatureFactory):
            return [self.first_feature, self.second_feature]
        else:
            return [self.first_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Addition, AdditionValue

        if isinstance(self.second_feature, FeatureFactory):
            return Addition(self.first_feature.name, self.second_feature.name)
        else:
            return AdditionValue(self.first_feature.name, LiteralValue.from_value(self.second_feature))"
101;aligned/compiler/transformation_factory.py;beta;"class PowerFactory(TransformationFactory):

    first: FeatureFactory
    second: FeatureFactory | Any

    @property
    def using_features(self) -> list[FeatureFactory]:
        if isinstance(self.second, FeatureFactory):
            return [self.first, self.second]
        return [self.first]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Power, PowerFeature

        if isinstance(self.second, FeatureFactory):
            return PowerFeature(self.first.name, self.second.name)
        else:
            value = LiteralValue.from_value(self.second)
            return Power(self.first.name, value)"
102;aligned/compiler/transformation_factory.py;beta;"class TimeDifferanceFactory(TransformationFactory):

    first_feature: FeatureFactory
    second_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.first_feature, self.second_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import TimeDifference as TDTransformation

        return TDTransformation(self.first_feature.name, self.second_feature.name)"
103;aligned/compiler/transformation_factory.py;beta;"class LogTransformFactory(TransformationFactory):

    feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import LogarithmOnePluss

        return LogarithmOnePluss(self.feature.name)"
104;aligned/compiler/transformation_factory.py;beta;"class ReplaceFactory(TransformationFactory):

    values: dict[str, str]
    source_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.source_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import ReplaceStrings

        return ReplaceStrings(self.source_feature.name, self.values)"
105;aligned/compiler/transformation_factory.py;beta;"class ToNumericalFactory(TransformationFactory):

    from_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.from_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import ToNumerical as ToNumericalTransformation

        return ToNumericalTransformation(self.from_feature.name)"
106;aligned/compiler/transformation_factory.py;beta;"class IsInFactory(TransformationFactory):

    feature: FeatureFactory
    values: list

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import IsIn as IsInTransformation

        return IsInTransformation(self.values, self.feature.name)"
107;aligned/compiler/transformation_factory.py;beta;"class AndFactory(TransformationFactory):

    first_feature: FeatureFactory
    second_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.first_feature, self.second_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import And as AndTransformation

        return AndTransformation(self.first_feature.name, self.second_feature.name)"
108;aligned/compiler/transformation_factory.py;beta;"class OrFactory(TransformationFactory):

    first_feature: FeatureFactory
    second_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.first_feature, self.second_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Or as OrTransformation

        return OrTransformation(self.first_feature.name, self.second_feature.name)"
109;aligned/compiler/transformation_factory.py;beta;"class InverseFactory(TransformationFactory):

    from_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.from_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Inverse as InverseTransformation

        return InverseTransformation(self.from_feature.name)"
110;aligned/compiler/transformation_factory.py;beta;"class FillNaStrategy:
    def compile(self) -> Any:
        pass"
111;aligned/compiler/transformation_factory.py;beta;"class ConstantFillNaStrategy(FillNaStrategy):
    value: Any

    def compile(self) -> Any:
        return self.value"
112;aligned/compiler/transformation_factory.py;beta;"class FillMissingFactory(TransformationFactory):

    feature: FeatureFactory
    strategy: FillNaStrategy

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import FillNaValues

        fill_value = self.strategy.compile()

        return FillNaValues(
            key=self.feature.name, value=LiteralValue.from_value(fill_value), dtype=self.feature.dtype
        )"
113;aligned/compiler/transformation_factory.py;beta;"class FloorFactory(TransformationFactory):

    feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Floor

        return Floor(self.feature.name)"
114;aligned/compiler/transformation_factory.py;beta;"class CeilFactory(TransformationFactory):

    feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Ceil

        return Ceil(self.feature.name)"
115;aligned/compiler/transformation_factory.py;beta;"class RoundFactory(TransformationFactory):

    feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Round

        return Round(self.feature.name)"
116;aligned/compiler/transformation_factory.py;beta;"class AbsoluteFactory(TransformationFactory):

    feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Absolute

        return Absolute(self.feature.name)"
117;aligned/compiler/transformation_factory.py;beta;"class PandasTransformationFactory(TransformationFactory):

    dtype: FeatureFactory
    method: Callable[[pd.DataFrame], pd.Series]
    _using_features: list[FeatureFactory]

    @property
    def using_features(self) -> list[FeatureFactory]:
        return self._using_features

    def compile(self) -> Transformation:
        import inspect
        import types

        import dill

        from aligned.schemas.transformation import PandasFunctionTransformation, PandasLambdaTransformation

        if isinstance(self.method, types.LambdaType) and self.method.__name__ == '<lambda>':
            return PandasLambdaTransformation(
                method=dill.dumps(self.method),
                code=inspect.getsource(self.method).strip(),
                dtype=self.dtype.dtype,
            )
        else:
            return PandasFunctionTransformation(
                code=inspect.getsource(self.method),
                function_name=dill.source.getname(self.method),
                dtype=self.dtype.dtype,
            )"
118;aligned/compiler/transformation_factory.py;beta;"class PolarsTransformationFactory(TransformationFactory):

    dtype: FeatureFactory
    method: pl.Expr | Callable[[pl.LazyFrame, pl.Expr], pl.LazyFrame]
    _using_features: list[FeatureFactory]

    @property
    def using_features(self) -> list[FeatureFactory]:
        return self._using_features

    def compile(self) -> Transformation:
        import inspect
        import types

        import dill

        from aligned.schemas.transformation import PolarsFunctionTransformation, PolarsLambdaTransformation

        if isinstance(self.method, pl.Expr):
            code = str(self.method)
            return PolarsLambdaTransformation(
                method=dill.dumps(self.method), code=code.strip(), dtype=self.dtype.dtype
            )
        else:
            code = inspect.getsource(self.method)

        if isinstance(self.method, types.LambdaType) and self.method.__name__ == '<lambda>':
            return PolarsLambdaTransformation(
                method=dill.dumps(self.method), code=code.strip(), dtype=self.dtype.dtype
            )
        else:
            return PolarsFunctionTransformation(
                code=code,
                function_name=dill.source.getname(self.method),
                dtype=self.dtype.dtype,
            )"
119;aligned/compiler/transformation_factory.py;beta;"class AggregatableTransformation:

    group_by: list[FeatureFactory] | None = field(default=None)

    def copy(self) -> 'AggregatableTransformation':
        pass"
120;aligned/compiler/transformation_factory.py;beta;"class MeanTransfomrationFactory(TransformationFactory, AggregatableTransformation):

    feature: FeatureFactory
    over: timedelta | None = field(default=None)
    group_by: list[FeatureFactory] | None = field(default=None)

    @property
    def using_features(self) -> list[FeatureFactory]:
        if self.group_by:
            return [self.feature] + self.group_by
        else:
            return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Mean

        return Mean(
            key=self.feature.name, group_keys=[feat.name for feat in self.group_by] if self.group_by else None
        )

    def copy(self) -> 'MeanTransfomrationFactory':
        return MeanTransfomrationFactory(self.feature, self.over, self.group_by)"
121;aligned/compiler/transformation_factory.py;beta;"class WordVectoriserFactory(TransformationFactory):

    feature: FeatureFactory
    model: TextVectoriserModel

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import WordVectoriser

        return WordVectoriser(self.feature.name, self.model)"
122;aligned/compiler/transformation_factory.py;beta;"class LoadImageFactory(TransformationFactory):

    url_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.url_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import LoadImageUrl

        return LoadImageUrl(self.url_feature.name)"
123;aligned/compiler/transformation_factory.py;beta;"class GrayscaleImageFactory(TransformationFactory):

    image_feature: FeatureFactory

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.image_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import GrayscaleImage

        return GrayscaleImage(self.image_feature.name)"
124;aligned/compiler/transformation_factory.py;beta;"class AppendStrings(TransformationFactory):

    first_feature: FeatureFactory
    second_feature: FeatureFactory | LiteralValue
    separator: str = field(default='')

    @property
    def using_features(self) -> list[FeatureFactory]:
        if isinstance(self.second_feature, LiteralValue):
            return [self.first_feature]
        else:
            return [self.first_feature, self.second_feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import AppendConstString, AppendStrings

        if isinstance(self.second_feature, LiteralValue):
            return AppendConstString(self.first_feature.name, self.second_feature.value)
        else:
            return AppendStrings(self.first_feature.name, self.second_feature.name, self.separator)"
125;aligned/compiler/transformation_factory.py;beta;"class ClipFactory(TransformationFactory):

    feature: FeatureFactory
    lower_bound: int | float
    upper_bound: int | float

    @property
    def using_features(self) -> list[FeatureFactory]:
        return [self.feature]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Clip

        return Clip(
            self.feature.name,
            LiteralValue.from_value(self.lower_bound),
            LiteralValue.from_value(self.upper_bound),
        )"
126;aligned/compiler/transformation_factory.py;beta;"class MultiplyFactory(TransformationFactory):

    first: FeatureFactory
    behind: FeatureFactory | LiteralValue

    @property
    def using_features(self) -> list[FeatureFactory]:
        if isinstance(self.behind, LiteralValue):
            return [self.first]
        else:
            return [self.first, self.behind]

    def compile(self) -> Transformation:
        from aligned.schemas.transformation import Multiply, MultiplyValue

        if isinstance(self.behind, LiteralValue):
            return MultiplyValue(self.first.name, self.behind)
        else:
            return Multiply(self.first.name, self.behind.name)"
127;aligned/compiler/vector_index_factory.py;beta;"class VectorIndexFactory:

    vector_dim: int
    metadata: list[FeatureFactory]
    storage: VectorStorage

    def compile(self, location: FeatureLocation, vector: Feature, entities: set[Feature]) -> VectorIndex:
        return VectorIndex(
            location=location,
            vector=vector,
            vector_dim=self.vector_dim,
            metadata=[feature.feature() for feature in self.metadata],
            storage=self.storage,
            entities=list(entities),
        )"
128;aligned/data_file.py;beta;"class DataFileReference:
    """"""
    A reference to a data file.

    It can therefore be loaded in and writen to.
    Either as a pandas data frame, or a dask data frame.
    """"""

    async def read_pandas(self) -> pd.DataFrame:
        raise NotImplementedError()

    async def to_pandas(self) -> pd.DataFrame:
        await self.read_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        raise NotImplementedError()

    async def write_polars(self, df: pl.LazyFrame) -> None:
        raise NotImplementedError()

    async def write_pandas(self, df: pd.DataFrame) -> None:
        raise NotImplementedError()"
129;aligned/data_source/batch_data_source.py;beta;"class BatchDataSourceFactory:

    supported_data_sources: dict[str, type[BatchDataSource]]

    _shared: BatchDataSourceFactory | None = None

    def __init__(self) -> None:
        from aligned.sources.local import CsvFileSource, ParquetFileSource
        from aligned.sources.psql import PostgreSQLDataSource
        from aligned.sources.redshift import RedshiftSQLDataSource
        from aligned.sources.s3 import AwsS3CsvDataSource, AwsS3ParquetDataSource

        self.supported_data_sources = {
            PostgreSQLDataSource.type_name: PostgreSQLDataSource,
            ParquetFileSource.type_name: ParquetFileSource,
            CsvFileSource.type_name: CsvFileSource,
            AwsS3CsvDataSource.type_name: AwsS3CsvDataSource,
            AwsS3ParquetDataSource.type_name: AwsS3ParquetDataSource,
            RedshiftSQLDataSource.type_name: RedshiftSQLDataSource,
        }

    @classmethod
    def shared(cls) -> BatchDataSourceFactory:
        if cls._shared:
            return cls._shared
        cls._shared = BatchDataSourceFactory()
        return cls._shared


T = TypeVar('T')"
130;aligned/data_source/batch_data_source.py;beta;"class BatchDataSource(ABC, Codable, SerializableType):
    """"""
    A definition to where a specific pice of data can be found.
    E.g: A database table, a file, a web service, etc.

    Ths can thereafter be combined with other BatchDataSources in order to create a rich dataset.
    """"""

    type_name: str

    @abstractmethod
    def job_group_key(self) -> str:
        pass

    def _serialize(self) -> dict:
        assert (
            self.type_name in BatchDataSourceFactory.shared().supported_data_sources
        ), f'Unknown type_name: {self.type_name}'
        return self.to_dict()

    def __hash__(self) -> int:
        return hash(self.job_group_key())

    @classmethod
    def _deserialize(cls, value: dict) -> BatchDataSource:
        name_type = value['type_name']
        if name_type not in BatchDataSourceFactory.shared().supported_data_sources:
            raise ValueError(
                f""Unknown batch data source id: '{name_type}'.\nRemember to add the""
                ' data source to the BatchDataSourceFactory.supported_data_sources if'
                ' it is a custom type.'
            )
        del value['type_name']
        data_"
131;aligned/data_source/batch_data_source.py;beta;"class = BatchDataSourceFactory.shared().supported_data_sources[name_type]
        return data_class.from_dict(value)

    def all_data(self, request: RetrivalRequest, limit: int | None) -> FullExtractJob:
        raise NotImplementedError()

    def all_between_dates(
        self,
        request: RetrivalRequest,
        start_date: datetime,
        end_date: datetime,
    ) -> DateRangeJob:
        raise NotImplementedError()

    @classmethod
    def multi_source_features_for(
        cls: type[T], facts: RetrivalJob, requests: list[tuple[T, RetrivalRequest]]
    ) -> RetrivalJob:
        raise NotImplementedError()

    def features_for(self, facts: RetrivalJob, request: RetrivalRequest) -> RetrivalJob:
        return type(self).multi_source_features_for(facts, [(self, request)])"
132;aligned/data_source/batch_data_source.py;beta;"class ColumnFeatureMappable:
    mapping_keys: dict[str, str]

    def columns_for(self, features: list[Feature]) -> list[str]:
        return [self.mapping_keys.get(feature.name, feature.name) for feature in features]

    def feature_identifier_for(self, columns: list[str]) -> list[str]:
        reverse_map = {v: k for k, v in self.mapping_keys.items()}
        return [reverse_map.get(column, column) for column in columns]"
133;aligned/data_source/stream_data_source.py;beta;"class StreamDataSourceFactory:

    supported_data_sources: dict[str, type[StreamDataSource]]

    _shared: StreamDataSourceFactory | None = None

    def __init__(self) -> None:
        from aligned.sources.redis import RedisStreamSource

        self.supported_data_sources = {
            HttpStreamSource.name: HttpStreamSource,
            RedisStreamSource.name: RedisStreamSource,
        }

    @classmethod
    def shared(cls) -> StreamDataSourceFactory:
        if cls._shared:
            return cls._shared
        cls._shared = StreamDataSourceFactory()
        return cls._shared"
134;aligned/data_source/stream_data_source.py;beta;"class StreamDataSource(ABC, Codable, SerializableType):
    """"""
    Used to determend if an API call should be created, or if we should listen to a stream.
    """"""

    name: str
    topic_name: str

    def _serialize(self) -> dict:
        assert self.name in StreamDataSourceFactory.shared().supported_data_sources
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> StreamDataSource:
        name = value['name']
        if name not in StreamDataSourceFactory.shared().supported_data_sources:
            raise ValueError(
                f""Unknown stream data source id: '{name}'.\nRemember to add the""
                ' data source to the StreamDataSourceFactory.supported_data_sources if'
                ' it is a custom type.'
            )
        del value['name']
        data_"
135;aligned/data_source/stream_data_source.py;beta;"class = StreamDataSourceFactory.shared().supported_data_sources[name]
        return data_class.from_dict(value)"
136;aligned/data_source/stream_data_source.py;beta;"class HttpStreamSource(StreamDataSource):

    topic_name: str
    mappings: dict[str, str] = field(default_factory=dict)

    name: str = 'http'

    def map_values(self, mappings: dict[str, str]) -> HttpStreamSource:
        return HttpStreamSource(topic_name=self.topic_name, mappings=self.mappings | mappings)"
137;aligned/data_source/stream_data_source.py;beta;"class SinkableDataSource:
    async def write_to_stream(self, job: RetrivalJob) -> None:
        pass"
138;aligned/enricher.py;beta;"class TimespanSelector(Codable):
    timespand: timedelta
    time_column: str"
139;aligned/enricher.py;beta;"class StatisticEricher:
    def std(
        self, columns: set[str], time: TimespanSelector | None = None, limit: int | None = None
    ) -> Enricher:
        raise NotImplementedError()

    def mean(
        self, columns: set[str], time: TimespanSelector | None = None, limit: int | None = None
    ) -> Enricher:
        raise NotImplementedError()"
140;aligned/enricher.py;beta;"class Enricher(ABC, Codable, SerializableType):

    name: str

    def _serialize(self) -> dict:
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> Enricher:
        name_type = value['name']
        del value['name']
        data_"
141;aligned/enricher.py;beta;"class = SupportedEnrichers.shared().types[name_type]
        return data_class.from_dict(value)

    def lock(self, lock_name: str, redis_config: RedisConfig, timeout: int = 60) -> Enricher:
        return RedisLockEnricher(lock_name=lock_name, enricher=self, config=redis_config, timeout=timeout)

    def cache(self, ttl: timedelta, cache_key: str) -> Enricher:
        return FileCacheEnricher(ttl, cache_key, self)

    @abstractmethod
    async def as_df(self) -> pd.DataFrame:
        pass"
142;aligned/enricher.py;beta;"class SupportedEnrichers:

    types: dict[str, type[Enricher]]

    _shared: SupportedEnrichers | None = None

    def __init__(self) -> None:
        self.types = {}

        default_types: list[type[Enricher]] = [RedisLockEnricher, FileCacheEnricher, SqlDatabaseEnricher]
        for enrich_type in default_types:
            self.add(enrich_type)

    def add(self, enrich_type: type[Enricher]) -> None:
        self.types[enrich_type.name] = enrich_type

    @classmethod
    def shared(cls) -> SupportedEnrichers:
        if cls._shared:
            return cls._shared
        cls._shared = SupportedEnrichers()
        return cls._shared"
143;aligned/enricher.py;beta;"class RedisLockEnricher(Enricher):

    enricher: Enricher
    config: RedisConfig
    lock_name: str
    timeout: int
    name: str = 'redis_lock'

    def __init__(self, lock_name: str, enricher: Enricher, config: RedisConfig, timeout: int):
        self.lock_name = lock_name
        self.config = config
        self.enricher = enricher
        self.timeout = timeout

    async def as_df(self) -> pd.DataFrame:
        redis = self.config.redis()
        async with redis.lock(self.lock_name, timeout=self.timeout) as _:
            return await self.enricher.as_df()"
144;aligned/enricher.py;beta;"class CsvFileSelectedEnricher(Enricher):
    file: str
    time: TimespanSelector | None = field(default=None)
    limit: int | None = field(default=None)
    name: str = 'selective_file'

    async def as_df(self) -> pd.DataFrame:
        dates_to_parse = None
        if self.time:
            dates_to_parse = [self.time.time_column]

        uri = self.file
        path = Path(self.file)
        if 'http' not in path.parts[0]:
            uri = str(path.absolute())

        if self.limit:
            file = pd.read_csv(uri, nrows=self.limit, parse_dates=dates_to_parse)
        else:
            file = pd.read_csv(uri, nrows=self.limit, parse_dates=dates_to_parse)

        if not self.time:
            return file

        date = datetime.now() - self.time.timespand
        selector = file[self.time.time_column] >= date
        return file.loc[selector]"
145;aligned/enricher.py;beta;"class CsvFileEnricher(Enricher):

    file: str
    name: str = 'file'

    def selector(
        self, time: TimespanSelector | None = None, limit: int | None = None
    ) -> CsvFileSelectedEnricher:
        return CsvFileSelectedEnricher(self.file, time=time, limit=limit)

    async def as_df(self) -> pd.DataFrame:
        return pd.read_csv(self.file)"
146;aligned/enricher.py;beta;"class LoadedStatEnricher(Enricher):

    stat: str
    columns: list[str]
    enricher: Enricher
    mapping_keys: dict[str, str] = field(default_factory=dict)

    async def as_df(self) -> pd.DataFrame:
        data = await self.enricher.as_df()
        renamed = data.rename(columns=self.mapping_keys)
        if self.stat == 'mean':
            return renamed[self.columns].mean()
        elif self.stat == 'std':
            return renamed[self.columns].std()
        else:
            raise ValueError(f'Not supporting stat: {self.stat}')"
147;aligned/enricher.py;beta;"class FileCacheEnricher(Enricher):

    ttl: timedelta
    file_path: str
    enricher: Enricher
    name: str = 'file_cache'

    def is_out_of_date_cache(self) -> bool:
        file_uri = Path(self.file_path).absolute()
        try:
            # Checks last modified metadata field
            modified_at = datetime.fromtimestamp(file_uri.stat().st_mtime)
            compare = datetime.now() - self.ttl
            return modified_at < compare
        except FileNotFoundError:
            return True

    async def as_df(self) -> pd.DataFrame:
        file_uri = Path(self.file_path).absolute()

        if self.is_out_of_date_cache():
            logger.info('Fetching from source')
            data: pd.DataFrame = await self.enricher.as_df()
            file_uri.parent.mkdir(exist_ok=True, parents=True)
            logger.info(f'Storing cache at file {file_uri.as_uri()}')
            data.to_parquet(file_uri)
        else:
            logger.info('Loading cache')
            data = pd.read_parquet(file_uri)
        return data"
148;aligned/enricher.py;beta;"class SqlDatabaseEnricher(Enricher):

    query: str
    values: dict | None
    url_env: str
    name: str = 'sql'

    def __init__(self, url_env: str, query: str, values: dict | None = None) -> None:
        self.query = query
        self.values = values
        self.url_env = url_env

    async def as_df(self) -> pd.DataFrame:
        import os

        import connectorx as cx

        df = cx.read_sql(os.environ[self.url_env], self.query, return_type='pandas')

        for name, dtype in df.dtypes.iteritems():
            if dtype == 'object':  # Need to convert the databases UUID type
                df[name] = df[name].astype('str')

        return df"
149;aligned/entity_data_source.py;beta;"class EntityDataSource:
    async def all_in_range(self, start_date: datetime, end_date: datetime) -> DataFrame:
        pass

    async def last(self, days: int, hours: int, seconds: int) -> DataFrame:
        pass"
150;aligned/exceptions.py;beta;"class UnableToFindFileException(Exception):
    pass"
151;aligned/exceptions.py;beta;"class CombinedFeatureViewQuerying(Exception):
    pass"
152;aligned/exceptions.py;beta;"class NotSupportedYet(Exception):
    def __init__(self, message: str) -> None:
        super().__init__(f'{message}. What about contributing and adding a PR for this?')"
153;aligned/exceptions.py;beta;"class StreamWorkerNotFound(Exception):
    def __init__(self, module: str) -> None:
        super().__init__(
            f'Unable to find the stream worker. Tried to load module ""{module}"". '
            'This is needed in order to know where to store the processed features. '
            'Try adding a worker.py file in your root folder and define a StreamWorker object.'
        )"
154;aligned/feature_source.py;beta;"class FeatureSourceFactory:
    def feature_source(self) -> FeatureSource:
        raise NotImplementedError()"
155;aligned/feature_source.py;beta;"class FeatureSource:
    def features_for(self, facts: RetrivalJob, request: FeatureRequest) -> RetrivalJob:
        raise NotImplementedError()"
156;aligned/feature_source.py;beta;"class WritableFeatureSource:
    async def write(self, job: RetrivalJob, requests: list[RetrivalRequest]) -> None:
        raise NotImplementedError()"
157;aligned/feature_source.py;beta;"class RangeFeatureSource:
    def all_for(self, request: FeatureRequest, limit: int | None = None) -> RetrivalJob:
        raise NotImplementedError()

    def all_between(self, start_date: datetime, end_date: datetime, request: FeatureRequest) -> RetrivalJob:
        raise NotImplementedError()"
158;aligned/feature_source.py;beta;"class BatchFeatureSource(FeatureSource, RangeFeatureSource):
    """"""A factory for different type of jobs
    This could either be a ""fetch all"", or ""fetch features for ids""

    This"
159;aligned/feature_source.py;beta;"class will then know how to strucutre the query in the correct way
    """"""

    sources: dict[str, BatchDataSource]

    @property
    def source_types(self) -> dict[str, type[BatchDataSource]]:
        return {source.job_group_key(): type(source) for source in self.sources.values()}

    def features_for(self, facts: RetrivalJob, request: FeatureRequest) -> RetrivalJob:
        from aligned.retrival_job import CombineFactualJob

        core_requests = [
            (self.sources[request.location.identifier], request)
            for request in request.needed_requests
            if request.location.identifier in self.sources
        ]
        source_groupes = {
            self.sources[request.location.identifier].job_group_key()
            for request in request.needed_requests
            if request.location.identifier in self.sources
        }

        # The combined views basicly, as they have no direct
        combined_requests = [
            request for request in request.needed_requests if request.location.identifier not in self.sources
        ]
        jobs = [
            self.source_types[source_group].multi_source_features_for(
                facts=facts,
                requests=[
                    (source, req) for source, req in core_requests if source.job_group_key() == source_group
                ],
            )
            for source_group in source_groupes
        ]
        return (
            CombineFactualJob(
                jobs=jobs,
                combined_requests=combined_requests,
            )
            .ensure_types(request.needed_requests)
            .derive_features(request.needed_requests)
        )

    def all_for(self, request: FeatureRequest, limit: int | None = None) -> RetrivalJob:
        if len(request.needed_requests) != 1:
            raise ValueError(""Can't use all_for with a request that has subrequests"")
        if request.location.identifier not in self.sources:
            raise ValueError(
                (
                    f""Unable to find feature view named '{request.location.identifier}'."",
                    'Make sure it is added to the featuer store',
                )
            )
        return (
            self.sources[request.location.identifier]
            .all_data(request.needed_requests[0], limit)
            .ensure_types(request.needed_requests)
            .derive_features(request.needed_requests)
        )

    def all_between(self, start_date: datetime, end_date: datetime, request: FeatureRequest) -> RetrivalJob:
        if len(request.needed_requests) != 1:
            raise ValueError(""Can't use all_for with a request that has subrequests"")
        if request.location.identifier not in self.sources:
            raise ValueError(
                (
                    f""Unable to find feature view named '{request.location.identifier}'."",
                    'Make sure it is added to the featuer store',
                )
            )
        return (
            self.sources[request.location.identifier]
            .all_between_dates(request.needed_requests[0], start_date, end_date)
            .ensure_types(request.needed_requests)
            .derive_features(requests=request.needed_requests)
        )"
160;aligned/feature_source.py;beta;"class FactualInMemoryJob(RetrivalJob):
    """"""
    A job using a in mem storage, aka a dict.

    This will store the features in the following format:

    values = {
        ""feature_view:entity-id:feature-name"": value,

        ...
        ""titanic_passenger:20:age"": 22,
        ""titanic_passenger:21:age"": 50,
        ...
        ""titanic_passenger:20:class"": ""Eco"",
        ""titanic_passenger:21:class"": ""VIP"",
    }
    """"""

    values: dict[str, Any]
    requests: list[RetrivalRequest]
    facts: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.requests)

    def __init__(self, values: dict[str, Any], requests: list[RetrivalRequest], facts: RetrivalJob) -> None:
        self.values = values
        self.requests = requests
        self.facts = facts

    def key(self, request: RetrivalRequest, entity: str, feature_name: str) -> str:
        return f'{request.location}:{entity}:{feature_name}'

    async def to_pandas(self) -> pd.DataFrame:

        columns = set()
        for request in self.requests:
            for feature in request.all_feature_names:
                columns.add(feature)

        result_df = await self.facts.to_pandas()

        for request in self.requests:
            entity_ids = result_df[list(request.entity_names)]
            entities = entity_ids.sum(axis=1)

            for feature in request.all_feature_names:
                # if feature.name in request.entity_names:
                #     continue
                # Fetch one column at a time
                result_df[feature] = [
                    self.values.get(self.key(request, entity, feature)) for entity in entities
                ]

        return result_df

    async def to_polars(self) -> pl.LazyFrame:
        return pl.from_pandas(await self.to_pandas()).lazy()"
161;aligned/feature_source.py;beta;"class InMemoryFeatureSource(FeatureSource, WritableFeatureSource):

    values: dict[str, Any]

    def features_for(self, facts: RetrivalJob, request: FeatureRequest) -> RetrivalJob:
        return FactualInMemoryJob(self.values, request.needed_requests, facts)

    def key(self, request: RetrivalRequest, entity: str, feature_name: str) -> str:
        return f'{request.location}:{entity}:{feature_name}'

    async def write(self, job: RetrivalJob, requests: list[RetrivalRequest]) -> None:
        data = await job.to_pandas()

        for _, row in data.iterrows():
            # Run one query per row
            for request in requests:
                entity_ids = row[list(request.entity_names)]
                entity_id = ':'.join([str(entity_id) for entity_id in entity_ids])
                if not entity_id:
                    continue

                for feature in request.all_features:
                    feature_key = self.key(request, entity_id, feature.name)
                    value = row[feature.name]
                    if value is not None and not np.isnan(value):
                        self.values[feature_key] = row[feature.name]"
162;aligned/feature_store.py;beta;"class RawStringFeatureRequest:

    features: set[str]

    @property
    def locations(self) -> set[FeatureLocation]:
        return {RawStringFeatureRequest.unpack_feature(feature)[0] for feature in self.features}

    @property
    def grouped_features(self) -> dict[FeatureLocation, set[str]]:
        unpacked_features = [RawStringFeatureRequest.unpack_feature(feature) for feature in self.features]
        grouped = defaultdict(set)
        for feature_view, feature in unpacked_features:
            grouped[feature_view].add(feature)
        return grouped

    @property
    def feature_names(self) -> set[str]:
        return {RawStringFeatureRequest.unpack_feature(feature)[1] for feature in self.features}

    @staticmethod
    def unpack_feature(feature: str) -> tuple[FeatureLocation, str]:
        splits = feature.split(':')
        if len(splits) == 3:
            return (FeatureLocation(splits[1], splits[0]), splits[2])
        if len(splits) == 2:
            return (FeatureLocation(splits[0], 'feature_view'), splits[1])
        else:
            raise ValueError(f'Unable to decode {splits}')"
163;aligned/feature_store.py;beta;"class FeatureStore:

    feature_source: FeatureSource
    feature_views: dict[str, CompiledFeatureView]
    combined_feature_views: dict[str, CompiledCombinedFeatureView]
    models: dict[str, ModelSchema]
    event_timestamp_column = 'event_timestamp'

    @property
    def all_models(self) -> list[str]:
        return list(self.models.keys())

    def __init__(
        self,
        feature_views: dict[str, CompiledFeatureView],
        combined_feature_views: dict[str, CompiledCombinedFeatureView],
        models: dict[str, ModelSchema],
        feature_source: FeatureSource,
    ) -> None:
        self.feature_source = feature_source
        self.combined_feature_views = combined_feature_views
        self.feature_views = feature_views
        self.models = models

    @staticmethod
    def experimental() -> 'FeatureStore':
        return FeatureStore.from_definition(
            RepoDefinition(
                metadata=RepoMetadata(created_at=datetime.utcnow(), name='experimental'),
            )
        )

    @staticmethod
    def register_enrichers(enrichers: list[EnricherReference]) -> None:
        from types import ModuleType"
164;aligned/feature_store.py;beta;"class DynamicEnricher(ModuleType):
            def __init__(self, values: dict[str, Enricher]) -> None:
                for key, item in values.items():
                    self.__setattr__(key, item)

        def set_module(path: str, module_class: DynamicEnricher) -> None:
            import sys

            components = path.split('.')
            cum_path = ''

            for component in components:
                cum_path += f'.{component}'
                if cum_path.startswith('.'):
                    cum_path = cum_path[1:]

                try:
                    sys.modules[cum_path] = import_module(cum_path)
                except Exception:
                    logger.info(f'Setting enricher at {cum_path}')
                    sys.modules[cum_path] = module_class

        grouped_enrichers: dict[str, list[EnricherReference]] = defaultdict(list)

        for enricher in enrichers:
            grouped_enrichers[enricher.module].append(enricher)

        for module, values in grouped_enrichers.items():
            set_module(
                module, DynamicEnricher({enricher.attribute_name: enricher.enricher for enricher in values})
            )

    @staticmethod
    def from_definition(repo: RepoDefinition, feature_source: FeatureSource | None = None) -> 'FeatureStore':
        """"""Creates a feature store based on a repo definition
        A feature source can also be defined if wanted, otherwise will the batch source be used for reads

        ```
        repo_file: bytes = ...
        repo_def = RepoDefinition.from_json(repo_file)
        feature_store = FeatureStore.from_definition(repo_def)
        ```

        Args:
            repo (RepoDefinition): The definition to setup
            feature_source (FeatureSource | None, optional): The source to read from and potentially write to.

        Returns:
            FeatureStore: A ready to use feature store
        """"""
        feature_views = {fv.name: fv for fv in repo.feature_views}
        combined_feature_views = {fv.name: fv for fv in repo.combined_feature_views}

        FeatureStore.register_enrichers(repo.enrichers)

        return FeatureStore(
            feature_views=feature_views,
            combined_feature_views=combined_feature_views,
            models={model.name: model for model in repo.models},
            feature_source=BatchFeatureSource(
                {FeatureLocation.feature_view(fv.name).identifier: fv.batch_data_source for fv in repo.feature_views}
            ),
        )

    def repo_definition(self) -> RepoDefinition:
        return RepoDefinition(
            metadata=RepoMetadata(datetime.utcnow(), 'feature_store_location.py'),
            feature_views=set(self.feature_views.values()),
            combined_feature_views=set(self.combined_feature_views.values()),
            models=set(self.models.values()),
            enrichers=[],
        )

    @staticmethod
    async def from_reference_at_path(
        path: str = '.', reference_file: str = 'feature_store_location.py'
    ) -> 'FeatureStore':
        """"""Looks for a file reference struct, and loads the associated repo.

        This can be used for changing which feature store definitions
        to read based on defined enviroment variables.

        If you rather want to generate a feature store based on a dir,
        then consider using `FeatureStore.from_dir(...)` instead.

        Args:
            path (str, optional): The path of the dir to search. Defaults to ""."".

        Returns:
            FeatureStore: A feature store based on the feature references
        """"""
        repo_def = await RepoDefinition.from_reference_at_path(path, reference_file)
        return FeatureStore.from_definition(repo_def)

    @staticmethod
    async def from_dir(path: str = '.') -> 'FeatureStore':
        """"""Reads and generates a feature store based on the given directory's content.

        This will read the feature views, services etc in a given repo and generate a feature store.
        This can be used for fast development purposes.

        If you rather want a more flexible deployable solution.
        Consider using `FeatureStore.from_reference_at_path(...)` which will can read an existing
        generated file from differnet storages, based on an enviroment variable.

        Args:
            path (str, optional): the directory to read from. Defaults to ""."".

        Returns:
            FeatureStore: The generated feature store
        """"""
        definition = await RepoDefinition.from_path(path)
        return FeatureStore.from_definition(definition)

    def features_for_request(
        self, requests: FeatureRequest, entities: dict[str, list] | RetrivalJob, feature_names: set[str]
    ) -> RetrivalJob:
        entity_request: RetrivalJob

        if isinstance(entities, dict):
            if requests.needs_event_timestamp and self.event_timestamp_column not in entities:
                raise ValueError(f'Missing {self.event_timestamp_column} in entities')

            entity_request = RetrivalJob.from_dict(entities, requests)
        else:
            entity_request = entities

        return FilterJob(
            feature_names,
            self.feature_source.features_for(entity_request, requests),
        )

    def features_for(self, entities: dict[str, list] | RetrivalJob, features: list[str]) -> RetrivalJob:

        feature_request = RawStringFeatureRequest(features=set(features))
        requests = self.requests_for(feature_request)

        feature_names = set()

        if requests.needs_event_timestamp:
            feature_names.add(self.event_timestamp_column)

        for view, feature_set in feature_request.grouped_features.items():
            if feature_set != {'*'}:
                feature_names.update(feature_set)
            else:
                for request in requests.needed_requests:
                    if view.name == request.location.name:
                        feature_names.update(request.all_feature_names)
        for request in requests.needed_requests:
            feature_names.update(request.entity_names)

        return self.features_for_request(requests, entities, feature_names)

    def model(self, name: str) -> 'ModelFeatureStore':
        model = self.models[name]
        return ModelFeatureStore(model, self)

    def event_triggers_for(self, feature_view: str) -> set[EventTrigger]:
        triggers = self.feature_views[feature_view].event_triggers or set()
        for model in self.models.values():
            for target in model.predictions_view.classification_targets:
                if target.event_trigger and target.estimating.location.location == feature_view:
                    triggers.add(target.event_trigger)
        return triggers

    @staticmethod
    def _requests_for(
        feature_request: RawStringFeatureRequest,
        feature_views: dict[str, CompiledFeatureView],
        combined_feature_views: dict[str, CompiledCombinedFeatureView],
    ) -> FeatureRequest:
        features = feature_request.grouped_features
        requests: list[RetrivalRequest] = []
        entity_names = set()
        needs_event_timestamp = False

        for location in feature_request.locations:
            feature_view_name = location.name
            if feature_view_name in combined_feature_views:
                cfv = combined_feature_views[feature_view_name]
                if len(features[location]) == 1 and list(features[location])[0] == '*':
                    sub_requests = cfv.request_all
                else:
                    sub_requests = cfv.requests_for(features[location])
                requests.extend(sub_requests.needed_requests)
                for request in sub_requests.needed_requests:
                    entity_names.update(request.entity_names)
                    if request.event_timestamp:
                        needs_event_timestamp = True

            elif feature_view_name in feature_views:
                feature_view = feature_views[feature_view_name]
                if len(features[location]) == 1 and list(features[location])[0] == '*':
                    sub_requests = feature_view.request_all
                else:
                    sub_requests = feature_view.request_for(features[location])
                requests.extend(sub_requests.needed_requests)
                for request in sub_requests.needed_requests:
                    entity_names.update(request.entity_names)
                    if request.event_timestamp:
                        needs_event_timestamp = True
            else:
                raise ValueError(
                    f'Unable to find: {feature_view_name}, '
                    f'availible views are: {combined_feature_views.keys()}, and: {feature_views.keys()}'
                )

        if needs_event_timestamp:
            entity_names.add('event_timestamp')

        return FeatureRequest(
            FeatureLocation.model('custom features'),
            feature_request.feature_names.union(entity_names),
            RetrivalRequest.combine(requests),
        )

    def requests_for(self, feature_request: RawStringFeatureRequest) -> FeatureRequest:
        return FeatureStore._requests_for(feature_request, self.feature_views, self.combined_feature_views)

    def feature_view(self, view: str) -> 'FeatureViewStore':
        """"""
        Selects a feature view based on a name.

        From here can you query the feature view for features.

        ```python
        data = await store.feature_view('my_view').all(limit=10).to_pandas()
        ```

        Args:
            view (str): The name of the feature view

        Raises:
            CombinedFeatureViewQuerying: If the name is a combined feature view

        Returns:
            FeatureViewStore: The selected feature view ready for querying
        """"""
        if view in self.combined_feature_views:
            raise CombinedFeatureViewQuerying(
                'You are trying to get a combined feature view. ',
                'This is only possible through store.features_for(...), as of now.\n',
            )
        feature_view = self.feature_views[view]
        return FeatureViewStore(self, feature_view, self.event_triggers_for(view))

    def add_feature_view(self, feature_view: FeatureView) -> None:
        """"""
        Compiles and adds the feature view to the store

        ```python"
165;aligned/feature_store.py;beta;"class MyFeatureView(FeatureView):
            metadata = ...

            id = Int32().as_entity()

            my_feature = String()

        store.add_feature_view(MyFeatureView())
        ```

        Args:
            feature_view (FeatureView): The feature view to add
        """"""
        compiled_view = type(feature_view).compile()
        self.feature_views[compiled_view.name] = compiled_view
        if isinstance(self.feature_source, BatchFeatureSource):
            self.feature_source.sources[
                FeatureLocation.feature_view(compiled_view.name).identifier
            ] = compiled_view.batch_data_source

    def add_combined_feature_view(self, feature_view: CombinedFeatureView) -> None:
        compiled_view = type(feature_view).compile()
        self.combined_feature_views[compiled_view.name] = compiled_view

    def add_model(self, model: Model) -> None:
        """"""
        Compiles and adds the model to the store

        Args:
            model (Model): The model to add
        """"""
        compiled_model = type(model).compile()
        self.models[compiled_model.name] = compiled_model

    def with_source(self, source: FeatureSource | FeatureSourceFactory | None = None) -> 'FeatureStore':
        """"""
        Creates a new instance of a feature store, but changes where to fetch the features from

        ```
        store = # Load the store
        redis_store = store.with_source(redis)
        batch_source = redis_store.with_source()
        ```

        Args:
            source (FeatureSource): The source to fetch from, None will lead to using the batch source

        Returns:
            FeatureStore: A new feature store instance
        """"""
        if isinstance(source, FeatureSourceFactory):
            feature_source = source.feature_source()
        else:
            feature_source = source or BatchFeatureSource(
                {
                    FeatureLocation.feature_view(view.name).identifier: view.batch_data_source
                    for view in set(self.feature_views.values())
                }
            )

        return FeatureStore(
            feature_views=self.feature_views,
            combined_feature_views=self.combined_feature_views,
            models=self.models,
            feature_source=feature_source,
        )

    def offline_store(self) -> 'FeatureStore':
        """"""
        Will set the source to the defined batch sources.

        Returns:
            FeatureStore: A new feature store that loads features from the batch sources
        """"""
        return self.with_source()

    def model_features_for(self, view_name: str) -> set[str]:
        all_model_features: set[str] = set()
        for model in self.models.values():
            all_model_features.update(
                {feature.name for feature in model.features if feature.location.name == view_name}
            )
        return all_model_features"
166;aligned/feature_store.py;beta;"class ModelFeatureStore:

    model: ModelSchema
    store: FeatureStore

    @property
    def raw_string_features(self) -> set[str]:
        return {f'{feature.location.identifier}:{feature.name}' for feature in self.model.features}

    @property
    def request(self) -> FeatureRequest:
        return self.store.requests_for(RawStringFeatureRequest(self.raw_string_features))

    def features_for(self, entities: dict[str, list] | RetrivalJob) -> RetrivalJob:
        request = self.request
        features = self.raw_string_features
        return self.store.features_for(entities, list(features)).filter(request.features_to_include)

    def with_target(self) -> 'SupervisedModelFeatureStore':
        return SupervisedModelFeatureStore(self.model, self.store)

    def cached_at(self, location: DataFileReference) -> RetrivalJob:
        from aligned.local.job import FileFullJob

        features = {f'{feature.location.identifier}:{feature.name}' for feature in self.model.features}
        request = self.store.requests_for(RawStringFeatureRequest(features))

        return FileFullJob(location, RetrivalRequest.unsafe_combine(request.needed_requests)).filter(
            request.features_to_include
        )

    def process_features(self, input: RetrivalJob | dict[str, list]) -> RetrivalJob:
        request = self.request

        if isinstance(input, RetrivalJob):
            job = input.filter(request.features_to_include)
        elif isinstance(input, dict):
            job = RetrivalJob.from_dict(input, request=request.needed_requests)
        else:
            raise ValueError(f'features must be a dict or a RetrivalJob, was {type(input)}')

        return (
            job.ensure_types(request.needed_requests)
            .derive_features(request.needed_requests)
            .filter(request.features_to_include)
        )"
167;aligned/feature_store.py;beta;"class SupervisedModelFeatureStore:

    model: ModelSchema
    store: FeatureStore

    def features_for(self, entities: dict[str, list] | RetrivalJob) -> SupervisedJob:
        feature_refs = self.model.features
        features = {f'{feature.location.identifier}:{feature.name}' for feature in feature_refs}
        pred_view = self.model.predictions_view
        target_features = set()
        targets = set()
        if pred_view.classification_targets:
            target_features = {
                f'{feature.estimating.location.identifier}:{feature.estimating.name}'
                for feature in pred_view.classification_targets
            }
            targets = {feature.estimating.name for feature in pred_view.classification_targets}
        elif pred_view.regression_targets:
            target_features = {
                f'{feature.estimating.location.identifier}:{feature.estimating.name}'
                for feature in pred_view.regression_targets
            }
            targets = {feature.estimating.name for feature in pred_view.regression_targets}
        else:
            raise ValueError('Found no targets in the model')

        request = self.store.requests_for(RawStringFeatureRequest(features))
        target_request = self.store.requests_for(
            RawStringFeatureRequest(target_features)
        ).without_event_timestamp(name_sufix='target')

        total_request = FeatureRequest(
            FeatureLocation.model(self.model.name),
            request.features_to_include.union(target_request.features_to_include),
            request.needed_requests + target_request.needed_requests,
        )
        job = self.store.features_for_request(total_request, entities, total_request.features_to_include)
        return SupervisedJob(
            job,
            target_columns=targets,
        )"
168;aligned/feature_store.py;beta;"class FeatureViewStore:

    store: FeatureStore
    view: CompiledFeatureView
    event_triggers: set[EventTrigger] = field(default_factory=set)
    feature_filter: set[str] | None = field(default=None)
    only_write_model_features: bool = field(default=False)

    @property
    def request(self) -> RetrivalRequest:
        if self.only_write_model_features:
            features_in_models = self.store.model_features_for(self.view.name)
            return self.view.request_for(features_in_models).needed_requests[0]
        else:
            return self.view.request_all.needed_requests[0]

    @property
    def source(self) -> FeatureSource:
        return self.store.feature_source

    def with_optimised_write(self) -> 'FeatureViewStore':
        features_in_models = self.store.model_features_for(self.view.name)
        return self.select(features_in_models)

    def all(self, limit: int | None = None) -> RetrivalJob:
        if not isinstance(self.source, RangeFeatureSource):
            raise ValueError(f'The source ({self.source}) needs to conform to RangeFeatureSource')

        request = self.view.request_all
        if self.feature_filter:
            request = self.view.request_for(self.feature_filter)

        job = (
            self.source.all_for(request, limit)
            .ensure_types(request.needed_requests)
            .derive_features(request.needed_requests)
        )
        if self.feature_filter:
            return FilterJob(include_features=self.feature_filter, job=job)
        else:
            return job

    def between_dates(self, start_date: datetime, end_date: datetime) -> RetrivalJob:
        if not isinstance(self.source, RangeFeatureSource):
            raise ValueError(
                f'The source needs to conform to RangeFeatureSource, you got a {type(self.source)}'
            )

        if self.feature_filter:
            request = self.view.request_for(self.feature_filter)
            return FilterJob(self.feature_filter, self.source.all_between(start_date, end_date, request))

        request = self.view.request_all
        return self.source.all_between(start_date, end_date, request)

    def previous(self, days: int = 0, minutes: int = 0, seconds: int = 0) -> RetrivalJob:
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=days, minutes=minutes, seconds=seconds)
        return self.between_dates(start_date, end_date)

    def features_for(self, entities: dict[str, list] | RetrivalJob) -> RetrivalJob:

        request = self.view.request_all
        if self.feature_filter:
            request = self.view.request_for(self.feature_filter)

        if isinstance(entities, dict):
            entity_job = RetrivalJob.from_dict(entities, request)
        elif isinstance(entities, RetrivalJob):
            entity_job = entities
        else:
            raise ValueError(f'entities must be a dict or a RetrivalJob, was {type(entities)}')

        job = self.source.features_for(entity_job, request)
        if self.feature_filter:
            return job.filter(self.feature_filter)
        else:
            return job

    def select(self, features: set[str]) -> 'FeatureViewStore':
        return FeatureViewStore(self.store, self.view, self.event_triggers, features)

    @property
    def write_input(self) -> set[str]:
        features = set()
        for request in self.view.request_all.needed_requests:
            features.update(request.all_required_feature_names)
            features.update(request.entity_names)
            if event_timestamp := request.event_timestamp:
                features.add(event_timestamp.name)
        return features

    async def write(self, values: dict[str, list[Any]]) -> None:
        from aligned import FileSource
        from aligned.data_file import DataFileReference
        from aligned.schemas.derivied_feature import AggregateOver

        request = self.view.request_all.needed_requests[0]
        job = (
            RetrivalJob.from_dict(values, request)
            .validate_entites()
            .fill_missing_columns()
            .ensure_types([request])
        )

        aggregations = request.aggregate_over()
        if aggregations:
            checkpoints: dict[AggregateOver, DataFileReference] = {}

            for aggregation in aggregations.keys():
                name = f'{self.view.name}_agg'

                if aggregation.window:
                    name += f'_{aggregation.window.time_window.total_seconds()}'

                if aggregation.condition:
                    name += f'_{aggregation.condition.name}'

                checkpoints[aggregation] = FileSource.parquet_at(name)

            job = StreamAggregationJob(job, checkpoints)

        await self.batch_write(job)

    def process_input(self, values: dict[str, list[Any]]) -> RetrivalJob:

        request = self.view.request_all.needed_requests[0]

        job = RetrivalJob.from_dict(values, request)

        return job.fill_missing_columns().ensure_types([request]).derive_features([request])

    async def batch_write(self, values: dict[str, list[Any]] | RetrivalJob) -> None:

        if not isinstance(self.source, WritableFeatureSource):
            logger.info('Feature Source is not writable')
            return

        # As it is a feature view, should it only contain one request
        request = self.request

        core_job: RetrivalJob

        if isinstance(values, RetrivalJob):
            core_job = values
        elif isinstance(values, dict):
            core_job = RetrivalJob.from_dict(values, request)
        else:
            raise ValueError(f'values must be a dict or a RetrivalJob, was {type(values)}')

        job = (
            core_job.derive_features([request])
            .listen_to_events(self.event_triggers)
            .update_vector_index(self.view.indexes)
        )

        if self.feature_filter:
            logger.info(f'Only writing features used by models: {self.feature_filter}')
            job = job.filter(self.feature_filter)

        await self.source.write(job, job.retrival_requests)"
169;aligned/feature_view/combined_view.py;beta;"class CombinedFeatureViewMetadata:
    name: str
    description: str | None = None
    tags: dict[str, str] | None = None
    owner: str | None = None"
170;aligned/feature_view/combined_view.py;beta;"class CombinedFeatureView(ABC):
    @abstractproperty
    def metadata(self) -> CombinedFeatureViewMetadata:
        pass

    @staticmethod
    def _needed_features(
        depending_on: list[FeatureFactory], feature_views: dict[FeatureLocation, CompiledFeatureView]
    ) -> list[RetrivalRequest]:

        feature_refs: dict[CompiledFeatureView, set[str]] = {}

        for feature_dep in depending_on:
            view = feature_views[feature_dep._location]
            feature_refs.setdefault(view, set()).add(feature_dep.name)

        return [
            feature_view.request_for(features).needed_requests[0]
            for feature_view, features in feature_refs.items()
        ]

    @classmethod
    def compile(cls) -> CompiledCombinedFeatureView:
        transformations: set[DerivedFeature] = set()
        metadata = cls().metadata
        var_names = [name for name in cls().__dir__() if not name.startswith('_')]

        requests: dict[str, list[RetrivalRequest]] = {}
        feature_view_deps: dict[FeatureLocation, CompiledFeatureView] = {}

        for var_name in var_names:
            feature = getattr(cls, var_name)
            if isinstance(feature, FeatureView):
                # Needs to compile the view one more time. unfortunally..
                # not optimal as the view will be duplicated in the definition file
                feature_view_deps[FeatureLocation.feature_view(feature.metadata.name)] = feature.compile()
            if isinstance(feature, FeatureFactory):
                feature._location = FeatureLocation.combined_view(var_name)
                if not feature.transformation:
                    logger.info('Feature had no transformation, which do not make sense in a CombinedView')
                    continue
                requests[var_name] = CombinedFeatureView._needed_features(
                    feature.transformation.using_features, feature_view_deps
                )

                transformations.add(feature.compile())

        return CompiledCombinedFeatureView(
            name=metadata.name,
            features=transformations,
            feature_referances=requests,
        )"
171;aligned/feature_view/feature_view.py;beta;"class FeatureViewMetadata:
    name: str
    description: str
    batch_source: BatchDataSource
    stream_source: StreamDataSource | None = field(default=None)
    contacts: list[str] | None = field(default=None)
    tags: dict[str, str] = field(default_factory=dict)

    @staticmethod
    def from_compiled(view: CompiledFeatureView) -> FeatureViewMetadata:
        return FeatureViewMetadata(
            name=view.name,
            description=view.description,
            tags=view.tags,
            batch_source=view.batch_data_source,
            stream_source=view.stream_data_source,
        )"
172;aligned/feature_view/feature_view.py;beta;"class FeatureView(ABC):
    """"""
    A collection of features, and a way to combine them.

    This should contain the core features, and might contain derived features (aka. transformations).
    """"""

    @abstractproperty
    def metadata(self) -> FeatureViewMetadata:
        pass

    @staticmethod
    def metadata_with(
        name: str,
        description: str,
        batch_source: BatchDataSource,
        stream_source: StreamDataSource | None = None,
        contacts: list[str] | None = None,
        tags: dict[str, str] | None = None,
    ) -> FeatureViewMetadata:
        from aligned import HttpStreamSource

        return FeatureViewMetadata(
            name,
            description,
            batch_source,
            stream_source or HttpStreamSource(name),
            contacts=contacts,
            tags=tags or {},
        )

    @classmethod
    def compile(cls) -> CompiledFeatureView:
        from aligned.compiler.feature_factory import FeatureFactory

        # Used to deterministicly init names for hidden features
        hidden_features = 0

        metadata = cls().metadata
        var_names = [name for name in cls().__dir__() if not name.startswith('_')]

        view = CompiledFeatureView(
            name=metadata.name,
            description=metadata.description,
            tags=metadata.tags,
            batch_data_source=metadata.batch_source,
            entities=set(),
            features=set(),
            derived_features=set(),
            aggregated_features=set(),
            event_timestamp=None,
            stream_data_source=metadata.stream_source,
            indexes=[],
        )
        aggregations: list[FeatureFactory] = []

        for var_name in var_names:
            feature = getattr(cls, var_name)

            if not isinstance(feature, FeatureFactory):
                continue

            feature._name = var_name
            feature._location = FeatureLocation.feature_view(metadata.name)
            compiled_feature = feature.feature()

            if isinstance(feature, Embedding) and feature.indexes:
                view.indexes.extend(
                    [
                        index.compile(feature._location, compiled_feature, view.entities)
                        for index in feature.indexes
                    ]
                )

            if feature.transformation:
                # Adding features that is not stored in the view
                # e.g:
                #"
173;aligned/feature_view/feature_view.py;beta;"class SomeView(FeatureView):
                #     ...
                #     x, y = Bool(), Bool()
                #     z = (x & y) | x
                #
                # Here will (x & y)'s result be a 'hidden' feature
                feature_deps = [(feat.depth(), feat) for feat in feature.feature_dependencies()]

                # Sorting by key in order to instanciate the ""core"" features first
                # And then making it possible for other features to reference them
                def sort_key(x: tuple[int, FeatureFactory]) -> int:
                    return x[0]

                for depth, feature_dep in sorted(feature_deps, key=sort_key):

                    if not feature_dep._location:
                        feature_dep._location = FeatureLocation.feature_view(metadata.name)

                    if feature_dep._name:
                        feat_dep = feature_dep.feature()
                        if feat_dep in view.features or feat_dep in view.entities:
                            continue

                    if depth == 0:
                        # The raw value and the transformed have the same name
                        feature_dep._name = var_name
                        feat_dep = feature_dep.feature()
                        view.features.add(feat_dep)
                        continue

                    if not feature_dep._name:
                        feature_dep._name = str(hidden_features)
                        hidden_features += 1

                    if isinstance(feature_dep.transformation, AggregationTransformationFactory):
                        aggregations.append(feature_dep)
                    else:
                        feature_graph = feature_dep.compile()  # Should decide on which payload to send
                        if feature_graph in view.derived_features:
                            continue

                        view.derived_features.add(feature_dep.compile())

                if isinstance(feature.transformation, AggregationTransformationFactory):
                    aggregations.append(feature)
                else:
                    view.derived_features.add(feature.compile())  # Should decide on which payload to send

            elif isinstance(feature, Entity):
                view.entities.add(compiled_feature)
            elif isinstance(feature, EventTimestamp):
                if view.event_timestamp is not None:
                    raise Exception(
                        'Can only have one EventTimestamp for each'
                        ' FeatureViewDefinition. Check that this is the case for'
                        f' {cls.__name__}'
                    )
                view.features.add(compiled_feature)
                view.event_timestamp = feature.event_timestamp()
            else:
                view.features.add(compiled_feature)

        if not view.entities:
            raise ValueError(f'FeatureView {metadata.name} must contain at least one Entity')

        aggregation_group_by = [
            FeatureReferance(entity.name, FeatureLocation.feature_view(view.name), entity.dtype)
            for entity in view.entities
        ]

        for aggr in aggregations:
            agg_trans = aggr.transformation
            if not isinstance(agg_trans, AggregationTransformationFactory):
                continue

            if view.event_timestamp is None and agg_trans.time_window:
                raise ValueError(f'FeatureView {metadata.name} must contain an EventTimestamp')

            time_window: AggregationTimeWindow | None = None
            if agg_trans.time_window:

                timestamp_ref = FeatureReferance(
                    view.event_timestamp.name,
                    FeatureLocation.feature_view(view.name),
                    dtype=view.event_timestamp.dtype,
                )
                time_window = AggregationTimeWindow(agg_trans.time_window, timestamp_ref)

            aggr.transformation = agg_trans.with_group_by(aggregation_group_by)
            config = AggregateOver(aggregation_group_by, window=time_window, condition=None)
            feature = aggr.compile()
            feat = AggregatedFeature(
                derived_feature=feature,
                aggregate_over=config,
            )
            view.aggregated_features.add(feat)

        return view

    @classmethod
    def query(cls) -> FeatureViewStore:
        from aligned import FeatureStore

        self = cls()
        store = FeatureStore.experimental()
        store.add_feature_view(self)
        return store.feature_view(self.metadata.name)"
174;aligned/feature_view/tests/test_hidden_variable.py;beta;"class TestView(FeatureView):

    metadata = FeatureViewMetadata(name='test', description='test', tags={}, batch_source=source)

    test_id = Entity(String())

    variable = String()
    some_bool = Bool()

    is_not_true = (~(variable == 'true')) & some_bool
    is_not_true_other = some_bool & (~(variable == 'true'))
    is_true = variable == 'True'

    y_value = Float()
    x_value = Float()

    some_ratio = (y_value - x_value) / x_value


@pytest.mark.asyncio
async def test_hidden_variable() -> None:

    view = TestView.compile()

    assert len(view.derived_features) == 9


@pytest.mark.asyncio
async def test_select_variables() -> None:

    view = TestView.compile()

    assert len(view.derived_features) == 9

    request = view.request_for({'some_ratio'})

    assert len(request.needed_requests) == 1
    needed_req = request.needed_requests[0]
    assert len(needed_req.derived_features) == 2"
175;aligned/local/job.py;beta;"class LiteralRetrivalJob(RetrivalJob):

    df: pl.LazyFrame
    result: RequestResult

    def __init__(self, df: pl.LazyFrame | pd.DataFrame, result: RequestResult) -> None:
        self.result = result
        if isinstance(df, pd.DataFrame):
            self.df = pl.from_pandas(df).lazy()
        else:
            self.df = df

    @property
    def request_result(self) -> RequestResult:
        return self.result

    async def to_pandas(self) -> pd.DataFrame:
        return self.df.collect().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        return self.df"
176;aligned/local/job.py;beta;"class FileFullJob(FullExtractJob):

    source: DataFileReference
    request: RetrivalRequest
    limit: int | None = field(default=None)

    @property
    def request_result(self) -> RequestResult:
        return self.request.request_result

    def file_transformations(self, df: pd.DataFrame) -> pd.DataFrame:
        from aligned.data_source.batch_data_source import ColumnFeatureMappable

        entity_names = self.request.entity_names
        all_names = list(self.request.all_required_feature_names.union(entity_names))

        request_features = all_names
        if isinstance(self.source, ColumnFeatureMappable):
            request_features = self.source.feature_identifier_for(all_names)

        df = df.rename(
            columns={org_name: wanted_name for org_name, wanted_name in zip(request_features, all_names)},
        )

        if self.limit and df.shape[0] > self.limit:
            return df.iloc[: self.limit]
        else:
            return df

    def file_transform_polars(self, df: pl.LazyFrame) -> pl.LazyFrame:
        from aligned.data_source.batch_data_source import ColumnFeatureMappable

        entity_names = self.request.entity_names
        all_names = list(self.request.all_required_feature_names.union(entity_names))

        request_features = all_names
        if isinstance(self.source, ColumnFeatureMappable):
            request_features = self.source.feature_identifier_for(all_names)
        renames = {
            org_name: wanted_name
            for org_name, wanted_name in zip(request_features, all_names)
            if org_name != wanted_name
        }
        df = df.rename(mapping=renames)

        if self.limit:
            return df.limit(self.limit)
        else:
            return df

    async def to_pandas(self) -> pd.DataFrame:
        file = await self.source.read_pandas()
        return self.file_transformations(file)

    async def to_polars(self) -> pl.LazyFrame:
        file = await self.source.to_polars()
        return self.file_transform_polars(file)"
177;aligned/local/job.py;beta;"class FileDateJob(DateRangeJob):

    source: DataFileReference
    request: RetrivalRequest
    start_date: datetime
    end_date: datetime

    @property
    def request_result(self) -> RequestResult:
        return self.request.request_result

    def file_transformations(self, df: pd.DataFrame) -> pd.DataFrame:
        from aligned.data_source.batch_data_source import ColumnFeatureMappable

        entity_names = self.request.entity_names
        all_names = list(self.request.all_required_feature_names.union(entity_names))

        request_features = all_names
        if isinstance(self.source, ColumnFeatureMappable):
            request_features = self.source.feature_identifier_for(all_names)

        df.rename(
            columns={org_name: wanted_name for org_name, wanted_name in zip(request_features, all_names)},
            inplace=True,
        )

        event_timestamp_column = self.request.event_timestamp.name
        # Making sure it is in the correct format
        df[event_timestamp_column] = pd.to_datetime(
            df[event_timestamp_column], infer_datetime_format=True, utc=True
        )

        start_date_ts = pd.to_datetime(self.start_date, utc=True)
        end_date_ts = pd.to_datetime(self.end_date, utc=True)
        return df.loc[df[event_timestamp_column].between(start_date_ts, end_date_ts)]

    def file_transform_polars(self, df: pl.LazyFrame) -> pl.LazyFrame:
        from aligned.data_source.batch_data_source import ColumnFeatureMappable

        entity_names = self.request.entity_names
        all_names = list(self.request.all_required_feature_names.union(entity_names))

        request_features = all_names
        if isinstance(self.source, ColumnFeatureMappable):
            request_features = self.source.feature_identifier_for(all_names)

        df = df.rename(
            mapping={org_name: wanted_name for org_name, wanted_name in zip(request_features, all_names)}
        )
        event_timestamp_column = self.request.event_timestamp.name

        return df.filter(pl.col(event_timestamp_column).is_between(self.start_date, self.end_date))

    async def to_pandas(self) -> pd.DataFrame:
        file = await self.source.read_pandas()
        return self.file_transformations(file)

    async def to_polars(self) -> pl.LazyFrame:
        file = await self.source.to_polars()
        return self.file_transform_polars(file)"
178;aligned/local/job.py;beta;"class FileFactualJob(FactualRetrivalJob):

    source: DataFileReference
    requests: list[RetrivalRequest]
    facts: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.requests)

    async def file_transformations(self, df: pl.LazyFrame) -> pl.LazyFrame:
        """"""Selects only the wanted subset from the loaded source

        ```python
        await self.file_transformations(await self.source.to_polars())
        ```

        Args:
            df (pl.LazyFrame): The loaded file source which contains all features

        Returns:
            pl.LazyFrame: The subset of the source which is needed for the request
        """"""
        from aligned.data_source.batch_data_source import ColumnFeatureMappable

        all_features: set[Feature] = set()
        for request in self.requests:
            all_features.update(request.all_required_features)

        result = await self.facts.to_polars()
        event_timestamp_col = 'event_timestamp'
        row_id_name = 'row_id'
        result = result.with_row_count(row_id_name)

        for request in self.requests:
            entity_names = request.entity_names
            all_names = request.all_required_feature_names.union(entity_names)

            if request.event_timestamp:
                all_names.add(request.event_timestamp.name)

            request_features = all_names
            if isinstance(self.source, ColumnFeatureMappable):
                request_features = self.source.feature_identifier_for(all_names)

            feature_df = df.select(request_features)

            renames = {org_name: wanted_name for org_name, wanted_name in zip(request_features, all_names)}
            feature_df = feature_df.rename(renames)

            for entity in request.entities:
                feature_df = feature_df.with_column(pl.col(entity.name).cast(entity.dtype.polars_type))
                result = result.with_column(pl.col(entity.name).cast(entity.dtype.polars_type))

            column_selects = list(entity_names.union({'row_id'}))
            if request.event_timestamp:
                column_selects.append('event_timestamp')

            # Need to only select the relevent entities and row_id
            # Otherwise will we get a duplicate column error
            # We also need to remove the entities after the row_id is joined
            new_result: pl.LazyFrame = result.select(column_selects).join(
                feature_df, on=list(entity_names), how='left'
            )
            new_result = new_result.select(pl.exclude(list(entity_names)))

            if request.event_timestamp:
                new_result = new_result.with_columns(
                    pl.col(request.event_timestamp.name)
                    .str.strptime(pl.Datetime, '%+')
                    .alias(request.event_timestamp.name)
                )
                field = request.event_timestamp.name
                ttl = request.event_timestamp.ttl
                if ttl:
                    ttl_request = (pl.col(field) <= pl.col(event_timestamp_col)) & (
                        pl.col(field) >= pl.col(event_timestamp_col) - ttl
                    )
                    new_result = new_result.filter(pl.col(field).is_null() | ttl_request)
                else:
                    new_result = new_result.filter(
                        pl.col(field).is_null() | (pl.col(field) <= pl.col(event_timestamp_col))
                    )

            unique = new_result.unique(subset=row_id_name, keep='first')
            result = result.join(unique, on=row_id_name, how='left')
            result = result.select(pl.exclude('.*_right$'))

        return result.select([pl.exclude('row_id')])

    async def to_pandas(self) -> pd.DataFrame:
        return (await self.to_polars()).collect().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        return await self.file_transformations(await self.source.to_polars())"
179;aligned/psql/jobs.py;beta;"class SQLQuery:
    sql: str"
180;aligned/psql/jobs.py;beta;"class SqlColumn:
    selection: str
    alias: str

    @property
    def sql_select(self) -> str:
        selection = self.selection
        # if not special operation e.g function. Then wrap in quotes
        if not ('(' in selection or '-' in selection or '.' in selection or selection == '*'):
            selection = f'""{self.selection}""'

        if self.selection == self.alias:
            return f'{selection}'
        return f'{selection} AS ""{self.alias}""'

    def __hash__(self) -> int:
        return hash(self.sql_select)"
181;aligned/psql/jobs.py;beta;"class SqlJoin:
    table: str
    conditions: list[str]"
182;aligned/psql/jobs.py;beta;"class TableFetch:
    name: str
    id_column: str
    table: str | TableFetch
    columns: set[SqlColumn]
    joins: list[str] = field(default_factory=list)
    conditions: list[str] = field(default_factory=list)
    group_by: list[str] = field(default_factory=list)
    order_by: str | None = field(default=None)

    def sql_query(self, distinct: str | None = None) -> str:
        # Select the core features
        wheres = ''
        order_by = ''
        group_by = ''
        select = 'SELECT'

        if distinct:
            select = f'SELECT DISTINCT ON ({distinct})'

        if self.conditions:
            wheres = 'WHERE ' + ' AND '.join(self.conditions)

        if self.order_by:
            order_by = 'ORDER BY ' + self.order_by

        if self.group_by:
            group_by = 'GROUP BY ' + ', '.join(self.group_by)

        table_columns = [col.sql_select for col in self.columns]

        if isinstance(self.table, TableFetch):
            from_sql = f'FROM ({self.table.sql_query()}) as entities'
        else:
            from_sql = f""""""FROM entities
    LEFT JOIN ""{ self.table }"" ta ON { ' AND '.join(self.joins) }""""""

        return f""""""
    { select } { ', '.join(table_columns) }
    { from_sql }
    { wheres }
    { order_by }
    { group_by }"""""""
183;aligned/psql/jobs.py;beta;"class PostgreSqlJob(RetrivalJob):

    config: PostgreSQLConfig
    query: str
    requests: list[RetrivalRequest] = field(default_factory=list)

    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.retrival_requests)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.requests

    async def to_pandas(self) -> pd.DataFrame:
        df = await self.to_polars()
        return df.collect().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        try:
            return pl.read_sql(self.query, self.config.url).lazy()
        except Exception as e:
            logger.error(f'Error running query: {self.query}')
            logger.error(f'Error: {e}')
            raise e

    def describe(self) -> str:
        return f'PostgreSQL Job: \n{self.query}\n'"
184;aligned/psql/jobs.py;beta;"class FullExtractPsqlJob(FullExtractJob):

    source: PostgreSQLDataSource
    request: RetrivalRequest
    limit: int | None = None

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request(self.request)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return [self.request]

    @property
    def config(self) -> PostgreSQLConfig:
        return self.source.config

    def describe(self) -> str:
        return self.psql_job().describe()

    async def to_pandas(self) -> pd.DataFrame:
        return await self.psql_job().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        return await self.psql_job().to_polars()

    def psql_job(self) -> PostgreSqlJob:
        return PostgreSqlJob(self.config, self.build_request())

    def build_request(self) -> str:

        all_features = [
            feature.name for feature in list(self.request.all_required_features.union(self.request.entities))
        ]
        sql_columns = self.source.feature_identifier_for(all_features)
        columns = [
            f'""{sql_col}"" AS {alias}' if sql_col != alias else sql_col
            for sql_col, alias in zip(sql_columns, all_features)
        ]
        column_select = ', '.join(columns)
        schema = f'{self.config.schema}.' if self.config.schema else ''

        limit_query = ''
        if self.limit:
            limit_query = f'LIMIT {int(self.limit)}'

        return f'SELECT {column_select} FROM {schema}""{self.source.table}"" {limit_query}'"
185;aligned/psql/jobs.py;beta;"class DateRangePsqlJob(DateRangeJob):

    source: PostgreSQLDataSource
    start_date: datetime
    end_date: datetime
    request: RetrivalRequest

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request(self.request)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return [self.request]

    @property
    def config(self) -> PostgreSQLConfig:
        return self.source.config

    async def to_pandas(self) -> pd.DataFrame:
        return await self.psql_job().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        return await self.psql_job().to_polars()

    def psql_job(self) -> PostgreSqlJob:
        return PostgreSqlJob(self.config, self.build_request())

    def build_request(self) -> str:

        if not self.request.event_timestamp:
            raise ValueError('Event timestamp is needed in order to run a data range job')

        event_timestamp_column = self.source.feature_identifier_for([self.request.event_timestamp.name])[0]
        all_features = [
            feature.name for feature in list(self.request.all_required_features.union(self.request.entities))
        ]
        sql_columns = self.source.feature_identifier_for(all_features)
        columns = [
            f'""{sql_col}"" AS {alias}' if sql_col != alias else sql_col
            for sql_col, alias in zip(sql_columns, all_features)
        ]
        column_select = ', '.join(columns)
        schema = f'{self.config.schema}.' if self.config.schema else ''
        start_date = self.start_date.strftime('%Y-%m-%d %H:%M:%S')
        end_date = self.end_date.strftime('%Y-%m-%d %H:%M:%S')

        return (
            f'SELECT {column_select} FROM {schema}""{self.source.table}"" WHERE'
            f' {event_timestamp_column} BETWEEN \'{start_date}\' AND \'{end_date}\''
        )"
186;aligned/psql/jobs.py;beta;"class SqlValue:
    value: str | None
    data_type: str

    @property
    def to_sql(self) -> str:
        if self.value:
            return f""'{self.value}'::{self.data_type}""
        else:
            return f'NULL::{self.data_type}'"
187;aligned/psql/jobs.py;beta;"class FactPsqlJob(FactualRetrivalJob):
    """"""Fetches features for defined facts within a postgres DB

    It is supported to fetch from different tables, in one request
    This is hy the `source` property is a dict with sources

    NB: It is expected that the data sources are for the same psql instance
    """"""

    sources: dict[FeatureLocation, PostgreSQLDataSource]
    requests: list[RetrivalRequest]
    facts: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.requests)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.requests

    @property
    def config(self) -> PostgreSQLConfig:
        return list(self.sources.values())[0].config

    def describe(self) -> str:
        if isinstance(self.facts, PostgreSqlJob):
            psql_job = self.build_sql_entity_query(self.facts)
            return f'Loading features for {self.facts.describe()}\n\nQuery: {psql_job}'
        else:
            raise ValueError(
                'Only PostgreSqlJob is supported as facts when describing,'
                f'but fetching features for facts: {self.facts.describe()}'
            )

    async def to_pandas(self) -> pd.DataFrame:
        job = await self.psql_job()
        return await job.to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        job = await self.psql_job()
        return await job.to_polars()

    async def psql_job(self) -> PostgreSqlJob:
        if isinstance(self.facts, PostgreSqlJob):
            return PostgreSqlJob(self.config, self.build_sql_entity_query(self.facts))
        return PostgreSqlJob(self.config, await self.build_request())

    def dtype_to_sql_type(self, dtype: object) -> str:
        if isinstance(dtype, str):
            return dtype
        if dtype == FeatureType('').string:
            return 'text'
        if dtype == FeatureType('').uuid:
            return 'uuid'
        if dtype == FeatureType('').int32 or dtype == FeatureType('').int64:
            return 'integer'
        if dtype == FeatureType('').datetime:
            return 'TIMESTAMP WITH TIME ZONE'
        return 'uuid'

    def value_selection(self, request: RetrivalRequest, entities_has_event_timestamp: bool) -> TableFetch:

        source = self.sources[request.location]

        entity_selects = {f'entities.{entity}' for entity in request.entity_names}
        field_selects = request.all_required_feature_names.union(entity_selects).union({'entities.row_id'})
        field_identifiers = source.feature_identifier_for(field_selects)
        selects = {
            SqlColumn(db_field_name, feature)
            for feature, db_field_name in zip(field_selects, field_identifiers)
        }

        entities = list(request.entity_names)
        entity_db_name = source.feature_identifier_for(entities)
        sort_query = 'entities.row_id'

        event_timestamp_clause: str | None = None
        if request.event_timestamp and entities_has_event_timestamp:
            event_timestamp_column = source.feature_identifier_for([request.event_timestamp.name])[0]
            event_timestamp_clause = f'entities.event_timestamp >= ta.{event_timestamp_column}'
            sort_query += f', {event_timestamp_column} DESC'

        join_conditions = [
            f'ta.""{entity_db_name}"" = entities.{entity}'
            for entity, entity_db_name in zip(entities, entity_db_name)
        ]
        if event_timestamp_clause:
            join_conditions.append(event_timestamp_clause)

        rename_fetch = TableFetch(
            name=f'{request.name}_cte',
            id_column='row_id',
            table=source.table,
            columns=selects,
            joins=join_conditions,
            order_by=sort_query,
        )

        derived_features = [
            feature
            for feature in request.derived_features
            if isinstance(feature.transformation, PsqlTransformation)
            and all([field in field_selects for field in feature.depending_on_names])
        ]
        if derived_features:
            derived_alias = source.feature_identifier_for([feature.name for feature in derived_features])
            derived_selects = {
                SqlColumn(feature.transformation.as_psql(), name)
                for feature, name in zip(derived_features, derived_alias)
                if isinstance(feature.transformation, PsqlTransformation)
            }.union({SqlColumn('*', '*')})

            return TableFetch(
                name=rename_fetch.name,
                id_column=rename_fetch.id_column,
                table=rename_fetch,
                columns=derived_selects,
            )
        else:
            return rename_fetch

    def sql_aggregated_request(
        self, window: AggregateOver, features: set[AggregatedFeature], request: RetrivalRequest
    ) -> TableFetch:
        source = self.sources[request.location]
        name = f'{request.name}_agg_cte'

        if not all(
            [isinstance(feature.derived_feature.transformation, PsqlTransformation) for feature in features]
        ):
            raise ValueError('All features must have a PsqlTransformation')

        group_by_names = {feature.name for feature in window.group_by}
        if window.window:
            time_window_config = window.window
            time_window = int(time_window_config.time_window.total_seconds())
            name = f'{request.name}_agg_{time_window}_cte'
            group_by_names = {'entities.row_id'}

        group_by_selects = {SqlColumn(feature, feature) for feature in group_by_names}

        aggregates = {
            SqlColumn(
                feature.derived_feature.transformation.as_psql(),
                feature.name,
            )
            for feature in features
        }

        id_column = window.group_by[0].name
        event_timestamp_clause: str | None = None
        if request.event_timestamp:
            id_column = 'row_id'
            group_by_names = {id_column}
            # Use row_id as the main join key
            event_timestamp_name = source.feature_identifier_for([request.event_timestamp.name])[0]
            if window.window:
                time_window_config = window.window
                window_in_seconds = int(time_window_config.time_window.total_seconds())
                event_timestamp_clause = (
                    f'ta.{event_timestamp_name} BETWEEN entities.event_timestamp'
                    f"" - interval '{window_in_seconds} seconds' AND entities.event_timestamp""
                )
            else:
                event_timestamp_clause = f'ta.{event_timestamp_name} <= entities.event_timestamp'

        entities = list(request.entity_names)
        entity_db_name = source.feature_identifier_for(entities)
        join_conditions = [
            f'ta.""{entity_db_name}"" = entities.{entity}'
            for entity, entity_db_name in zip(entities, entity_db_name)
        ]
        if event_timestamp_clause:
            join_conditions.append(event_timestamp_clause)

        field_selects = request.all_required_feature_names.union({'entities.*'})
        field_identifiers = source.feature_identifier_for(field_selects)
        selects = {
            SqlColumn(db_field_name, feature)
            for feature, db_field_name in zip(field_selects, field_identifiers)
        }

        rename_table = TableFetch(
            name='ta',
            id_column='row_id',
            table=source.table,
            columns=selects,
            joins=join_conditions,
        )

        needed_derived_features = set()
        derived_map = request.derived_feature_map()

        for agg_feature in request.aggregated_features:
            for depended in agg_feature.depending_on_names:
                if depended in derived_map:
                    needed_derived_features.add(derived_map[depended])

        if needed_derived_features:
            features = {
                SqlColumn(feature.transformation.as_psql(), feature.name)
                for feature in needed_derived_features
            }.union(
                {SqlColumn('*', '*')}
            )  # Need the * in order to select the ""original values""
            select_table = TableFetch(
                name='derived',
                id_column=rename_table.id_column,
                table=rename_table,
                columns=features,
            )
        else:
            select_table = rename_table

        return TableFetch(
            name=name,
            id_column=id_column,
            # Need to do a subquery, in order to renmae the core features
            table=select_table,
            columns=aggregates.union(group_by_selects),
            group_by=list(group_by_names),
        )

    def aggregated_values_from_request(self, request: RetrivalRequest) -> list[TableFetch]:

        aggregation_windows: dict[AggregateOver, set[AggregatedFeature]] = {}

        for aggregate in request.aggregated_features:
            if aggregate.aggregate_over not in aggregation_windows:
                aggregation_windows[aggregate.aggregate_over] = {aggregate}
            else:
                aggregation_windows[aggregate.aggregate_over].add(aggregate)

        fetches: list[TableFetch] = []
        supported_aggregation_features = set(request.feature_names).union(request.entity_names)
        for feature in request.derived_features:
            if isinstance(feature.transformation, PsqlTransformation):
                supported_aggregation_features.add(feature.name)

        for window, aggregates in aggregation_windows.items():
            needed_features = set()

            for agg in aggregates:
                for depended_feature in agg.derived_feature.depending_on:
                    needed_features.add(depended_feature.name)

            missing_features = needed_features - supported_aggregation_features
            if not missing_features:
                fetches.append(self.sql_aggregated_request(window, aggregates, request))
            else:
                raise ValueError(
                    f'Only SQL aggregates are supported at the moment. Missing features {missing_features}'
                )
                # fetches.append(self.fetch_all_aggregate_values(window, aggregates, request))

        return fetches

    async def build_request(self) -> str:
        import numpy as np

        final_select_names: set[str] = set()
        entity_types: dict[str, FeatureType] = {}
        has_event_timestamp = False

        for request in self.requests:
            final_select_names = final_select_names.union(
                {f'{request.location.name}_cte.{feature}' for feature in request.all_required_feature_names}
            )
            final_select_names = final_select_names.union(
                {f'entities.{entity}' for entity in request.entity_names}
            )
            for entity in request.entities:
                entity_types[entity.name] = entity.dtype
            if request.event_timestamp:
                has_event_timestamp = True

        if has_event_timestamp:
            final_select_names.add('event_timestamp')
            entity_types['event_timestamp'] = FeatureType('').datetime

        # Need to replace nan as it will not be encoded
        fact_df = await self.facts.to_pandas()
        fact_df = fact_df.replace(np.nan, None)

        number_of_values = fact_df.shape[0]
        # + 1 is needed as 0 is evaluated for null
        fact_df['row_id'] = list(range(1, number_of_values + 1))

        entity_type_list = [
            self.dtype_to_sql_type(entity_types.get(entity, FeatureType('').int32))
            for entity in fact_df.columns
        ]

        query_values: list[list[SqlValue]] = []
        all_entities = []
        for values in fact_df.values:
            row_placeholders = []
            for column_index, value in enumerate(values):
                row_placeholders.append(SqlValue(value, entity_type_list[column_index]))
                if fact_df.columns[column_index] not in all_entities:
                    all_entities.append(fact_df.columns[column_index])
            query_values.append(row_placeholders)

        feature_view_names: list[str] = [location.name for location in self.sources.keys()]
        # Add the joins to the fact

        tables: list[TableFetch] = []
        aggregates: list[TableFetch] = []
        for request in self.requests:
            fetch = self.value_selection(request, has_event_timestamp)
            tables.append(fetch)
            aggregate_fetches = self.aggregated_values_from_request(request)
            aggregates.extend(aggregate_fetches)
            for aggregate in aggregate_fetches:
                final_select_names = final_select_names.union(
                    {column.alias for column in aggregate.columns if column.alias != aggregate.id_column}
                )

        joins = '\n    '.join(
            [
                f'LEFT JOIN {feature_view}_cte ON {feature_view}_cte.row_id = entities.row_id'
                for feature_view in feature_view_names
            ]
        )
        if aggregates:
            joins += '\n    '
            joins += '\n    '.join(
                [
                    f'LEFT JOIN {table.name} ON {table.name}.{table.id_column} = entities.{table.id_column}'
                    for table in aggregates
                ]
            )

        entity_values = self.build_entities_from_values(query_values)

        return self.generate_query(
            entity_columns=list(all_entities),
            entity_query=entity_values,
            tables=tables,
            aggregates=aggregates,
            final_select=list(final_select_names),
            final_joins=joins,
        )

    def build_entities_from_values(self, values: list[list[SqlValue]]) -> str:
        query = 'VALUES '
        for row in values:
            query += '\n    ('
            for value in row:
                query += value.to_sql + ', '
            query = query[:-2]
            query += '),'
        query = query[:-1]
        return query

    def build_sql_entity_query(self, sql_facts: PostgreSqlJob) -> str:

        final_select_names: set[str] = set()
        has_event_timestamp = False
        all_entities = set()

        if 'event_timestamp' in sql_facts.query:
            has_event_timestamp = True
            all_entities.add('event_timestamp')

        for request in self.requests:
            final_select_names = final_select_names.union(
                {f'entities.{entity}' for entity in request.entity_names}
            )

        if has_event_timestamp:
            final_select_names.add('event_timestamp')

        # Add the joins to the fact

        tables: list[TableFetch] = []
        aggregates: list[TableFetch] = []
        for request in self.requests:
            all_entities.update(request.entity_names)

            if request.aggregated_features:
                aggregate_fetches = self.aggregated_values_from_request(request)
                aggregates.extend(aggregate_fetches)
                for aggregate in aggregate_fetches:
                    final_select_names = final_select_names.union(
                        {column.alias for column in aggregate.columns if column.alias != 'entites.row_id'}
                    )
            else:
                fetch = self.value_selection(request, has_event_timestamp)
                tables.append(fetch)
                final_select_names = final_select_names.union(
                    {f'{fetch.name}.{feature}' for feature in request.all_required_feature_names}
                )

        all_entities_list = list(all_entities)
        all_entities_str = ', '.join(all_entities_list)
        all_entities_list.append('row_id')

        entity_query = (
            f'SELECT {all_entities_str}, ROW_NUMBER() OVER (ORDER BY '
            f'{list(request.entity_names)[0]}) AS row_id FROM ({sql_facts.query}) AS entities'
        )
        joins = '\n    '.join(
            [f'LEFT JOIN {table.name} ON {table.name}.row_id = entities.row_id' for table in tables]
        )
        if aggregates:
            joins += '\n    '
            joins += '\n    '.join(
                [
                    f'LEFT JOIN {table.name} ON {table.name}.{table.id_column} = entities.{table.id_column}'
                    for table in aggregates
                ]
            )

        return self.generate_query(
            entity_columns=all_entities_list,
            entity_query=entity_query,
            tables=tables,
            aggregates=aggregates,
            final_select=list(final_select_names),
            final_joins=joins,
        )

    def generate_query(
        self,
        entity_columns: list[str],
        entity_query: str,
        tables: list[TableFetch],
        aggregates: list[TableFetch],
        final_select: list[str],
        final_joins: str,
    ) -> str:

        query = f""""""
WITH entities (
    { ', '.join(entity_columns) }
) AS (
    { entity_query }
),""""""

        # Select the core features
        for table in tables:
            query += f""""""
{table.name} AS (
    { table.sql_query(distinct='entities.row_id') }
    ),""""""

        # Select the aggregate features
        for table in aggregates:
            query += f""""""
{table.name} AS (
    { table.sql_query() }
    ),""""""

        query = query[:-1]  # Dropping the last comma
        query += f""""""
SELECT { ', '.join(final_select) }
FROM entities
{ final_joins }
""""""
        return query"
188;aligned/redis/job.py;beta;"class FactualRedisJob(FactualRetrivalJob):

    config: RedisConfig
    requests: list[RetrivalRequest]
    facts: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.requests)

    async def to_pandas(self) -> pd.DataFrame:
        return (await self.to_polars()).collect().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        redis = self.config.redis()

        result_df = (await self.facts.to_polars()).collect()

        for request in self.requests:
            redis_combine_id = 'redis_combine_entity_id'
            entities = result_df.select(
                [
                    (
                        pl.lit(request.location.identifier)
                        + pl.lit(':')
                        + pl.concat_str(sorted(request.entity_names))
                    ).alias(redis_combine_id),
                    pl.col(list(request.entity_names)),
                ]
            ).filter(pl.col(redis_combine_id).is_not_null())

            if entities.shape[0] == 0:
                # Do not connect to redis if there are no entities to fetch
                result_df = result_df.with_columns(
                    [pl.lit(None).alias(column.name) for column in request.all_features]
                )
                continue

            features = list({feature.name for feature in request.returned_features})

            async with redis.pipeline(transaction=False) as pipe:
                for entity in entities[redis_combine_id]:
                    pipe.hmget(entity, keys=features)
                result = await pipe.execute()

            reqs: pl.DataFrame = pl.concat(
                [entities, pl.DataFrame(result, columns=features, orient='row')], how='horizontal'
            ).select(pl.exclude(redis_combine_id))

            for feature in request.returned_features:
                if feature.dtype == FeatureType('').bool:
                    reqs = reqs.with_column(pl.col(feature.name).cast(pl.Int8).cast(pl.Boolean))
                elif reqs[feature.name].dtype == pl.Utf8 and (
                    feature.dtype == FeatureType('').int32 or feature.dtype == FeatureType('').int64
                ):
                    reqs = reqs.with_column(
                        pl.col(feature.name)
                        .str.splitn('.', 2)
                        .struct.field('field_0')
                        .cast(feature.dtype.polars_type)
                        .alias(feature.name)
                    )
                elif feature.dtype == FeatureType('').embedding or feature.dtype == FeatureType('').array:
                    import numpy as np

                    reqs = reqs.with_column(pl.col(feature.name).apply(lambda row: np.frombuffer(row)))
                else:
                    reqs = reqs.with_column(pl.col(feature.name).cast(feature.dtype.polars_type))
                # if feature.dtype == FeatureType('').datetime:
                #     dates = pd.to_datetime(result_series[result_value_mask], unit='s', utc=True)
                #     result_df.loc[set_mask, feature.name] = dates
                # elif feature.dtype == FeatureType('').embedding:
                #     result_df.loc[set_mask, feature.name] = (
                #         result_series[result_value_mask].str.split(',')
                # .apply(lambda x: [float(i) for i in x])
                #     )

            result_df = result_df.join(reqs, on=list(request.entity_names), how='left')

        return result_df.lazy()"
189;aligned/redis/stream.py;beta;"class RedisStream:

    client: Redis

    async def read_from_timestamp(
        self, streams: dict[str, str], count: int | None = None, timeout: int | None = 5000
    ) -> list[tuple[str, list[tuple[str, dict[str, str]]]]]:
        """"""Read different streams from a given timestamp.

        Args:
            streams (dict[str, str]): The streams, and the last id to read from
            count (int | None, optional): The max number of values to return.
            Defaults to None.
            timeout (int | None, optional): The amount of milliseconds to block for a read.
            Defaults to 5000, aka 5 sec.

        Returns:
            list[tuple[str, list[tuple[str, dict[str, str]]]]]: The returned streams with their values
        """"""
        return await self.client.xread(streams, count=count, block=timeout)

    async def read_newest(
        self, streams: list[str], count: int | None = None, timeout: int | None = 5000
    ) -> list[tuple[str, list[tuple[str, dict[str, str]]]]]:
        """"""Read the values that gets pushed to the stream from now.

        Args:
            streams (list[str]): The streams to read from
            count (int | None, optional): The max number of values to return. Defaults to None.
            timeout (int | None, optional): The amount of milliseconds to block for a read.
             Defaults to 5000, aka 5 sec.

        Returns:
            list[tuple[str, list[tuple[str, dict[str, str]]]]]: The streams with their values
        """"""
        return await self.read_from_timestamp(
            {stream: '$' for stream in streams}, count=count, timeout=timeout
        )

    # The following methods is used to support a fan out.
    # this is not supported yet

    # async def create_consumer_group(self, group: str) -> None:
    #     await self.client.xgroup_create(self.channel, group)

    # async def read_group(self, group: str, consumer_name: str) -> list[Any]:
    #     return await self.client.xreadgroup(group, consumer_name, {self.channel: "">""})

    # async def acknowledge_message_for(self, group: str, messages: list[str]) -> None:
    #     await self.client.xack(self.channel, group, *messages)"
190;aligned/redshift/jobs.py;beta;"class SQLQuery:
    sql: str"
191;aligned/redshift/jobs.py;beta;"class SqlColumn:
    selection: str
    alias: str

    @property
    def sql_select(self) -> str:
        if self.selection == self.alias:
            return f'{self.selection}'
        return f'{self.selection} AS ""{self.alias}""'

    def __hash__(self) -> int:
        return hash(self.sql_select)"
192;aligned/redshift/jobs.py;beta;"class SqlJoin:
    table: str
    conditions: list[str]"
193;aligned/redshift/jobs.py;beta;"class TableFetch:
    name: str
    id_column: str
    table: str | TableFetch
    columns: set[SqlColumn]
    schema: str | None = field(default=None)
    joins: list[str] = field(default_factory=list)
    conditions: list[str] = field(default_factory=list)
    group_by: list[str] = field(default_factory=list)
    order_by: str | None = field(default=None)

    def sql_query(self, distinct: str | None = None) -> str:
        # Select the core features
        wheres = ''
        order_by = ''
        group_by = ''
        select = 'SELECT'

        if self.conditions:
            wheres = 'WHERE ' + ' AND '.join(self.conditions)

        if self.order_by:
            order_by = 'ORDER BY ' + self.order_by

        if self.group_by:
            group_by = 'GROUP BY ' + ', '.join(self.group_by)

        table_columns = [col.sql_select for col in self.columns]

        if isinstance(self.table, TableFetch):
            from_sql = f'FROM ({self.table.sql_query()}) as entities'
        else:
            schema = f'{self.schema}.' if self.schema else ''
            from_sql = f""""""FROM entities
        LEFT JOIN {schema}""{ self.table }"" ta ON { ' AND '.join(self.joins) }""""""

        if distinct:
            aliases = [col.alias for col in self.columns]
            return f""""""
            SELECT { ', '.join(aliases) }
            FROM (
        { select } { ', '.join(table_columns) },
            ROW_NUMBER() OVER(
                PARTITION BY entities.row_id
                { order_by }
            ) AS row_number
            { from_sql }
            { wheres }
            { order_by }
            { group_by }
        ) AS entities
        WHERE row_number = 1""""""
        else:
            return f""""""
        { select } { ', '.join(table_columns) }
        { from_sql }
        { wheres }
        { order_by }
        { group_by }"""""""
194;aligned/redshift/jobs.py;beta;"class FullExtractPsqlJob(FullExtractJob):

    source: PostgreSQLDataSource
    request: RetrivalRequest
    limit: int | None = None

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request(self.request)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return [self.request]

    @property
    def config(self) -> PostgreSQLConfig:
        return self.source.config

    async def to_pandas(self) -> pd.DataFrame:
        return await self.psql_job().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        return await self.psql_job().to_polars()

    def psql_job(self) -> PostgreSqlJob:
        return PostgreSqlJob(self.config, self.build_request())

    def build_request(self) -> str:

        all_features = [
            feature.name for feature in list(self.request.all_required_features.union(self.request.entities))
        ]
        sql_columns = self.source.feature_identifier_for(all_features)
        columns = [
            f'""{sql_col}"" AS {alias}' if sql_col != alias else sql_col
            for sql_col, alias in zip(sql_columns, all_features)
        ]
        column_select = ', '.join(columns)
        schema = f'{self.config.schema}.' if self.config.schema else ''

        limit_query = ''
        if self.limit:
            limit_query = f'LIMIT {int(self.limit)}'

        f'SELECT {column_select} FROM {schema}""{self.source.table}"" {limit_query}',"
195;aligned/redshift/jobs.py;beta;"class DateRangePsqlJob(DateRangeJob):

    source: PostgreSQLDataSource
    start_date: datetime
    end_date: datetime
    request: RetrivalRequest

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request(self.request)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return [self.request]

    @property
    def config(self) -> PostgreSQLConfig:
        return self.source.config

    async def to_pandas(self) -> pd.DataFrame:
        return await self.psql_job().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        return await self.psql_job().to_polars()

    def psql_job(self) -> PostgreSqlJob:
        return PostgreSqlJob(self.config, self.build_request())

    def build_request(self) -> str:

        if not self.request.event_timestamp:
            raise ValueError('Event timestamp is needed in order to run a data range job')

        event_timestamp_column = self.source.feature_identifier_for([self.request.event_timestamp.name])[0]
        all_features = [
            feature.name for feature in list(self.request.all_required_features.union(self.request.entities))
        ]
        sql_columns = self.source.feature_identifier_for(all_features)
        columns = [
            f'""{sql_col}"" AS {alias}' if sql_col != alias else sql_col
            for sql_col, alias in zip(sql_columns, all_features)
        ]
        column_select = ', '.join(columns)
        schema = f'{self.config.schema}.' if self.config.schema else ''
        start_date = self.start_date.strftime('%Y-%m-%d %H:%M:%S')
        end_date = self.end_date.strftime('%Y-%m-%d %H:%M:%S')

        return (
            f'SELECT {column_select} FROM {schema}""{self.source.table}"" WHERE'
            f' {event_timestamp_column} BETWEEN \'{start_date}\' AND \'{end_date}\''
        )"
196;aligned/redshift/jobs.py;beta;"class SqlValue:
    value: str | None
    data_type: str

    @property
    def to_sql(self) -> str:
        if self.value:
            return f""'{self.value}'::{self.data_type}""
        else:
            return f'NULL::{self.data_type}'"
197;aligned/redshift/jobs.py;beta;"class FactRedshiftJob(FactualRetrivalJob):
    """"""Fetches features for defined facts within a postgres DB

    It is supported to fetch from different tables, in one request
    This is hy the `source` property is a dict with sources

    NB: It is expected that the data sources are for the same psql instance
    """"""

    sources: dict[FeatureLocation, PostgreSQLDataSource]
    requests: list[RetrivalRequest]
    facts: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.requests)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.requests

    @property
    def config(self) -> PostgreSQLConfig:
        return list(self.sources.values())[0].config

    async def to_pandas(self) -> pd.DataFrame:
        job = await self.psql_job()
        return await job.to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        job = await self.psql_job()
        return await job.to_polars()

    async def psql_job(self) -> PostgreSqlJob:
        if isinstance(self.facts, PostgreSqlJob):
            return PostgreSqlJob(self.config, self.build_sql_entity_query(self.facts))
        raise ValueError(f'Redshift only support SQL entity queries. Got: {self.facts}')

    def describe(self) -> str:
        if isinstance(self.facts, PostgreSqlJob):
            return PostgreSqlJob(self.config, self.build_sql_entity_query(self.facts)).describe()
        raise ValueError(f'Redshift only support SQL entity queries. Got: {self.facts}')

    def dtype_to_sql_type(self, dtype: object) -> str:
        if isinstance(dtype, str):
            return dtype
        if dtype == FeatureType('').string:
            return 'text'
        if dtype == FeatureType('').uuid:
            return 'uuid'
        if dtype == FeatureType('').int32 or dtype == FeatureType('').int64:
            return 'integer'
        if dtype == FeatureType('').datetime:
            return 'TIMESTAMP WITH TIME ZONE'
        return 'uuid'

    def value_selection(self, request: RetrivalRequest, entities_has_event_timestamp: bool) -> TableFetch:

        source = self.sources[request.location]

        entity_selects = {f'entities.{entity}' for entity in request.entity_names}
        field_selects = request.all_required_feature_names.union(entity_selects).union({'entities.row_id'})
        field_identifiers = source.feature_identifier_for(field_selects)
        selects = {
            SqlColumn(db_field_name, feature)
            for feature, db_field_name in zip(field_selects, field_identifiers)
        }

        entities = list(request.entity_names)
        entity_db_name = source.feature_identifier_for(entities)
        sort_query = 'entities.row_id'

        event_timestamp_clause: str | None = None
        if request.event_timestamp and entities_has_event_timestamp:
            event_timestamp_column = source.feature_identifier_for([request.event_timestamp.name])[0]
            event_timestamp_clause = f'entities.event_timestamp >= ta.{event_timestamp_column}'
            sort_query += f', {event_timestamp_column} DESC'

        join_conditions = [
            f'ta.""{entity_db_name}"" = entities.{entity}'
            for entity, entity_db_name in zip(entities, entity_db_name)
        ]
        if event_timestamp_clause:
            join_conditions.append(event_timestamp_clause)

        rename_fetch = TableFetch(
            name=f'{request.name}_cte',
            id_column='row_id',
            table=source.table,
            columns=selects,
            joins=join_conditions,
            order_by=sort_query,
            schema=self.config.schema,
        )

        derived_map = request.derived_feature_map()
        derived_features = [
            feature
            for feature in request.derived_features
            if isinstance(feature.transformation, RedshiftTransformation)
            and all([name not in derived_map for name in feature.depending_on_names])
        ]
        if derived_features:
            derived_alias = source.feature_identifier_for([feature.name for feature in derived_features])
            derived_selects = {
                SqlColumn(feature.transformation.as_redshift(), name)
                for feature, name in zip(derived_features, derived_alias)
                if isinstance(feature.transformation, RedshiftTransformation)
            }.union({SqlColumn('*', '*')})

            return TableFetch(
                name=rename_fetch.name,
                id_column=rename_fetch.id_column,
                table=rename_fetch,
                columns=derived_selects,
            )
        else:
            return rename_fetch

    def sql_aggregated_request(
        self, window: AggregateOver, features: set[AggregatedFeature], request: RetrivalRequest
    ) -> TableFetch:
        source = self.sources[request.location]
        name = f'{request.name}_agg_cte'

        if not all(
            [
                isinstance(feature.derived_feature.transformation, RedshiftTransformation)
                for feature in features
            ]
        ):
            raise ValueError('All features must have a RedshiftTransformation')

        group_by_names = {feature.name for feature in window.group_by}
        if window.window:
            time_window_config = window.window
            time_window = int(time_window_config.time_window.total_seconds())
            name = f'{request.name}_agg_{time_window}_cte'
            group_by_names = {'entities.row_id'}

        aggregates = {
            SqlColumn(
                feature.derived_feature.transformation.as_redshift(),
                feature.name,
            )
            for feature in features
        }

        id_column = window.group_by[0].name
        event_timestamp_clause: str | None = None
        if request.event_timestamp:
            id_column = 'row_id'
            group_by_names = {'entities.row_id'}
            # Use row_id as the main join key
            event_timestamp_name = source.feature_identifier_for([request.event_timestamp.name])[0]
            if window.window:
                time_window_config = window.window
                window_in_seconds = int(time_window_config.time_window.total_seconds())
                event_timestamp_clause = (
                    f'ta.{event_timestamp_name} BETWEEN entities.event_timestamp'
                    f"" - interval '{window_in_seconds} seconds' AND entities.event_timestamp""
                )
            else:
                event_timestamp_clause = f'ta.{event_timestamp_name} <= entities.event_timestamp'

        group_by_selects = {SqlColumn(feature, feature) for feature in group_by_names}

        entities = list(request.entity_names)
        entity_db_name = source.feature_identifier_for(entities)
        join_conditions = [
            f'ta.""{entity_db_name}"" = entities.{entity}'
            for entity, entity_db_name in zip(entities, entity_db_name)
        ]
        if event_timestamp_clause:
            join_conditions.append(event_timestamp_clause)

        field_selects = request.all_required_feature_names.union({'entities.*'})
        field_identifiers = source.feature_identifier_for(field_selects)
        selects = {
            SqlColumn(db_field_name, feature)
            for feature, db_field_name in zip(field_selects, field_identifiers)
        }

        rename_table = TableFetch(
            name='ta',
            id_column='row_id',
            table=source.table,
            columns=selects,
            joins=join_conditions,
            schema=source.config.schema,
        )

        needed_derived_features: set[DerivedFeature] = set()
        derived_map = request.derived_feature_map()

        def resolve_sql_transformation(feature: str) -> str:
            """"""Create the sql query for hidden features.

            This will loop through the hidden features,
            and ""concat"" the sql query for each hidden feature.

            e.g:
            1 + x as my_feature
            => (z + y) + x as my_feature
            """"""
            hidden_feature = derived_map[feature]
            assert isinstance(hidden_feature.transformation, RedshiftTransformation)
            sql_query = hidden_feature.transformation.as_redshift()
            for depended in hidden_feature.depending_on_names:
                if depended not in derived_map:
                    continue
                sub_feature = derived_map[depended]
                if depended.isnumeric():
                    assert isinstance(sub_feature.transformation, RedshiftTransformation)
                    hidden_sql = sub_feature.transformation.as_redshift()
                    hidden_sql = resolve_sql_transformation(sub_feature.name)
                    sql_query = sql_query.replace(depended, f'({hidden_sql})')

            return sql_query

        for agg_feature in request.aggregated_features:
            for depended in agg_feature.depending_on_names:
                if depended in derived_map:  # if it is a derived feature
                    needed_derived_features.add(derived_map[depended])

        if needed_derived_features:
            features = {
                SqlColumn(resolve_sql_transformation(feature.name), feature.name)
                for feature in needed_derived_features
            }.union(
                {SqlColumn('*', '*')}
            )  # Need the * in order to select the ""original values""
            select_table = TableFetch(
                name='derived',
                id_column=rename_table.id_column,
                table=rename_table,
                columns=features,
            )
        else:
            select_table = rename_table

        return TableFetch(
            name=name,
            id_column=id_column,
            # Need to do a subquery, in order to renmae the core features
            table=select_table,
            columns=aggregates.union(group_by_selects),
            schema=source.config.schema,
            group_by=list(group_by_names),
        )

    def aggregated_values_from_request(self, request: RetrivalRequest) -> list[TableFetch]:

        aggregation_windows: dict[AggregateOver, set[AggregatedFeature]] = {}

        for aggregate in request.aggregated_features:
            if aggregate.aggregate_over not in aggregation_windows:
                aggregation_windows[aggregate.aggregate_over] = {aggregate}
            else:
                aggregation_windows[aggregate.aggregate_over].add(aggregate)

        fetches: list[TableFetch] = []
        supported_aggregation_features = set(request.feature_names).union(request.entity_names)
        for feature in request.derived_features:
            if isinstance(feature.transformation, RedshiftTransformation):
                supported_aggregation_features.add(feature.name)

        for window, aggregates in aggregation_windows.items():
            needed_features = set()

            for agg in aggregates:
                for depended_feature in agg.derived_feature.depending_on:
                    needed_features.add(depended_feature.name)

            missing_features = needed_features - supported_aggregation_features
            if not missing_features:
                fetches.append(self.sql_aggregated_request(window, aggregates, request))
            else:
                raise ValueError(
                    f'Only SQL aggregates are supported at the moment. Missing features {missing_features}'
                )

        return fetches

    def build_sql_entity_query(self, sql_facts: PostgreSqlJob) -> str:

        final_select_names: set[str] = set()
        has_event_timestamp = False
        all_entities = set()

        if 'event_timestamp' in sql_facts.query:
            has_event_timestamp = True
            all_entities.add('event_timestamp')

        for request in self.requests:
            final_select_names = final_select_names.union(
                {f'entities.{entity}' for entity in request.entity_names}
            )

        if has_event_timestamp:
            final_select_names.add('event_timestamp')

        # Add the joins to the fact

        tables: list[TableFetch] = []
        aggregates: list[TableFetch] = []
        for request in self.requests:

            all_entities.update(request.entity_names)

            if request.aggregated_features:
                aggregate_fetches = self.aggregated_values_from_request(request)
                aggregates.extend(aggregate_fetches)

                for aggregate in aggregate_fetches:
                    agg_features = {
                        f'{aggregate.name}.{column.alias}'
                        for column in aggregate.columns
                        if column.alias != 'entites.row_id' and column.alias not in all_entities
                    }
                    final_select_names = final_select_names.union(agg_features)
            else:
                fetch = self.value_selection(request, has_event_timestamp)
                tables.append(fetch)
                final_select_names = final_select_names.union(
                    {f'{fetch.name}.{feature}' for feature in request.all_required_feature_names}
                )

        all_entities_list = list(all_entities)
        all_entities_str = ', '.join(all_entities_list)
        all_entities_list.append('row_id')

        entity_query = (
            f'SELECT {all_entities_str}, ROW_NUMBER() OVER (ORDER BY '
            f'{list(request.entity_names)[0]}) AS row_id FROM ({sql_facts.query}) AS entities'
        )
        joins = '\n    '.join(
            [f'LEFT JOIN {table.name} ON {table.name}.row_id = entities.row_id' for table in tables]
        )
        if aggregates:
            joins += '\n    '
            joins += '\n    '.join(
                [
                    f'LEFT JOIN {table.name} ON {table.name}.{table.id_column} = entities.{table.id_column}'
                    for table in aggregates
                ]
            )

        return self.generate_query(
            entity_columns=all_entities_list,
            entity_query=entity_query,
            tables=tables,
            aggregates=aggregates,
            final_select=list(final_select_names),
            final_joins=joins,
            event_timestamp_col='event_timestamp' if has_event_timestamp else None,
        )

    def generate_query(
        self,
        entity_columns: list[str],
        entity_query: str,
        tables: list[TableFetch],
        aggregates: list[TableFetch],
        final_select: list[str],
        final_joins: str,
        event_timestamp_col: str | None,
    ) -> str:

        query = f""""""
WITH entities (
    { ', '.join(entity_columns) }
) AS (
    { entity_query }
),""""""

        # Select the core features
        for table in tables:
            query += f""""""
{table.name} AS (
    { table.sql_query(distinct='entities.row_id') }
    ),""""""

        # Select the aggregate features
        for table in aggregates:
            query += f""""""
{table.name} AS (
    { table.sql_query() }
    ),""""""

        query = query[:-1]  # Dropping the last comma
        if event_timestamp_col:

            query += f""""""
SELECT val.*
FROM (
    SELECT { ', '.join(final_select) }
    FROM (
        SELECT *, ROW_NUMBER() OVER(
            PARTITION BY entities.row_id
            ORDER BY entities.""{event_timestamp_col}"" DESC
        ) AS row_number
        FROM entities
    ) as entities
        { final_joins }
    WHERE entities.row_number = 1
) as val
""""""
        else:
            query += f""""""
SELECT { ', '.join(final_select) }
FROM entities
    { final_joins }
""""""
        return query"
198;aligned/request/retrival_request.py;beta;"class RetrivalRequest(Codable):
    """"""
    Describes all the information needed for a request to be successful.

    This do not mean all the data is shown to the end user,
    as there may be some features that depend on other features.
    """"""

    name: str
    location: FeatureLocation
    entities: set[Feature]
    features: set[Feature]
    derived_features: set[DerivedFeature]
    aggregated_features: set[AggregatedFeature] = field(default_factory=set)
    event_timestamp: EventTimestamp | None = field(default=None)

    features_to_include: set[str] = field(default_factory=set)

    def __init__(
        self,
        name: str,
        location: FeatureLocation,
        entities: set[Feature],
        features: set[Feature],
        derived_features: set[DerivedFeature],
        aggregated_features: set[AggregatedFeature] | None = None,
        event_timestamp: EventTimestamp | None = None,
        features_to_include: set[str] | None = None,
    ):
        self.name = name
        self.location = location
        self.entities = entities
        self.features = features
        self.derived_features = derived_features
        self.aggregated_features = aggregated_features or set()
        self.event_timestamp = event_timestamp
        self.features_to_include = features_to_include or self.all_feature_names

    def filter_features(self, feature_names: set[str]) -> 'RetrivalRequest':
        return RetrivalRequest(
            name=self.name,
            location=self.location,
            entities=self.entities,
            features=self.features,
            derived_features=self.derived_features,
            aggregated_features=self.aggregated_features,
            event_timestamp=self.event_timestamp,
            features_to_include=feature_names,
        )

    @property
    def returned_features(self) -> set[Feature]:
        return {feature for feature in self.all_features if feature.name in self.features_to_include}

    @property
    def feature_names(self) -> list[str]:
        return [feature.name for feature in self.features]

    @property
    def request_result(self) -> 'RequestResult':
        return RequestResult.from_request(self)

    def derived_feature_map(self) -> dict[str, DerivedFeature]:
        return {
            feature.name: feature for feature in self.derived_features.union(self.derived_aggregated_features)
        }

    @property
    def derived_aggregated_features(self) -> set[DerivedFeature]:
        return {feature.derived_feature for feature in self.aggregated_features}

    @property
    def all_required_features(self) -> set[Feature]:
        return self.features - self.entities

    @property
    def all_required_feature_names(self) -> set[str]:
        return {feature.name for feature in self.all_required_features}

    @property
    def all_features(self) -> set[Feature]:
        return self.features.union(self.derived_features).union(
            {feature.derived_feature for feature in self.aggregated_features}
        )

    @property
    def all_feature_names(self) -> set[str]:
        return {feature.name for feature in self.all_features}

    @property
    def entity_names(self) -> set[str]:
        return {entity.name for entity in self.entities}

    def derived_features_order(self) -> list[set[DerivedFeature]]:
        from collections import defaultdict

        feature_deps = defaultdict(set)
        feature_orders: list[set] = []
        feature_map = self.derived_feature_map()
        features = self.derived_features
        dependent_features = self.derived_features.copy()

        while dependent_features:
            feature = dependent_features.pop()
            for dep_ref in feature.depending_on:
                if dep_ref.name == feature.name:
                    continue

                if dep := feature_map.get(dep_ref.name):
                    feature_deps[feature.name].add(dep)
                    if dep.name not in feature_deps:
                        dependent_features.add(dep)
                        feature_map[dep.name] = dep

        for feature in features:
            depth = feature.depth
            while depth >= len(feature_orders):
                feature_orders.append(set())
            feature_orders[depth].add(feature_map[feature.name])

        return feature_orders

    def aggregate_over(self) -> dict[AggregateOver, set[AggregatedFeature]]:
        features = defaultdict(set)
        for feature in self.aggregated_features:
            features[feature.aggregate_over].add(feature)
        return features

    def without_event_timestamp(self, name_sufix: str | None = None) -> 'RetrivalRequest':
        return RetrivalRequest(
            name=f'{self.name}{name_sufix or """"}',
            location=self.location,
            entities=self.entities,
            features=self.features,
            derived_features=self.derived_features,
            aggregated_features=self.aggregated_features,
        )

    @staticmethod
    def combine(requests: list['RetrivalRequest']) -> list['RetrivalRequest']:
        grouped_requests: dict[FeatureLocation, RetrivalRequest] = {}
        returned_features: dict[FeatureLocation, set[Feature]] = {}
        entities = set()
        for request in requests:
            entities.update(request.entities)
            fv_name = request.location
            if fv_name not in grouped_requests:
                grouped_requests[fv_name] = RetrivalRequest(
                    name=request.name,
                    location=fv_name,
                    entities=request.entities,
                    features=request.features,
                    derived_features=request.derived_features,
                    aggregated_features=request.aggregated_features,
                    event_timestamp=request.event_timestamp,
                )
                returned_features[fv_name] = request.returned_features
            else:
                grouped_requests[fv_name].derived_features.update(request.derived_features)
                grouped_requests[fv_name].features.update(request.features)
                grouped_requests[fv_name].aggregated_features.update(request.aggregated_features)
                grouped_requests[fv_name].entities.update(request.entities)
                returned_features[fv_name].update(request.returned_features)

        for request in grouped_requests.values():
            request.features_to_include = request.features_to_include.union(
                request.all_feature_names - {feature.name for feature in returned_features[request.location]}
            )

        return list(grouped_requests.values())

    @staticmethod
    def unsafe_combine(requests: list['RetrivalRequest']) -> list['RetrivalRequest']:

        result_request = RetrivalRequest(
            name=requests[0].name,
            location=requests[0].location,
            entities=set(),
            features=set(),
            derived_features=set(),
            aggregated_features=set(),
            event_timestamp=None,
        )
        for request in requests:
            result_request.derived_features.update(request.derived_features)
            result_request.features.update(request.features)
            result_request.entities.update(request.entities)
            result_request.aggregated_features.update(request.aggregated_features)

        return result_request"
199;aligned/request/retrival_request.py;beta;"class RequestResult(Codable):
    """"""
    Describes the returend response of a request
    """"""

    entities: set[Feature]
    features: set[Feature]
    event_timestamp: str | None

    @property
    def feature_columns(self) -> list[str]:
        return [feature.name for feature in self.features]

    @property
    def entity_columns(self) -> list[str]:
        return [entity.name for entity in self.entities]

    def __add__(self, obj: 'RequestResult') -> 'RequestResult':
        return RequestResult(
            entities=self.entities.union(obj.entities),
            features=self.features.union(obj.features),
            event_timestamp='event_timestamp' if self.event_timestamp or obj.event_timestamp else None,
        )

    def filter_features(self, features_to_include: set[str]) -> 'RequestResult':
        return RequestResult(
            entities=self.entities,
            features={feature for feature in self.features if feature.name in features_to_include},
            event_timestamp=self.event_timestamp,
        )

    @staticmethod
    def from_request(request: RetrivalRequest) -> 'RequestResult':
        return RequestResult(
            entities=request.entities,
            features=request.all_features - request.entities,
            event_timestamp=request.event_timestamp.name if request.event_timestamp else None,
        )

    @staticmethod
    def from_request_list(requests: list[RetrivalRequest]) -> 'RequestResult':
        request_len = len(requests)
        if request_len == 0:
            return RequestResult(entities=set(), features=set(), event_timestamp=None)
        elif request_len > 1:
            return RequestResult(
                entities=set().union(*[request.entities for request in requests]),
                features=set().union(
                    *[
                        {
                            feature
                            for feature in request.all_features
                            if feature.name in request.features_to_include
                        }
                        - request.entities
                        for request in requests
                    ]
                ),
                event_timestamp='event_timestamp'
                if any(request.event_timestamp for request in requests)
                else None,
            )
        else:
            return RequestResult.from_request(requests[0])

    @staticmethod
    def from_result_list(requests: list['RequestResult']) -> 'RequestResult':
        request_len = len(requests)
        if request_len == 0:
            return RequestResult(entities=set(), features=set(), event_timestamp=None)
        elif request_len > 1:
            return RequestResult(
                entities=set().union(*[request.entities for request in requests]),
                features=set().union(*[request.features for request in requests]),
                event_timestamp='event_timestamp'
                if any(request.event_timestamp for request in requests)
                else None,
            )
        else:
            return requests[0]"
200;aligned/request/retrival_request.py;beta;"class FeatureRequest(Codable):
    """"""Representing a request of a set of features
    This data"
201;aligned/request/retrival_request.py;beta;"class would be used to represent which
    features to fetch for a given model.

    It would therefore contain the different features that is for the endgoal.
    But also which features that is needed in order to get the wanted features.

    E.g:
    Let's say we have the following feature view:

    ```"
202;aligned/request/retrival_request.py;beta;"class TitanicPassenger(FeatureView):

        ... # The metadata and entity is unrelevent here

        sex = String()
        is_male, is_female = sex.one_hot_encode([""male"", ""female""])
    ```

    If we ask for only the feature `is_male` the this data"
203;aligned/request/retrival_request.py;beta;"class would contain

    name = the name of the feature view it originates from, or some something random
    features_to_include = {'is_male'}
    needed_requests = [
        features={'sex'}, # would fetch only the sex feature, as `is_male` relies on it
        derived_features={'is_male'} # The feature to be computed
    ]
    """"""

    location: FeatureLocation
    features_to_include: set[str]
    needed_requests: list[RetrivalRequest]

    @property
    def needs_event_timestamp(self) -> bool:
        return any(request.event_timestamp for request in self.needed_requests)

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.needed_requests)

    def without_event_timestamp(self, name_sufix: str | None = None) -> 'FeatureRequest':
        return FeatureRequest(
            location=self.location,
            features_to_include=self.features_to_include,
            needed_requests=[request.without_event_timestamp(name_sufix) for request in self.needed_requests],
        )"
204;aligned/retrival_job.py;beta;"class SupervisedJob:

    job: RetrivalJob
    target_columns: set[str]

    async def to_pandas(self) -> SupervisedDataSet[pd.DataFrame]:
        data = await self.job.to_pandas()
        features = {
            feature.name
            for feature in self.job.request_result.features
            if feature.name not in self.target_columns
        }
        entities = {feature.name for feature in self.job.request_result.entities}
        return SupervisedDataSet(
            data, entities, features, self.target_columns, self.job.request_result.event_timestamp
        )

    async def to_polars(self) -> SupervisedDataSet[pl.LazyFrame]:
        data = await self.job.to_polars()
        features = [
            feature.name
            for feature in self.job.request_result.features
            if feature.name not in self.target_columns
        ]
        entities = [feature.name for feature in self.job.request_result.entities]
        return SupervisedDataSet(
            data, set(entities), set(features), self.target_columns, self.job.request_result.event_timestamp
        )

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    def train_set(self, train_size: float) -> SupervisedTrainJob:
        return SupervisedTrainJob(self, train_size)

    def with_subfeatures(self) -> SupervisedJob:
        return SupervisedJob(self.job.with_subfeatures(), self.target_columns)

    def cached_at(self, location: DataFileReference | str) -> SupervisedJob:
        return SupervisedJob(
            self.job.cached_at(location),
            self.target_columns,
        )

    def validate(self, validator: Validator) -> SupervisedJob:
        return SupervisedJob(
            self.job.validate(validator),
            self.target_columns,
        )

    def log_each_job(self) -> SupervisedJob:
        return SupervisedJob(
            self.job.log_each_job(),
            self.target_columns,
        )

    def describe(self) -> str:
        return f'{self.job.describe()} with target columns {self.target_columns}'"
205;aligned/retrival_job.py;beta;"class SupervisedTrainJob:

    job: SupervisedJob
    train_size: float

    async def to_pandas(self) -> TrainTestSet[pd.DataFrame]:
        core_data = await self.job.to_polars()
        data = core_data.data.collect()
        data = data.to_pandas()

        test_ratio_start = self.train_size
        return TrainTestSet(
            data=data,
            entity_columns=core_data.entity_columns,
            features=core_data.features,
            target_columns=core_data.target_columns,
            train_index=split(data, 0, test_ratio_start, core_data.event_timestamp_column),
            test_index=split(data, test_ratio_start, 1, core_data.event_timestamp_column),
            event_timestamp_column=core_data.event_timestamp_column,
        )

    async def to_polars(self) -> TrainTestSet[pl.DataFrame]:
        # Use the pandas method, as the split is not created for polars yet
        # A but unsure if I should use the same index concept for polars
        pandas_data = await self.to_pandas()
        return TrainTestSet(
            data=pl.from_pandas(pandas_data.data),
            entity_columns=pandas_data.entity_columns,
            features=pandas_data.features,
            target_columns=pandas_data.target_columns,
            train_index=pandas_data.train_index,
            test_index=pandas_data.test_index,
            event_timestamp_column=pandas_data.event_timestamp_column,
        )

    def validation_set(self, validation_size: float) -> SupervisedValidationJob:
        return SupervisedValidationJob(self, validation_size)"
206;aligned/retrival_job.py;beta;"class SupervisedValidationJob:

    job: SupervisedTrainJob
    validation_size: float

    async def to_pandas(self) -> TrainTestValidateSet[pd.DataFrame]:
        data = await self.job.to_pandas()

        test_start = self.job.train_size
        validate_start = test_start + self.validation_size

        return TrainTestValidateSet(
            data=data.data,
            entity_columns=set(data.entity_columns),
            features=data.features,
            target=data.target_columns,
            train_index=split(data.data, 0, test_start, data.event_timestamp_column),
            test_index=split(data.data, test_start, validate_start, data.event_timestamp_column),
            validate_index=split(data.data, validate_start, 1, data.event_timestamp_column),
            event_timestamp_column=data.event_timestamp_column,
        )

    async def to_polars(self) -> TrainTestValidateSet[pl.DataFrame]:
        data = await self.to_pandas()

        return TrainTestValidateSet(
            data=pl.from_pandas(data.data),
            entity_columns=data.entity_columns,
            features=data.features,
            target=data.target,
            train_index=data.train_index,
            test_index=data.test_index,
            validate_index=data.validate_index,
            event_timestamp_column=data.event_timestamp_column,
        )"
207;aligned/retrival_job.py;beta;"class RetrivalJob(ABC):
    @property
    def request_result(self) -> RequestResult:
        if isinstance(self, ModificationJob):
            return self.job.request_result
        raise NotImplementedError()

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        if isinstance(self, ModificationJob):
            return self.job.retrival_requests
        raise NotImplementedError()

    @abstractmethod
    async def to_pandas(self) -> pd.DataFrame:
        raise NotImplementedError()

    @abstractmethod
    async def to_polars(self) -> pl.LazyFrame:
        raise NotImplementedError()

    def describe(self) -> str:
        if isinstance(self, ModificationJob):
            return f'{self.job.describe()} -> {self.__class__.__name__}'
        raise NotImplementedError(f'Describe not implemented for {self.__class__.__name__}')

    def remove_derived_features(self) -> RetrivalJob:
        if isinstance(self, ModificationJob):
            return self.copy_with(self.job.remove_derived_features())
        return self

    def log_each_job(self) -> RetrivalJob:
        if isinstance(self, ModificationJob):
            return self.copy_with(self.job.log_each_job())
        return LogJob(self)

    def chuncked(self, size: int) -> DataLoaderJob:
        return DataLoaderJob(self, size)

    def with_subfeatures(self) -> RetrivalJob:
        if isinstance(self, ModificationJob):
            return self.copy_with(self.job.with_subfeatures())
        return self

    def cached_at(self, location: DataFileReference | str) -> RetrivalJob:
        if isinstance(location, str):
            from aligned.sources.local import ParquetFileSource

            return FileCachedJob(ParquetFileSource(location), self)
        else:
            return FileCachedJob(location, self)

    def test_size(self, test_size: float, target_column: str) -> SupervisedTrainJob:
        return SupervisedJob(self, {target_column}).train_set(train_size=1 - test_size)

    def train_set(self, train_size: float, target_column: str) -> SupervisedTrainJob:
        return SupervisedJob(self, {target_column}).train_set(train_size=train_size)

    def validate(self, validator: Validator) -> RetrivalJob:
        return ValidationJob(self, validator)

    def derive_features(self, requests: list[RetrivalRequest]) -> RetrivalJob:
        return DerivedFeatureJob(job=self, requests=requests)

    def ensure_types(self, requests: list[RetrivalRequest]) -> RetrivalJob:
        return EnsureTypesJob(job=self, requests=requests)

    def filter(self, include_features: set[str]) -> RetrivalJob:
        return FilterJob(include_features, self)

    def listen_to_events(self, events: set[EventTrigger]) -> RetrivalJob:
        return ListenForTriggers(self, events)

    def update_vector_index(self, indexes: list[VectorIndex]) -> RetrivalJob:
        return UpdateVectorIndexJob(self, indexes)

    def validate_entites(self) -> RetrivalJob:
        return ValidateEntitiesJob(self)

    def fill_missing_columns(self) -> RetrivalJob:
        return FillMissingColumnsJob(self)

    @staticmethod
    def from_dict(data: dict[str, list], request: list[RetrivalRequest] | RetrivalRequest) -> RetrivalJob:
        if isinstance(request, RetrivalRequest):
            request = [request]
        return LiteralDictJob(data, request)

    @staticmethod
    def from_polars_df(df: pl.DataFrame, request: list[RetrivalRequest]) -> RetrivalJob:
        from aligned.local.job import LiteralRetrivalJob

        return LiteralRetrivalJob(df.lazy(), RequestResult.from_request_list(request))


JobType = TypeVar('JobType')"
208;aligned/retrival_job.py;beta;"class ModificationJob:

    job: RetrivalJob

    def copy_with(self: JobType, job: RetrivalJob) -> JobType:
        self.job = job  # type: ignore
        return self"
209;aligned/retrival_job.py;beta;"class UpdateVectorIndexJob(RetrivalJob, ModificationJob):

    job: RetrivalJob
    indexes: list[VectorIndex]

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    async def to_pandas(self) -> pd.DataFrame:
        raise NotImplementedError()

    async def to_polars(self) -> pl.LazyFrame:
        data = await self.job.to_polars()

        update_jobs = []
        for index in self.indexes:

            select = index.entities + index.metadata + [index.vector]
            select_names = {feat.name for feat in select}

            filter_expr = pl.lit(True)
            for entity in index.entities:
                filter_expr = filter_expr & pl.col(entity.name).is_not_null()

            filtered_data = data.select(select_names).filter(filter_expr)
            update_jobs.append(index.storage.upsert_polars(filtered_data, index))

        await asyncio.gather(*update_jobs)
        return data"
210;aligned/retrival_job.py;beta;"class LiteralDictJob(RetrivalJob):

    data: dict[str, list]
    requests: list[RetrivalRequest]

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_request_list(self.requests)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.requests

    async def to_pandas(self) -> pd.DataFrame:
        return pd.DataFrame(self.data)

    async def to_polars(self) -> pl.LazyFrame:
        return pl.DataFrame(self.data).lazy()"
211;aligned/retrival_job.py;beta;"class LogJob(RetrivalJob, ModificationJob):

    job: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    async def to_pandas(self) -> pd.DataFrame:
        df = await self.job.to_pandas()
        logger.info(f'Results from {type(self.job)}')
        logger.info(df)
        return df

    async def to_polars(self) -> pl.LazyFrame:
        df = await self.job.to_polars()
        logger.info(f'Results from {type(self.job)}')
        logger.info(df.head(10).collect())
        return df

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()

    def log_each_job(self) -> RetrivalJob:
        return self.job"
212;aligned/retrival_job.py;beta;"class ValidationJob(RetrivalJob, ModificationJob):

    job: RetrivalJob
    validator: Validator

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    @property
    def features_to_validate(self) -> set[Feature]:
        return RequestResult.from_request_list(
            [request for request in self.retrival_requests if not request.aggregated_features]
        ).features

    async def to_pandas(self) -> pd.DataFrame:
        return await self.validator.validate_pandas(
            list(self.features_to_validate), await self.job.to_pandas()
        )

    async def to_polars(self) -> pl.LazyFrame:
        return await self.validator.validate_polars(
            list(self.features_to_validate), await self.job.to_polars()
        )

    def with_subfeatures(self) -> RetrivalJob:
        return ValidationJob(self.job.with_subfeatures(), self.validator)

    def cached_at(self, location: DataFileReference | str) -> RetrivalJob:
        if isinstance(location, str):
            from aligned.sources.local import ParquetFileSource

            return FileCachedJob(ParquetFileSource(location), self)
        else:
            return FileCachedJob(location, self)

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
213;aligned/retrival_job.py;beta;"class DerivedFeatureJob(RetrivalJob, ModificationJob):

    job: RetrivalJob
    requests: list[RetrivalRequest]

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    async def compute_derived_features_polars(self, df: pl.LazyFrame) -> pl.LazyFrame:

        for request in self.requests:
            for feature_round in request.derived_features_order():

                round_expressions: list[pl.Expr] = []

                for feature in feature_round:
                    if feature.name in df.columns:
                        logger.info(f'Skipped adding feature {feature.name} to computation plan')
                        continue
                    logger.info(f'Adding feature to computation plan in polars: {feature.name}')

                    method = await feature.transformation.transform_polars(df, feature.name)
                    if isinstance(method, pl.LazyFrame):
                        df = method
                    elif isinstance(method, pl.Expr):
                        round_expressions.append(method.alias(feature.name))
                    else:
                        raise ValueError('Invalid result from transformation')

                if round_expressions:
                    df = df.with_columns(round_expressions)
        return df

    async def compute_derived_features_pandas(self, df: pd.DataFrame) -> pd.DataFrame:
        for request in self.requests:
            for feature_round in request.derived_features_order():
                for feature in feature_round:
                    if feature.name in df.columns:
                        logger.info(f'Skipping to compute {feature.name} as it is aleady computed')
                        continue
                    logger.info(f'Computing feature with pandas: {feature.name}')
                    df[feature.name] = await feature.transformation.transform_pandas(
                        df[feature.depending_on_names]
                    )
                    if df[feature.name].dtype != feature.dtype.pandas_type:
                        if feature.dtype.is_numeric:
                            df[feature.name] = pd.to_numeric(df[feature.name], errors='coerce').astype(
                                feature.dtype.pandas_type
                            )
                        else:
                            df[feature.name] = df[feature.name].astype(feature.dtype.pandas_type)
        return df

    async def to_pandas(self) -> pd.DataFrame:
        return await self.compute_derived_features_pandas(await self.job.to_pandas())

    async def to_polars(self) -> pl.LazyFrame:
        return await self.compute_derived_features_polars(await self.job.to_polars())

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
214;aligned/retrival_job.py;beta;"class ValidateEntitiesJob(RetrivalJob, ModificationJob):

    job: RetrivalJob

    async def to_pandas(self) -> pd.DataFrame:
        data = await self.job.to_pandas()

        for request in self.retrival_requests:
            if request.entity_names - set(data.columns):
                return pd.DataFrame({})

        return data

    async def to_polars(self) -> pl.DataFrame:
        data = await self.job.to_polars()

        for request in self.retrival_requests:
            if request.entity_names - set(data.columns):
                return pl.DataFrame({}).lazy()

        return data"
215;aligned/retrival_job.py;beta;"class FillMissingColumnsJob(RetrivalJob, ModificationJob):

    job: RetrivalJob

    async def to_pandas(self) -> pd.DataFrame:
        data = await self.job.to_pandas()
        for request in self.retrival_requests:

            missing = request.all_required_feature_names - set(data.columns)
            if not missing:
                continue

            logger.info(
                f""""""
Some features is missing.
Will fill values with None, but it could be a potential problem: {missing}
""""""
            )
            for feature in missing:
                data[feature] = None
        return data

    async def to_polars(self) -> pl.LazyFrame:
        data = await self.job.to_polars()
        for request in self.retrival_requests:

            missing = request.all_required_feature_names - set(data.columns)
            if not missing:
                continue

            logger.info(
                f""""""
Some features is missing.
Will fill values with None, but it could be a potential problem: {missing}
""""""
            )
            data = data.with_columns([pl.lit(None).alias(feature) for feature in missing])
        return data"
216;aligned/retrival_job.py;beta;"class StreamAggregationJob(RetrivalJob, ModificationJob):

    job: RetrivalJob
    checkpoints: dict[AggregateOver, DataFileReference]

    @property
    def time_windows(self) -> set[AggregateOver]:
        windows = set()
        for request in self.retrival_requests:
            for feature in request.aggregated_features:
                windows.add(feature.aggregate_over)
        return windows

    @property
    def aggregated_features(self) -> dict[AggregateOver, set[AggregatedFeature]]:
        features = defaultdict(set)
        for request in self.retrival_requests:
            for feature in request.aggregated_features:
                features[feature.aggregate_over].add(feature)
        return features

    async def data_windows(self, window: AggregateOver, data: pl.DataFrame, now: datetime) -> pl.DataFrame:
        checkpoint = self.checkpoints[window]
        filter_expr: pl.Expr | None = None

        if window.window:
            time_window = window.window
            filter_expr = pl.col(time_window.time_column.name) > now - time_window.time_window
        if window.condition:
            raise ValueError('Condition is not supported for stream aggregation, yet')

        try:
            window_data = (await checkpoint.to_polars()).collect()

            if filter_expr is not None:
                new_data = pl.concat([window_data.filter(filter_expr), data.filter(filter_expr)])
            else:
                new_data = pl.concat([window_data, data])

            await checkpoint.write_polars(new_data.lazy())
            return new_data
        except FileNotFoundError:

            if filter_expr is not None:
                window_data = data.filter(filter_expr)
            else:
                window_data = data

            await checkpoint.write_polars(window_data.lazy())
            return window_data

    async def to_pandas(self) -> pd.DataFrame:
        raise NotImplementedError()

    async def to_polars(self) -> pl.LazyFrame:
        data = (await self.job.to_polars()).collect()

        # This is used as a dummy frame, as the pl abstraction is not good enough
        lazy_df = pl.DataFrame({}).lazy()
        now = datetime.utcnow()

        for window in self.time_windows:

            aggregations = self.aggregated_features[window]

            required_features = set(window.group_by).union([window.window.time_column])
            for agg in aggregations:
                required_features.update(agg.derived_feature.depending_on)

            required_features_name = sorted({feature.name for feature in required_features})

            agg_transformations = await asyncio.gather(
                *[
                    agg.derived_feature.transformation.transform_polars(lazy_df, 'dummy')
                    for agg in aggregations
                ]
            )
            agg_expr = [
                agg.alias(feature.name)
                for agg, feature in zip(agg_transformations, aggregations)
                if isinstance(agg, pl.Expr)
            ]

            window_data = await self.data_windows(window, data.select(required_features_name), now)

            agg_data = window_data.lazy().groupby(window.group_by_names).agg(agg_expr).collect()
            data = data.join(agg_data, on=window.group_by_names, how='left')

        return data.lazy()

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
217;aligned/retrival_job.py;beta;"class DataLoaderJob:

    job: RetrivalJob
    chunk_size: int

    async def to_polars(self) -> AsyncIterator[pl.LazyFrame]:
        from math import ceil

        from aligned.local.job import LiteralRetrivalJob

        needed_requests = self.job.retrival_requests
        without_derived = self.job.remove_derived_features()
        raw_files = (await without_derived.to_polars()).collect()
        features_to_include = self.job.request_result.features.union(self.job.request_result.entities)
        features_to_include_names = {feature.name for feature in features_to_include}

        iterations = ceil(raw_files.shape[0] / self.chunk_size)
        for i in range(iterations):
            start = i * self.chunk_size
            end = (i + 1) * self.chunk_size
            df = raw_files[start:end, :]

            chunked_job = (
                LiteralRetrivalJob(df.lazy(), RequestResult.from_request_list(needed_requests))
                .derive_features(needed_requests)
                .filter(features_to_include_names)
            )

            chunked_df = await chunked_job.to_polars()
            yield chunked_df

    async def to_pandas(self) -> AsyncIterator[pd.DataFrame]:
        async for chunk in self.to_polars():
            yield chunk.collect().to_pandas()"
218;aligned/retrival_job.py;beta;"class RawFileCachedJob(RetrivalJob, ModificationJob):

    location: DataFileReference
    job: DerivedFeatureJob

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    async def to_pandas(self) -> pd.DataFrame:
        from aligned.local.job import FileFullJob
        from aligned.sources.local import LiteralReference

        try:
            logger.info('Trying to read cache file')
            df = await self.location.read_pandas()
        except UnableToFindFileException:
            logger.info('Unable to load file, so fetching from source')
            df = await self.job.job.to_pandas()
            logger.info('Writing result to cache')
            await self.location.write_pandas(df)
        return (
            await FileFullJob(LiteralReference(df), request=self.job.requests[0])
            .derive_features(self.job.requests)
            .to_pandas()
        )

    async def to_polars(self) -> pl.LazyFrame:
        return await self.job.to_polars()

    def cached_at(self, location: DataFileReference | str) -> RetrivalJob:
        return self

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
219;aligned/retrival_job.py;beta;"class FileCachedJob(RetrivalJob, ModificationJob):

    location: DataFileReference
    job: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    async def to_pandas(self) -> pd.DataFrame:
        try:
            logger.info('Trying to read cache file')
            df = await self.location.read_pandas()
        except UnableToFindFileException:
            logger.info('Unable to load file, so fetching from source')
            df = await self.job.to_pandas()
            logger.info('Writing result to cache')
            await self.location.write_pandas(df)
        return df

    async def to_polars(self) -> pl.LazyFrame:
        try:
            logger.info('Trying to read cache file')
            df = await self.location.to_polars()
        except UnableToFindFileException:
            logger.info('Unable to load file, so fetching from source')
            df = await self.job.to_polars()
            logger.info('Writing result to cache')
            await self.location.write_polars(df)
        return df

    def cached_at(self, location: DataFileReference | str) -> RetrivalJob:
        return self

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
220;aligned/retrival_job.py;beta;"class SplitJob:

    job: RetrivalJob
    target_column: str
    strategy: SplitStrategy

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    async def use_pandas(self) -> SplitDataSet[pd.DataFrame]:
        data = await self.job.to_pandas()
        return self.strategy.split_pandas(data, self.target_column)

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
221;aligned/retrival_job.py;beta;"class FullExtractJob(RetrivalJob):
    limit: int | None"
222;aligned/retrival_job.py;beta;"class DateRangeJob(RetrivalJob):
    start_date: datetime
    end_date: datetime

    """"""
    ```
    psql_config = PsqlConfig(...)
    entites = psql_config.fetch(""SELECT * FROM entities WHERE ..."")
    ```
    """""""
223;aligned/retrival_job.py;beta;"class FactualRetrivalJob(RetrivalJob):
    facts: RetrivalJob"
224;aligned/retrival_job.py;beta;"class EnsureTypesJob(RetrivalJob, ModificationJob):

    job: RetrivalJob
    requests: list[RetrivalRequest]

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.requests

    async def to_pandas(self) -> pd.DataFrame:
        df = await self.job.to_pandas()
        for request in self.requests:
            if request.aggregated_features:
                continue
            for feature in request.all_required_features:

                mask = ~df[feature.name].isnull()

                with suppress(AttributeError):
                    df[feature.name] = df[feature.name].mask(
                        ~mask, other=df.loc[mask, feature.name].str.strip('""')
                    )

                if feature.dtype == FeatureType('').datetime:
                    df[feature.name] = pd.to_datetime(df[feature.name], infer_datetime_format=True, utc=True)
                elif feature.dtype == FeatureType('').datetime or feature.dtype == FeatureType('').string:
                    continue
                else:

                    if feature.dtype.is_numeric:
                        df[feature.name] = pd.to_numeric(df[feature.name], errors='coerce').astype(
                            feature.dtype.pandas_type
                        )
                    else:
                        df[feature.name] = df[feature.name].astype(feature.dtype.pandas_type)
            if request.event_timestamp:
                feature = request.event_timestamp
                df[feature.name] = pd.to_datetime(df[feature.name], infer_datetime_format=True, utc=True)
        return df

    async def to_polars(self) -> pl.LazyFrame:
        df = await self.job.to_polars()
        for request in self.requests:
            if request.aggregated_features:
                continue

            for feature in request.all_required_features:
                if feature.dtype == FeatureType('').bool:
                    df = df.with_column(pl.col(feature.name).cast(pl.Int8).cast(pl.Boolean))
                elif feature.dtype == FeatureType('').datetime:
                    current_dtype = df.select([feature.name]).dtypes[0]
                    if isinstance(current_dtype, pl.Datetime):
                        continue
                    # Convert from ms to us
                    df = df.with_column(
                        (pl.col(feature.name).cast(pl.Int64) * 1000)
                        .cast(pl.Datetime(time_zone='UTC'))
                        .alias(feature.name)
                    )
                else:
                    df = df.with_column(pl.col(feature.name).cast(feature.dtype.polars_type, strict=False))
            if request.event_timestamp:
                feature = request.event_timestamp
                if feature.name not in df.columns:
                    continue
                current_dtype = df.select([feature.name]).dtypes[0]
                if isinstance(current_dtype, pl.Datetime):
                    continue
                df = df.with_column(
                    (pl.col(feature.name).cast(pl.Int64) * 1000)
                    .cast(pl.Datetime(time_zone='UTC'))
                    .alias(feature.name)
                )
        return df

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
225;aligned/retrival_job.py;beta;"class CombineFactualJob(RetrivalJob):
    """"""Computes features that depend on different retrical jobs

    The `job` therefore take in a list of jobs that output some data,
    and a `combined_requests` which defines the features depending on the data

    one example would be the following"
226;aligned/retrival_job.py;beta;"class SomeView(FeatureView):
        metadata = FeatureViewMetadata(
            name=""some_view"",
            batch_source=FileSource.csv_at(""data.csv"")
        )
        id = Entity(Int32())
        a = Int32()"
227;aligned/retrival_job.py;beta;"class OtherView(FeatureView):
        metadata = FeatureViewMetadata(
            name=""other_view"",
            batch_source=FileSource.parquet_at(""other.parquet"")
        )
        id = Entity(Int32())
        c = Int32()"
228;aligned/retrival_job.py;beta;"class Combined(CombinedFeatureView):
        metadata = CombinedMetadata(name=""combined"")

        some = SomeView()
        other = OtherView()

        added = some.a + other.c
    """"""

    jobs: list[RetrivalJob]
    combined_requests: list[RetrivalRequest]

    @property
    def request_result(self) -> RequestResult:
        return RequestResult.from_result_list(
            [job.request_result for job in self.jobs]
        ) + RequestResult.from_request_list(self.combined_requests)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return [job.retrival_requests for job in self.jobs] + [self.combined_requests]

    async def combine_data(self, df: pd.DataFrame) -> pd.DataFrame:
        for request in self.combined_requests:
            for feature in request.derived_features:
                if feature.name in df.columns:
                    logger.info(f'Skipping feature {feature.name}, already computed')
                    continue
                logger.info(f'Computing feature: {feature.name}')
                df[feature.name] = await feature.transformation.transform_pandas(
                    df[feature.depending_on_names]
                )
        return df

    async def combine_polars_data(self, df: pl.LazyFrame) -> pl.LazyFrame:
        for request in self.combined_requests:
            for feature in request.derived_features:
                if feature.name in df.columns:
                    logger.info(f'Skipping feature {feature.name}, already computed')
                    continue
                logger.info(f'Computing feature: {feature.name}')
                result = await feature.transformation.transform_polars(df, feature.name)
                if isinstance(result, pl.Expr):
                    df = df.with_columns([result.alias(feature.name)])
                elif isinstance(result, pl.LazyFrame):
                    df = result
                else:
                    raise ValueError(f'Unsupported transformation result type, got {type(result)}')
        return df

    async def to_pandas(self) -> pd.DataFrame:
        job_count = len(self.jobs)
        if job_count > 1:
            dfs = await asyncio.gather(*[job.to_pandas() for job in self.jobs])
            df = pd.concat(dfs, axis=1)
            combined = await self.combine_data(df)
            return combined.loc[:, ~df.columns.duplicated()].copy()
        elif job_count == 1:
            df = await self.jobs[0].to_pandas()
            return await self.combine_data(df)
        else:
            raise ValueError(
                'Have no jobs to fetch. This is probably an internal error.\n'
                'Please submit an issue, and describe how to reproduce it.\n'
                'Or maybe even submit a PR'
            )

    async def to_polars(self) -> pl.LazyFrame:
        if not self.jobs:
            raise ValueError(
                'Have no jobs to fetch. This is probably an internal error.\n'
                'Please submit an issue, and describe how to reproduce it.\n'
                'Or maybe even submit a PR'
            )

        dfs: list[pl.LazyFrame] = await asyncio.gather(*[job.to_polars() for job in self.jobs])

        df = dfs[0]

        for other_df in dfs[1:]:
            df = df.with_context(other_df).select(pl.all())

        # df = pl.concat(dfs_to_concat, how='horizontal')
        return await self.combine_polars_data(df)

    def cached_at(self, location: DataFileReference | str) -> RetrivalJob:
        return CombineFactualJob([job.cached_at(location) for job in self.jobs], self.combined_requests)

    def remove_derived_features(self) -> RetrivalJob:
        return CombineFactualJob([job.remove_derived_features() for job in self.jobs], self.combined_requests)

    def describe(self) -> str:
        description = f'Combining {len(self.jobs)} jobs:\n'
        for index, job in enumerate(self.jobs):
            description += f'{index + 1}: {job.describe()}\n'
        return description"
229;aligned/retrival_job.py;beta;"class FilterJob(RetrivalJob, ModificationJob):

    include_features: set[str]
    job: RetrivalJob

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result.filter_features(self.include_features)

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return [request.filter_features(self.include_features) for request in self.job.retrival_requests]

    async def to_pandas(self) -> pd.DataFrame:
        df = await self.job.to_pandas()
        if self.include_features:
            total_list = list({ent.name for ent in self.request_result.entities}.union(self.include_features))
            return df[total_list]
        else:
            return df

    async def to_polars(self) -> pl.LazyFrame:
        df = await self.job.to_polars()
        if self.include_features:
            total_list = list({ent.name for ent in self.request_result.entities}.union(self.include_features))
            return df.select(total_list)
        else:
            return df

    def validate(self, validator: Validator) -> RetrivalJob:
        return FilterJob(self.include_features, self.job.validate(validator))

    def cached_at(self, location: DataFileReference | str) -> RetrivalJob:

        return FilterJob(self.include_features, self.job.cached_at(location))

    def with_subfeatures(self) -> RetrivalJob:
        return self.job.with_subfeatures()

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
230;aligned/retrival_job.py;beta;"class ListenForTriggers(RetrivalJob, ModificationJob):

    job: RetrivalJob
    triggers: set[EventTrigger]

    @property
    def request_result(self) -> RequestResult:
        return self.job.request_result

    @property
    def retrival_requests(self) -> list[RetrivalRequest]:
        return self.job.retrival_requests

    async def to_pandas(self) -> pd.DataFrame:
        import asyncio

        df = await self.job.to_pandas()
        await asyncio.gather(*[trigger.check_pandas(df, self.request_result) for trigger in self.triggers])
        return df

    async def to_polars(self) -> pl.LazyFrame:
        import asyncio

        df = await self.job.to_polars()
        await asyncio.gather(*[trigger.check_polars(df, self.request_result) for trigger in self.triggers])
        return df

    def remove_derived_features(self) -> RetrivalJob:
        return self.job.remove_derived_features()"
231;aligned/s3/storage.py;beta;"class S3Client:  # type: ignore[no-redef]
        pass


from aligned.storage import Storage

if TYPE_CHECKING:
    from aligned.sources.s3 import AwsS3Config"
232;aligned/s3/storage.py;beta;"class AwsS3Storage(Storage):

    config: AwsS3Config
    timeout: int = field(default=60)

    async def read(self, path: str) -> bytes:
        from httpx import AsyncClient

        async with AsyncClient(timeout=self.timeout) as client:
            s3_client = S3Client(client, self.config.s3_config)
            url = s3_client.signed_download_url(path)
            response = await client.get(url)
            response.raise_for_status()
            return response.content

    async def write(self, path: str, content: bytes) -> None:
        from httpx import AsyncClient

        async with AsyncClient(timeout=self.timeout) as client:
            s3_client = S3Client(client, self.config.s3_config)
            await s3_client.upload(path, content)"
233;aligned/s3/storage.py;beta;"class FileStorage(Storage):
    async def read(self, path: str) -> bytes:
        return Path(path).read_bytes()

    async def write(self, path: str, content: bytes) -> None:
        lib_path = Path(path)
        lib_path.parent.mkdir(parents=True, exist_ok=True)
        lib_path.write_bytes(content)"
234;aligned/s3/storage.py;beta;"class HttpStorage(Storage):
    async def read(self, path: str) -> bytes:
        if not (path.startswith('http://') or path.startswith('https://')):
            raise ValueError('Invalid url')

        from httpx import AsyncClient

        async with AsyncClient() as client:
            response = await client.get(path)
            response.raise_for_status()
            return response.content

    async def write(self, path: str, content: bytes) -> None:
        raise NotImplementedError()
        # return Path(path).write_bytes(content)"
235;aligned/schemas/codable.py;beta;class Codable(DataClassJSONMixin):
236;aligned/schemas/codable.py;beta;"class Config(BaseConfig):
        code_generation_options = [TO_DICT_ADD_OMIT_NONE_FLAG]"
237;aligned/schemas/constraints.py;beta;"class Constraint(Codable, SerializableType):
    name: str

    def __hash__(self) -> int:
        return hash(self.name)

    def _serialize(self) -> dict:
        assert self.name in SupportedConstraints.shared().types, f'Constraint {self.name} is not supported'
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> 'Constraint':
        name_type = value['name']
        del value['name']
        data_"
238;aligned/schemas/constraints.py;beta;"class = SupportedConstraints.shared().types[name_type]

        return data_class.from_dict(value)"
239;aligned/schemas/constraints.py;beta;"class SupportedConstraints:

    types: dict[str, type[Constraint]]

    _shared: Optional['SupportedConstraints'] = None

    def __init__(self) -> None:
        self.types = {}

        for tran_type in [
            LowerBound,
            LowerBoundInclusive,
            UpperBound,
            UpperBoundInclusive,
            Required,
            InDomain,
            MaxLength,
            MinLength,
        ]:
            self.add(tran_type)

    def add(self, constraint: type[Constraint]) -> None:
        self.types[constraint.name] = constraint

    @classmethod
    def shared(cls) -> 'SupportedConstraints':
        if cls._shared:
            return cls._shared
        cls._shared = SupportedConstraints()
        return cls._shared"
240;aligned/schemas/constraints.py;beta;"class LowerBound(Constraint):
    value: float

    name = 'lower_bound'

    def __hash__(self) -> int:
        return hash(self.name)"
241;aligned/schemas/constraints.py;beta;"class LowerBoundInclusive(Constraint):
    value: float

    name = 'lower_bound_inc'

    def __hash__(self) -> int:
        return hash(self.name)"
242;aligned/schemas/constraints.py;beta;"class UpperBound(Constraint):
    value: float

    name = 'upper_bound'

    def __hash__(self) -> int:
        return hash(self.name)"
243;aligned/schemas/constraints.py;beta;"class UpperBoundInclusive(Constraint):
    value: float

    name = 'upper_bound_inc'

    def __hash__(self) -> int:
        return hash(self.name)"
244;aligned/schemas/constraints.py;beta;"class MinLength(Constraint):
    value: int

    name = 'min_length'

    def __hash__(self) -> int:
        return hash(self.name)"
245;aligned/schemas/constraints.py;beta;"class Regex(Constraint):
    value: str

    name = 'regex'

    def __hash__(self) -> int:
        return hash(self.name)"
246;aligned/schemas/constraints.py;beta;"class EndsWith(Constraint):
    value: str

    name = 'ends_with'

    def __hash__(self) -> int:
        return hash(self.name)"
247;aligned/schemas/constraints.py;beta;"class StartsWith(Constraint):
    value: str

    name = 'starts_with'

    def __hash__(self) -> int:
        return hash(self.name)"
248;aligned/schemas/constraints.py;beta;"class MaxLength(Constraint):
    value: int

    name = 'max_length'

    def __hash__(self) -> int:
        return hash(self.name)"
249;aligned/schemas/constraints.py;beta;"class And(Constraint):

    left: Constraint
    right: Constraint
    name = 'and'

    def __hash__(self) -> int:
        return hash(self.name)"
250;aligned/schemas/constraints.py;beta;"class Required(Constraint):

    name = 'requierd'

    def __hash__(self) -> int:
        return hash(self.name)"
251;aligned/schemas/constraints.py;beta;"class InDomain(Constraint):

    values: list[str]
    name = 'in_domain'

    def __hash__(self) -> int:
        return hash(self.name)"
252;aligned/schemas/derivied_feature.py;beta;"class DerivedFeature(Feature):

    depending_on: set[FeatureReferance]
    transformation: Transformation
    depth: int = 1

    def __init__(
        self,
        name: str,
        dtype: FeatureType,
        depending_on: set[FeatureReferance],
        transformation: Transformation,
        depth: int,
        description: str | None = None,
        tags: dict[str, str] | None = None,
        constraints: set[Constraint] | None = None,
    ):
        self.name = name
        self.dtype = dtype
        self.depending_on = depending_on
        self.transformation = transformation
        self.depth = depth
        self.description = description
        self.tags = tags
        self.constraints = constraints

    def __pre_serialize__(self) -> DerivedFeature:
        from aligned.schemas.transformation import SupportedTransformations

        for feature in self.depending_on:
            assert isinstance(feature, FeatureReferance)

        assert isinstance(self.transformation, Transformation)
        assert self.transformation.name in SupportedTransformations.shared().types

        return self

    @property
    def depending_on_names(self) -> list[str]:
        return [feature.name for feature in self.depending_on]

    @property
    def depending_on_views(self) -> set[FeatureLocation]:
        return {feature.location for feature in self.depending_on}

    @property
    def feature(self) -> Feature:
        return Feature(
            name=self.name,
            dtype=self.dtype,
            description=self.description,
            tags=self.tags,
            constraints=self.constraints,
        )"
253;aligned/schemas/derivied_feature.py;beta;"class AggregationTimeWindow(Codable):
    time_window: timedelta
    time_column: FeatureReferance

    @property
    def group_by_names(self) -> list[str]:
        return [feature.name for feature in self.group_by]

    def __hash__(self) -> int:
        return self.time_window.__hash__()"
254;aligned/schemas/derivied_feature.py;beta;"class AggregateOver(Codable):
    group_by: list[FeatureReferance]
    window: AggregationTimeWindow | None = field(default=None)
    condition: DerivedFeature | None = field(default=None)

    @property
    def group_by_names(self) -> list[str]:
        return [feature.name for feature in self.group_by]

    def __hash__(self) -> int:
        if self.window:
            return self.window.__hash__()

        name = ''
        for feature in self.group_by:
            name += feature.name
        return name.__hash__()"
255;aligned/schemas/derivied_feature.py;beta;"class AggregatedFeature(Codable):

    derived_feature: DerivedFeature
    aggregate_over: AggregateOver

    def __hash__(self) -> int:
        return self.derived_feature.name.__hash__()

    @property
    def depending_on(self) -> set[FeatureReferance]:
        return self.derived_feature.depending_on

    @property
    def depending_on_names(self) -> list[str]:
        return [feature.name for feature in self.depending_on]

    @property
    def depending_on_views(self) -> set[FeatureLocation]:
        return {feature.location for feature in self.depending_on}

    @property
    def feature(self) -> Feature:
        return self.derived_feature.feature

    @property
    def name(self) -> str:
        return self.derived_feature.name"
256;aligned/schemas/event_trigger.py;beta;"class EventTrigger(Codable):
    condition: DerivedFeature
    event: StreamDataSource
    payload: set[Feature]

    async def check_pandas(self, df: pd.DataFrame, result: RequestResult) -> None:
        from aligned.data_source.stream_data_source import SinkableDataSource
        from aligned.local.job import LiteralRetrivalJob

        if not isinstance(self.event, SinkableDataSource):
            logger.info(f'Event: {self.event.topic_name} is not sinkable, will return')
            return

        logger.info(f'Checking for event: {self.event.topic_name}')

        mask = await self.condition.transformation.transform_pandas(df)

        if mask.any():
            trigger_result = RequestResult(result.entities, self.payload, None)
            features = {entity.name for entity in result.entities}.union(
                {feature.name for feature in self.payload}
            )
            events = df[list(features)].loc[mask]
            logger.info(f'Sending {events.shape[0]} events: {self.event.topic_name}')
            await self.event.write_to_stream(LiteralRetrivalJob(events, trigger_result))

    async def check_polars(self, df: pl.LazyFrame, result: RequestResult) -> None:
        from aligned.data_source.stream_data_source import SinkableDataSource
        from aligned.local.job import LiteralRetrivalJob

        if not isinstance(self.event, SinkableDataSource):
            logger.info(f'Event: {self.event.topic_name} is not sinkable, will return')
            return

        logger.info(f'Checking for event: {self.event.topic_name}')

        mask: pl.LazyFrame = await self.condition.transformation.transform_polars(df, self.condition.name)
        mask = mask.filter(pl.col(self.condition.name))

        triggers = mask.collect()

        if triggers.shape[0] > 0:
            trigger_result = RequestResult(result.entities, self.payload, None)
            features = {entity.name for entity in result.entities}.union(
                {feature.name for feature in self.payload}
            )
            events = mask.lazy().select(features)
            logger.info(f'Sending {triggers.shape[0]} events: {self.event.topic_name}')
            await self.event.write_to_stream(LiteralRetrivalJob(events, trigger_result))

    def __hash__(self) -> int:
        return self.event.topic_name.__hash__()"
257;aligned/schemas/feature.py;beta;"class FeatureType(Codable):
    # FIXME: Should use a more Pythonic design, as this one did not behave as intended

    name: str

    @property
    def is_numeric(self) -> bool:
        return self.name in {'bool', 'int32', 'int64', 'float', 'double'}  # Can be represented as an int

    @property
    def python_type(self) -> type:
        from datetime import date, datetime, time, timedelta
        from uuid import UUID

        from numpy import double

        return {
            'string': str,
            'int32': int,
            'int64': int,
            'float': float,
            'double': double,
            'bool': bool,
            'date': date,
            'datetime': datetime,
            'time': time,
            'timedelta': timedelta,
            'uuid': UUID,
            'array': list,
            'embedding': list,
        }[self.name]

    @property
    def pandas_type(self) -> str | type:
        import numpy as np

        return {
            'string': str,
            'int32': 'Int32',
            'int64': 'Int64',
            'float': np.float64,
            'double': np.double,
            'bool': 'boolean',
            'date': np.datetime64,
            'datetime': np.datetime64,
            'time': np.datetime64,
            'timedelta': np.timedelta64,
            'uuid': str,
            'array': list,
            'embedding': list,
        }[self.name]

    @property
    def polars_type(self) -> type:
        import polars as pl

        return {
            'string': pl.Utf8,
            'int32': pl.Int32,
            'int64': pl.Int64,
            'float': pl.Float64,
            'double': pl.Float64,
            'bool': pl.Boolean,
            'date': pl.Date,
            'datetime': pl.Datetime,
            'time': pl.Time,
            'timedelta': pl.Duration,
            'uuid': pl.Utf8,
            'array': pl.List,
            'embedding': pl.List,
        }[self.name]

    def __eq__(self, other: object) -> bool:
        if isinstance(other, FeatureType):
            return self.name == other.name
        return False

    def __hash__(self) -> int:
        return self.name.__hash__()

    def __pre_serialize__(self) -> FeatureType:
        assert isinstance(self.name, str)
        return self

    @property
    def string(self) -> FeatureType:
        return FeatureType(name='string')

    @property
    def int32(self) -> FeatureType:
        return FeatureType(name='int32')

    @property
    def bool(self) -> FeatureType:
        return FeatureType(name='bool')

    @property
    def int64(self) -> FeatureType:
        return FeatureType(name='int64')

    @property
    def float(self) -> FeatureType:
        return FeatureType(name='float')

    @property
    def double(self) -> FeatureType:
        return FeatureType(name='double')

    @property
    def date(self) -> FeatureType:
        return FeatureType(name='date')

    @property
    def uuid(self) -> FeatureType:
        return FeatureType(name='uuid')

    @property
    def datetime(self) -> FeatureType:
        return FeatureType(name='datetime')

    @property
    def array(self) -> FeatureType:
        return FeatureType(name='array')

    @property
    def embedding(self) -> FeatureType:
        return FeatureType(name='embedding')"
258;aligned/schemas/feature.py;beta;"class Feature(Codable):
    name: str
    dtype: FeatureType
    description: str | None = None
    tags: dict[str, str] | None = None

    constraints: set[Constraint] | None = None

    def __pre_serialize__(self) -> Feature:
        assert isinstance(self.name, str)
        assert isinstance(self.dtype, FeatureType)
        assert isinstance(self.description, str) or self.description is None
        assert isinstance(self.tags, dict) or self.tags is None
        if self.constraints:
            for constraint in self.constraints:
                assert isinstance(constraint, Constraint)

        return self

    def __hash__(self) -> int:
        return hash(self.name)

    def __str__(self) -> str:
        value = f'{self.name} - {self.dtype.name}'
        if self.description:
            value += f' - {self.description}'
        return value"
259;aligned/schemas/feature.py;beta;"class EventTimestamp(Codable):
    name: str
    ttl: int | None = None
    description: str | None = None
    tags: dict[str, str] | None = None
    dtype: FeatureType = FeatureType('').datetime

    def __hash__(self) -> int:
        return hash(self.name)

    def __str__(self) -> str:
        value = f'{self.name} - {self.dtype.name}'
        if self.description:
            value += f' - {self.description}'
        return value"
260;aligned/schemas/feature.py;beta;"class FeatureLocation(Codable):
    name: str
    location: Literal['feature_view', 'combined_view', 'model']

    @property
    def identifier(self) -> str:
        return str(self)

    def __str__(self) -> str:
        return f'{self.location}:{self.name}'

    def __hash__(self) -> int:
        return (self.name + self.location).__hash__()

    @staticmethod
    def feature_view(name: str) -> FeatureLocation:
        return FeatureLocation(name, 'feature_view')

    @staticmethod
    def combined_view(name: str) -> FeatureLocation:
        return FeatureLocation(name, 'combined_view')

    @staticmethod
    def model(name: str) -> FeatureLocation:
        return FeatureLocation(name, 'model')"
261;aligned/schemas/feature.py;beta;"class FeatureReferance(Codable):
    name: str
    location: FeatureLocation
    dtype: FeatureType
    # is_derived: bool

    def __hash__(self) -> int:
        return hash(self.name)"
262;aligned/schemas/feature_view.py;beta;"class CompiledFeatureView(Codable):
    name: str
    description: str
    tags: dict[str, str]
    batch_data_source: BatchDataSource

    entities: set[Feature]
    features: set[Feature]
    derived_features: set[DerivedFeature]
    aggregated_features: set[AggregatedFeature] = field(default_factory=set)

    event_timestamp: EventTimestamp | None = field(default=None)
    stream_data_source: StreamDataSource | None = field(default=None)

    event_triggers: set[EventTrigger] | None = field(default=None)

    contacts: list[str] | None = field(default=None)
    indexes: list[VectorIndex] | None = field(default=None)

    def __pre_serialize__(self) -> CompiledFeatureView:
        assert isinstance(self.name, str)
        assert isinstance(self.description, str)
        assert isinstance(self.tags, dict)
        assert isinstance(self.batch_data_source, BatchDataSource)
        for entity in self.entities:
            assert isinstance(entity, Feature)
        for feature in self.features:
            assert isinstance(feature, Feature)
        for derived_feature in self.derived_features:
            assert isinstance(derived_feature, DerivedFeature)
        for aggregated_feature in self.aggregated_features:
            assert isinstance(aggregated_feature, AggregatedFeature)
        if self.event_timestamp is not None:
            assert isinstance(self.event_timestamp, EventTimestamp)
        if self.stream_data_source is not None:
            assert isinstance(self.stream_data_source, StreamDataSource)
        if self.event_triggers is not None:
            for event_trigger in self.event_triggers:
                assert isinstance(event_trigger, EventTrigger)
        if self.contacts is not None:
            assert isinstance(self.contacts, list)
            for contact in self.contacts:
                assert isinstance(contact, str)
        if self.indexes is not None:
            assert isinstance(self.indexes, list)
            for index in self.indexes:
                assert isinstance(index, VectorIndex)
        return self

    @property
    def full_schema(self) -> set[Feature]:
        return self.entities.union(self.features).union(self.derived_features)

    @property
    def entitiy_names(self) -> set[str]:
        return {entity.name for entity in self.entities}

    @property
    def request_all(self) -> FeatureRequest:
        return FeatureRequest(
            FeatureLocation.feature_view(self.name),
            {feature.name for feature in self.full_schema},
            needed_requests=[
                RetrivalRequest(
                    name=self.name,
                    location=FeatureLocation.feature_view(self.name),
                    entities=self.entities,
                    features=self.features,
                    derived_features=self.derived_features,
                    aggregated_features=self.aggregated_features,
                    event_timestamp=self.event_timestamp,
                )
            ],
        )

    def request_for(self, feature_names: set[str]) -> FeatureRequest:

        features = {feature for feature in self.features if feature.name in feature_names}.union(
            self.entities
        )
        derived_features = {feature for feature in self.derived_features if feature.name in feature_names}
        aggregated_features = {
            feature for feature in self.aggregated_features if feature.name in feature_names
        }
        derived_aggregated_feautres = {feature.derived_feature for feature in self.aggregated_features}

        def dependent_features_for(
            feature: DerivedFeature,
        ) -> tuple[set[Feature], set[Feature]]:
            core_features = set()
            derived_features = set()
            aggregated_features = set()

            for dep_ref in feature.depending_on:
                dep_feature = [
                    feat for feat in self.features.union(self.entities) if feat.name == dep_ref.name
                ]
                if len(dep_feature) == 1:
                    core_features.add(dep_feature[0])
                    continue

                dep_features = [
                    feat
                    for feat in self.derived_features.union(derived_aggregated_feautres)
                    if feat.name == dep_ref.name
                ]
                if not dep_features:
                    raise ValueError(
                        'Unable to find the referenced feature. This is most likely a bug in the systemd'
                    )
                dep_feature = dep_features[0]
                if dep_feature in derived_aggregated_feautres:
                    agg_feat = [
                        feat for feat in self.aggregated_features if feat.derived_feature == dep_feature
                    ][0]
                    aggregated_features.add(agg_feat)
                else:
                    derived_features.add(dep_feature)
                core, derived, aggregated = dependent_features_for(dep_feature)
                features.update(core)
                derived_features.update(derived)
                aggregated_features.update(aggregated)

            return core_features, derived_features, aggregated_features

        for dep_feature in derived_features.copy():
            core, intermediate, aggregated = dependent_features_for(dep_feature)
            features.update(core)
            derived_features.update(intermediate)
            aggregated_features.update(aggregated)

        for dep_feature in aggregated_features.copy():
            core, intermediate, aggregated = dependent_features_for(dep_feature)
            features.update(core)
            derived_features.update(intermediate)
            aggregated_features.update(aggregated)

        all_features = features.union(derived_features).union(
            {feature.derived_feature for feature in aggregated_features}
        )
        exclude_names = {feature.name for feature in all_features} - feature_names

        return FeatureRequest(
            FeatureLocation.feature_view(self.name),
            feature_names,
            needed_requests=[
                RetrivalRequest(
                    name=self.name,
                    location=FeatureLocation.feature_view(self.name),
                    entities=self.entities,
                    features=features - self.entities,
                    derived_features=derived_features,
                    aggregated_features=aggregated_features,
                    event_timestamp=self.event_timestamp,
                    features_to_include=exclude_names,
                )
            ],
        )

    def __hash__(self) -> int:
        return hash(self.name)

    def __str__(self) -> str:
        entites = '\n'.join([str(entity) for entity in self.entities])
        input_features = '\n'.join([str(features) for features in self.features])
        transformed_features = '\n'.join([str(features) for features in self.derived_features])
        return f""""""
{self.name}
Description: {self.description}
Tags: {self.tags}

Entities:
{entites}

Event Timestamp:
{self.event_timestamp}

Input features:
{input_features}

Transformed features:
{transformed_features}
"""""""
263;aligned/schemas/feature_view.py;beta;"class CompiledCombinedFeatureView(Codable):
    name: str
    features: set[DerivedFeature]  # FIXME: Should combine this and feature_referances into one class.
    feature_referances: dict[str, list[RetrivalRequest]]
    event_triggers: set[EventTrigger] | None = field(default=None)

    @property
    def entity_features(self) -> set[Feature]:
        values = set()
        for requests in self.feature_referances.values():
            for request in requests:
                values.update(request.entities)
        return values

    @property
    def entity_names(self) -> set[str]:
        return {feature.name for feature in self.entity_features}

    @property
    def request_all(self) -> FeatureRequest:
        requests: dict[str, RetrivalRequest] = {}
        entities = set()
        for sub_requests in self.feature_referances.values():
            for request in sub_requests:
                entities.update(request.entities)
                if request.location not in requests:
                    requests[request.location] = RetrivalRequest(
                        name=request.name,
                        location=request.location,
                        entities=request.entities,
                        features=set(),
                        derived_features=set(),
                        event_timestamp=request.event_timestamp,
                    )
                requests[request.location].derived_features.update(request.derived_features)
                requests[request.location].features.update(request.features)
                requests[request.location].entities.update(request.entities)

        requests[self.name] = RetrivalRequest(
            name=self.name,
            location=FeatureLocation.combined_view(self.name),
            entities=entities,
            features=set(),
            derived_features=self.features,
            event_timestamp=None,
        )

        return FeatureRequest(
            self.name,
            features_to_include={feature.name for feature in self.features.union(entities)},
            needed_requests=RetrivalRequest.combine(list(requests.values())),
        )

    def requests_for(self, feature_names: set[str]) -> FeatureRequest:
        entities = self.entity_names
        dependent_views: dict[str, RetrivalRequest] = {}
        for feature in feature_names:
            if feature in entities:
                continue

            if feature not in self.feature_referances.keys():
                raise ValueError(f'Invalid feature {feature} in {self.name}')

            requests = self.feature_referances[feature]
            for request in requests:
                if request.location not in dependent_views:
                    dependent_views[request.location] = RetrivalRequest(
                        name=request.name,
                        location=request.location,
                        entities=request.entities,
                        features=set(),
                        derived_features=set(),
                        event_timestamp=request.event_timestamp,
                    )
                current = dependent_views[request.location]
                current.derived_features = current.derived_features.union(request.derived_features)
                current.features = current.features.union(request.features)
                dependent_views[request.location] = current

        dependent_views[self.name] = RetrivalRequest(  # Add the request we want
            name=self.name,
            location=FeatureLocation.combined_view(self.name),
            entities=self.entity_features,
            features=set(),
            derived_features={feature for feature in self.features if feature.name in feature_names},
            event_timestamp=None,
        )

        return FeatureRequest(
            FeatureLocation.combined_view(self.name),
            features_to_include=feature_names,
            needed_requests=list(dependent_views.values()),
        )

    def __hash__(self) -> int:
        return hash(self.name)"
264;aligned/schemas/folder.py;beta;"class FolderFactory:

    supported_folders: dict[str, type[Folder]]

    _shared: FolderFactory | None = None

    def __init__(self) -> None:
        self.supported_folders = {folder_type.name: folder_type for folder_type in Folder.__subclasses__()}

    @classmethod
    def shared(cls) -> FolderFactory:
        if cls._shared:
            return cls._shared
        cls._shared = FolderFactory()
        return cls._shared"
265;aligned/schemas/folder.py;beta;"class Folder(Codable, SerializableType):

    name: str

    def file_at(self, path: Path) -> StorageFileReference:
        raise NotImplementedError()

    def _serialize(self) -> dict:
        assert self.name in FolderFactory.shared().supported_folders, f'Unknown type_name: {self.name}'
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> Folder:
        name_type = value['name']
        if name_type not in FolderFactory.shared().supported_folders:
            raise ValueError(
                f""Unknown batch data source id: '{name_type}'.\nRemember to add the""
                ' data source to the FolderFactory.supported_folders if'
                ' it is a custom type.'
                f' Have access to the following types: {FolderFactory.shared().supported_folders.keys()}'
            )
        del value['name']
        data_"
266;aligned/schemas/folder.py;beta;"class = FolderFactory.shared().supported_folders[name_type]
        return data_class.from_dict(value)"
267;aligned/schemas/literal_value.py;beta;"class SupportedLiteralValues:

    values: dict[str, type[LiteralValue]]

    _shared: SupportedLiteralValues | None = None

    def __init__(self) -> None:
        self.values = {}
        for lit in [IntValue, FloatValue, BoolValue, DateValue, DatetimeValue, StringValue, ArrayValue]:
            self.values[lit.name] = lit

    @classmethod
    def shared(cls) -> SupportedLiteralValues:
        if cls._shared:
            return cls._shared
        cls._shared = SupportedLiteralValues()
        return cls._shared"
268;aligned/schemas/literal_value.py;beta;"class LiteralValue(Codable, SerializableType):
    name: str

    @property
    def python_value(self) -> Any:
        raise NotImplementedError()

    @property
    def dtype(self) -> FeatureType:
        return FeatureType(self.name)

    def _serialize(self) -> dict:
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> LiteralValue:
        name_type = value['name']
        del value['name']
        data_"
269;aligned/schemas/literal_value.py;beta;"class = SupportedLiteralValues.shared().values[name_type]
        return data_class.from_dict(value)

    @staticmethod
    def from_value(value: Any) -> LiteralValue:
        if isinstance(value, bool):
            return BoolValue(value)
        elif isinstance(value, int):
            return IntValue(value)
        elif isinstance(value, float):
            return FloatValue(value)
        elif isinstance(value, date):
            return DateValue(value)
        elif isinstance(value, datetime):
            return DatetimeValue(value)
        elif isinstance(value, str):
            return StringValue(value)
        elif isinstance(value, list):
            return ArrayValue([LiteralValue.from_value(val) for val in value])
        raise ValueError(f'Unable to find literal value for type {type(value)}')"
270;aligned/schemas/literal_value.py;beta;"class IntValue(LiteralValue):
    value: int
    name = 'int'

    @property
    def python_value(self) -> Any:
        return self.value"
271;aligned/schemas/literal_value.py;beta;"class FloatValue(LiteralValue):
    value: float
    name = 'float'

    @property
    def python_value(self) -> Any:
        return self.value"
272;aligned/schemas/literal_value.py;beta;"class BoolValue(LiteralValue):
    value: bool
    name = 'bool'

    @property
    def python_value(self) -> Any:
        return self.value"
273;aligned/schemas/literal_value.py;beta;"class DateValue(LiteralValue):
    value: date
    name = 'date'

    @property
    def python_value(self) -> Any:
        return self.value"
274;aligned/schemas/literal_value.py;beta;"class DatetimeValue(LiteralValue):
    value: datetime
    name = 'datetime'

    @property
    def python_value(self) -> Any:
        return self.value"
275;aligned/schemas/literal_value.py;beta;"class StringValue(LiteralValue):
    value: str
    name = 'string'

    @property
    def python_value(self) -> Any:
        return self.value"
276;aligned/schemas/literal_value.py;beta;"class ArrayValue(LiteralValue):
    value: list[LiteralValue]
    name = 'array'

    @property
    def python_value(self) -> Any:
        return [lit.python_value for lit in self.value]"
277;aligned/schemas/model.py;beta;"class Target(Codable):
    estimating: FeatureReferance
    feature: Feature

    on_ground_truth_event: StreamDataSource | None = field(default=None)

    # This is a limitation of the current setup.
    # Optimaly will this be on the features feature view, but this is not possible at the moment
    event_trigger: EventTrigger | None = field(default=None)

    def __hash__(self) -> int:
        return self.feature.name.__hash__()"
278;aligned/schemas/model.py;beta;"class PredictionsView(Codable):
    entities: set[Feature]
    features: set[Feature]
    derived_features: set[DerivedFeature]
    event_timestamp: EventTimestamp | None = field(default=None)
    source: BatchDataSource | None = field(default=None)
    stream_source: StreamDataSource | None = field(default=None)

    regression_targets: set[RegressionTarget] | None = field(default=None)
    classification_targets: set[ClassificationTarget] | None = field(default=None)

    @property
    def full_schema(self) -> set[Feature]:

        schema = self.features.union(self.entities).union(
            {target.feature for target in self.classification_targets}
        )

        for target in self.classification_targets:
            schema.update({prob.feature for prob in target.class_probabilities})

        for target in self.regression_targets:
            if target.confidence:
                schema.add(target.confidence)

            if target.lower_confidence:
                schema.add(target.lower_confidence)

            if target.upper_confidence:
                schema.add(target.upper_confidence)

        return schema"
279;aligned/schemas/model.py;beta;"class Model(Codable):
    name: str
    features: set[FeatureReferance]
    predictions_view: PredictionsView
    description: str | None = field(default=None)
    contacts: list[str] | None = field(default=None)
    tags: dict[str, str] | None = field(default=None)
    dataset_folder: Folder | None = field(default=None)

    def __hash__(self) -> int:
        return self.name.__hash__()

    @property
    def request_all_predictions(self) -> FeatureRequest:
        return FeatureRequest(
            FeatureLocation.model(self.name),
            {feature.name for feature in self.predictions_view.full_schema},
            needed_requests=[
                RetrivalRequest(
                    name=self.name,
                    location=FeatureLocation.model(self.name),
                    features=self.predictions_view.full_schema,
                    derived_features=self.predictions_view.derived_features,
                    entities=self.predictions_view.entities,
                    event_timestamp=self.predictions_view.event_timestamp,
                )
            ],
        )"
280;aligned/schemas/repo_definition.py;beta;"class RepoReference:
    """"""
    NB: Is deprecated!
    """"""

    env_var_name: str
    repo_paths: dict[str, StorageFileReference]

    @property
    def selected(self) -> str:
        import os

        return os.environ[self.env_var_name]

    @property
    def selected_file(self) -> StorageFileReference | None:
        if self.env_var_name in self.repo_paths:
            return self.repo_paths.get(self.env_var_name)
        return self.repo_paths.get(self.selected)

    def feature_server(self, source: FeatureSource) -> FastAPI | FeatureSource:
        import os

        if os.environ.get('ALADDIN_ENABLE_SERVER', 'False').lower() == 'false':
            return source

        from aligned.server import FastAPIServer

        if not (selected_file := self.selected_file):
            raise ValueError('No selected file to serve features from')

        try:
            feature_store = asyncio.get_event_loop().run_until_complete(selected_file.feature_store())
        except RuntimeError:
            import nest_asyncio

            nest_asyncio.apply()
            feature_store = asyncio.new_event_loop().run_until_complete(selected_file.feature_store())

        return FastAPIServer.app(feature_store)

    @staticmethod
    def reference_object(repo: Path, file: Path, object: str) -> RepoReference:
        from aligned.compiler.repo_reader import import_module, path_to_py_module
        from aligned.sources.local import StorageFileReference

        module_path = path_to_py_module(file, repo)

        try:
            module = import_module(module_path)
            obj = getattr(module, object)
            if isinstance(obj, StorageFileReference):
                return RepoReference(env_var_name='const', repo_paths={'const': obj})
            raise ValueError('No reference found')
        except AttributeError:
            raise ValueError('No reference found')
        except ModuleNotFoundError:
            raise ValueError('No reference found')"
281;aligned/schemas/repo_definition.py;beta;"class FeatureServer:
    @staticmethod
    def from_reference(
        reference: StorageFileReference, online_source: FeatureSource | None = None
    ) -> FastAPI | None:
        """"""Creates a feature server
        This can process and serve features for both models and feature views

        ```python
        redis = RedisConfig.localhost()
        server = FeatureSever.from_reference(
            FileSource.from_json(""./feature-store.json""),
            online_source=redis
        )
        ```

        You can then run `aligned serve path_to_server_instance:server`.

        Args:
            reference (StorageFileReference): The location to the feature repository
            online_source (OnlineSource | None, optional): The online source to use.
                Defaults to None meaning the batch source.

        Returns:
            FastAPI: A FastAPI instance that contains paths for fetching features
        """"""
        import os

        if os.environ.get('ALADDIN_ENABLE_SERVER', 'False').lower() == 'false':
            return None

        from aligned.server import FastAPIServer

        try:
            feature_store = asyncio.get_event_loop().run_until_complete(reference.feature_store())
        except RuntimeError:
            import nest_asyncio

            nest_asyncio.apply()
            feature_store = asyncio.new_event_loop().run_until_complete(reference.feature_store())

        return FastAPIServer.app(feature_store.with_source(online_source))"
282;aligned/schemas/repo_definition.py;beta;"class EnricherReference(Codable):
    module: str
    attribute_name: str
    enricher: Enricher"
283;aligned/schemas/repo_definition.py;beta;"class RepoMetadata(Codable):
    created_at: datetime
    name: str
    github_url: str | None = field(default=None)"
284;aligned/schemas/repo_definition.py;beta;"class RepoDefinition(Codable):

    metadata: RepoMetadata

    feature_views: set[CompiledFeatureView] = field(default_factory=set)
    combined_feature_views: set[CompiledCombinedFeatureView] = field(default_factory=set)
    models: set[Model] = field(default_factory=set)
    enrichers: list[EnricherReference] = field(default_factory=list)

    def to_dict(self, **kwargs: dict) -> dict:
        for view in self.feature_views:
            assert isinstance(view, CompiledFeatureView)

        for view in self.combined_feature_views:
            assert isinstance(view, CompiledCombinedFeatureView)

        for model in self.models:
            assert isinstance(model, Model)

        for enricher in self.enrichers:
            assert isinstance(enricher, EnricherReference)
        return super().to_dict(**kwargs)

    @staticmethod
    async def from_file(file: StorageFileReference) -> RepoDefinition:
        repo = await file.read()
        return RepoDefinition.from_json(repo)

    @staticmethod
    async def from_reference_at_path(repo_path: str, file_path: str) -> RepoDefinition:
        from aligned.compiler.repo_reader import RepoReader

        dir_path = Path.cwd() if repo_path == '.' else Path(repo_path).absolute()
        absolute_file_path = Path(file_path).absolute()

        try:
            reference = RepoReader.reference_from_path(dir_path, absolute_file_path)
            if file := reference.selected_file:
                logger.info(f""Loading repo from configuration '{reference.selected}'"")
                return await RepoDefinition.from_file(file)
            else:
                logger.info('Found no configuration')
        except ValueError as error:
            logger.error(f'Error when loadin repo: {error}')

        logger.info('Generating repo definition')
        return await RepoReader.definition_from_path(dir_path)

    @staticmethod
    async def from_path(path: str) -> RepoDefinition:
        from aligned.compiler.repo_reader import RepoReader

        dir_path = Path.cwd() if path == '.' else Path(path).absolute()
        return await RepoReader.definition_from_path(dir_path)

    # def add_old_version(self, old_version: ""RepoDefinition"") -> ""RepoDefinition"":

    #     views: dict[str, VersionedData[CompiledFeatureView]] = {}
    #     for view in self.feature_views_2:
    #         old_views = [fv for fv in old_version.feature_views_2 if fv.identifier == view.identifier]
    #         if not old_views:
    #             views[view.identifier] = view
    #             continue

    #         old_view = old_views[0]

    #         if old_view.latest == view.latest:
    #             views[view.identifier] = old_view
    #         else:
    #             view[view.identifier] = VersionedData(
    #                   identifier=view.identifier,
    #                   versions=view.versions + old_view.versions
    #               )

    #     self.feature_views_2 = set(views.values())
    #     return self"
285;aligned/schemas/target.py;beta;"class RegressionTarget(Codable):
    estimating: FeatureReferance
    feature: Feature

    on_ground_truth_event: StreamDataSource | None = field(default=None)

    event_trigger: EventTrigger | None = field(default=None)

    confidence: Feature | None = field(default=None)
    lower_confidence: Feature | None = field(default=None)
    upper_confidence: Feature | None = field(default=None)

    def __hash__(self) -> int:
        return self.feature.name.__hash__()"
286;aligned/schemas/target.py;beta;"class ClassTargetProbability(Codable):
    outcome: LiteralValue
    feature: Feature

    def __hash__(self) -> int:
        return self.feature.name.__hash__()"
287;aligned/schemas/target.py;beta;"class ClassificationTarget(Codable):
    estimating: FeatureReferance
    feature: Feature

    on_ground_truth_event: StreamDataSource | None = field(default=None)

    event_trigger: EventTrigger | None = field(default=None)

    class_probabilities: set[ClassTargetProbability] = field(default_factory=set)
    confidence: Feature | None = field(default=None)

    def __hash__(self) -> int:
        return self.feature.name.__hash__()"
288;aligned/schemas/text_vectoriser.py;beta;"class SupportedTextModels:

    types: dict[str, type[TextVectoriserModel]]

    _shared: SupportedTextModels | None = None

    def __init__(self) -> None:
        self.types = {}

        for tran_type in [GensimModel, OpenAiEmbeddingModel, HuggingFaceTransformer]:
            self.add(tran_type)

    def add(self, transformation: type[TextVectoriserModel]) -> None:
        self.types[transformation.name] = transformation

    @classmethod
    def shared(cls) -> SupportedTextModels:
        if cls._shared:
            return cls._shared
        cls._shared = SupportedTextModels()
        return cls._shared"
289;aligned/schemas/text_vectoriser.py;beta;"class TextVectoriserModel(Codable, SerializableType):
    name: str

    @property
    def embedding_size(self) -> int | None:
        return None

    def _serialize(self) -> dict:
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> TextVectoriserModel:
        name_type = value['name']
        del value['name']
        data_"
290;aligned/schemas/text_vectoriser.py;beta;"class = SupportedTextModels.shared().types[name_type]
        with suppress(AttributeError):
            if data_class.dtype:
                del value['dtype']

        return data_class.from_dict(value)

    async def load_model(self):
        pass

    async def vectorise_pandas(self, texts: pd.Series) -> pd.Series:
        pass

    async def vectorise_polars(self, texts: pl.LazyFrame, text_key: str, output_key: str) -> pl.LazyFrame:
        pass

    @staticmethod
    def gensim(model_name: str, config: GensimConfig | None = None) -> GensimModel:
        return GensimModel(model_name=model_name, config=config or GensimConfig())

    @staticmethod
    def openai(
        model_name: str = 'text-embedding-ada-002', api_token_env_key: str = 'OPENAI_API_KEY'
    ) -> OpenAiEmbeddingModel:
        return OpenAiEmbeddingModel(model=model_name, api_token_env_key=api_token_env_key)

    @staticmethod
    def huggingface(model_name: str) -> HuggingFaceTransformer:
        return HuggingFaceTransformer(model=model_name)"
291;aligned/schemas/text_vectoriser.py;beta;"class GensimConfig(Codable):
    to_lowercase: bool = field(default=False)
    deaccent: bool = field(default=False)
    encoding: str = field(default='utf8')
    errors: str = field(default='strict')"
292;aligned/schemas/text_vectoriser.py;beta;"class GensimModel(TextVectoriserModel):

    model_name: str
    config: GensimConfig = field(default_factory=GensimConfig)

    loaded_model: Any = field(default=None)
    name: str = 'gensim'

    async def vectorise_pandas(self, texts: pd.Series) -> pd.Series:
        if not self.loaded_model:
            await self.load_model()

        from gensim.utils import tokenize

        def token(text: str) -> list[str]:
            return list(
                tokenize(
                    text,
                    lowercase=self.config.to_lowercase,
                    deacc=self.config.deaccent,
                    encoding=self.config.encoding,
                    errors=self.config.errors,
                )
            )

        tokens = texts.apply(token)

        def vector(tokens: list[str]):
            vector = np.zeros(self.loaded_model.vector_size)
            n = 0
            for token in tokens:
                if token in self.loaded_model:
                    vector += self.loaded_model[token]
                    n += 1
            if n > 0:
                vector = vector / n

            return vector

        return tokens.apply(vector)

    async def vectorise_polars(self, texts: pl.LazyFrame, text_key: str, output_key: str) -> pl.LazyFrame:
        if not self.loaded_model:
            await self.load_model()

        from gensim.utils import tokenize

        def token(text: str) -> list[str]:
            return list(
                tokenize(
                    text,
                    lowercase=self.config.to_lowercase,
                    deacc=self.config.deaccent,
                    encoding=self.config.encoding,
                    errors=self.config.errors,
                )
            )

        tokenised_text = texts.with_columns([pl.col(text_key).apply(token).alias(f'{text_key}_tokens')])

        def vector(tokens: list[str]) -> list[float]:
            logger.info('Computing vector', tokens)
            vector = np.zeros(self.loaded_model.vector_size)
            n = 0
            for token in tokens:
                if token in self.loaded_model:
                    vector += self.loaded_model[token]
                    n += 1
            if n > 0:
                vector = vector / n

            return vector.tolist()

        return tokenised_text.with_columns(
            [pl.col(f'{text_key}_tokens').apply(vector, return_dtype=pl.List(pl.Float64)).alias(output_key)]
        )

    async def load_model(self):
        import gensim.downloader as gensim_downloader

        self.loaded_model = gensim_downloader.load(self.model_name)
        logger.info(f'Loaded model {self.model_name}')"
293;aligned/schemas/text_vectoriser.py;beta;"class OpenAiEmbedding(BaseModel):
    index: int
    embedding: list[float]"
294;aligned/schemas/text_vectoriser.py;beta;"class OpenAiResponse(BaseModel):
    data: list[OpenAiEmbedding]"
295;aligned/schemas/text_vectoriser.py;beta;"class OpenAiEmbeddingModel(TextVectoriserModel):

    api_token_env_key: str = field(default='OPENAI_API_KEY')
    model: str = field(default='text-embedding-ada-002')
    name: str = 'openai'

    @property
    def embedding_size(self) -> int | None:
        return 768

    async def embeddings(self, input: list[str]) -> OpenAiResponse:
        # import openai
        import os

        # openai.api_key = os.environ[self.api_token_env_key]
        # return openai.Embedding.create(
        #     model=""text-embedding-ada-002"",
        #     input=input
        # )
        from httpx import AsyncClient

        api_token = os.environ[self.api_token_env_key]

        assert isinstance(input, list)
        for item in input:
            assert item is not None
            assert isinstance(item, str)
            assert len(item) > 0
            assert len(item) / 3 < 7000

        try:
            async with AsyncClient() as client:
                json = {'model': self.model, 'input': input}
                response = await client.post(
                    url='https://api.openai.com/v1/embeddings',
                    json=json,
                    headers={'Authorization': f'Bearer {api_token}', 'Content-Type': 'application/json'},
                )
                response.raise_for_status()
        except Exception as e:
            logger.error(f'Error calling OpenAi Embeddings API: {e}')
            logger.error(f'Input: {input}')
            raise e

        try:
            return OpenAiResponse(**response.json())
        except ValidationError as e:
            logger.error(f'Response: {response.json()}')
            logger.error(f'Error parsing OpenAi Embeddings API response: {e}')
            raise e

    async def load_model(self):
        pass

    async def vectorise_pandas(self, texts: pd.Series) -> pd.Series:
        data = await self.embeddings(texts.tolist())
        return pd.Series([embedding.embedding for embedding in data.data])

    async def vectorise_polars(self, texts: pl.LazyFrame, text_key: str, output_key: str) -> pl.LazyFrame:
        data = await self.embeddings(texts.select(text_key).collect().to_series().to_list())
        return texts.with_column(
            pl.Series(values=[embedding.embedding for embedding in data.data], name=output_key)
        )"
296;aligned/schemas/text_vectoriser.py;beta;"class HuggingFaceTransformer(TextVectoriserModel):

    model: str
    name: str = 'huggingface'
    loaded_model: Any = field(default=None)

    async def load_model(self):
        from sentence_transformers import SentenceTransformer

        self.loaded_model = SentenceTransformer(self.model)

    async def vectorise_polars(self, texts: pl.LazyFrame, text_key: str, output_key: str) -> pl.LazyFrame:
        if self.loaded_model is None:
            await self.load_model()
        return texts.with_column(
            pl.Series(
                self.loaded_model.encode(texts.select(pl.col(text_key)).collect().to_series().to_list())
            ).alias(output_key)
        )
        pass

    async def vectorise_pandas(self, texts: pd.Series) -> pd.Series:
        if self.loaded_model is None:
            await self.load_model()
        return pd.Series(self.loaded_model.encode(texts.tolist()).tolist())"
297;aligned/schemas/transformation.py;beta;"class TransformationTestDefinition:
    transformation: Transformation
    input: dict[str, list]
    output: list

    @property
    def input_pandas(self) -> pd.DataFrame:
        return pd.DataFrame(self.input)

    @property
    def output_pandas(self) -> pd.Series:
        return pd.Series(self.output)

    @property
    def input_polars(self) -> pl.DataFrame:
        return pl.from_dict(self.input)

    @property
    def output_polars(self) -> pl.Series:
        try:
            values = pl.Series(self.output).fill_nan(None)
            if self.transformation.dtype == FeatureType('').bool:
                return values.cast(pl.Boolean)
            else:
                return values
        except pl.InvalidOperationError:
            return pl.Series(self.output)


def gracefull_transformation(
    df: pd.DataFrame,
    is_valid_mask: pd.Series,
    transformation: Callable[[pd.Series], pd.Series],
) -> pd.Series:
    result = pd.Series(np.repeat(np.nan, repeats=is_valid_mask.shape[0]))
    result.loc[is_valid_mask] = transformation(df.loc[is_valid_mask])
    return result"
298;aligned/schemas/transformation.py;beta;"class PsqlTransformation:
    def as_psql(self) -> str:
        raise NotImplementedError()"
299;aligned/schemas/transformation.py;beta;"class RedshiftTransformation:
    def as_redshift(self) -> str:
        if isinstance(self, PsqlTransformation):
            return self.as_psql()
        raise NotImplementedError()"
300;aligned/schemas/transformation.py;beta;"class Transformation(Codable, SerializableType):
    name: str
    dtype: FeatureType

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        pass

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr | pl.Expr:
        raise NotImplementedError()

    def _serialize(self) -> dict:
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> Transformation:
        name_type = value['name']
        del value['name']
        data_"
301;aligned/schemas/transformation.py;beta;"class = SupportedTransformations.shared().types[name_type]
        with suppress(AttributeError):
            if data_class.dtype:
                del value['dtype']

        return data_class.from_dict(value)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        raise NotImplementedError()

    @classmethod
    async def run_transformation_test_polars(cls) -> None:
        from polars.testing import assert_series_equal

        try:
            test = cls.test_definition()
            alias = 'something'
            output_df = await test.transformation.transform_polars(test.input_polars.lazy(), alias=alias)
            if isinstance(output_df, pl.Expr):
                output_df = test.input_polars.lazy().with_columns([output_df.alias(alias)])
            output = output_df.select(pl.col(alias)).collect().to_series()
            assert (set(test.input_polars.columns) - set(output_df.columns)) == set()

            expected = test.output_polars
            if test.transformation.dtype == FeatureType('').bool:
                is_correct = output.series_equal(test.output_polars.alias(alias))
                assert is_correct, (
                    f'Output for {cls.__name__} is not correct.,'
                    f'\nGot: {output},\nexpected: {test.output_polars}'
                )
            else:
                assert_series_equal(expected.alias(alias), output, check_names=False, check_dtype=False)
        except pl.NotFoundError:
            AssertionError(
                f'Not able to find resulting transformation {cls.__name__}, remember to add .alias(alias)'
            )
        except AttributeError:
            raise AssertionError(
                f'Error for transformation {cls.__name__}. Could be missing a return in the transformation'
            )
        except NotImplementedError:
            pass

    @classmethod
    async def run_transformation_test_pandas(cls) -> None:
        import numpy as np
        from numpy.testing import assert_almost_equal

        with suppress(NotImplementedError):
            test = cls.test_definition()
            output = await test.transformation.transform_pandas(test.input_pandas)
            if test.transformation.dtype == FeatureType('').bool:
                is_correct = np.all(output == test.output_pandas) | output.equals(test.output_pandas)
                assert is_correct, (
                    f'Output for {cls.__name__} is not correct.,'
                    f'\nGot: {output},\nexpected: {test.output_pandas}'
                )
            elif test.transformation.dtype == FeatureType('').string:
                expected = test.output_pandas
                assert expected.equals(output), (
                    f'Output for {cls.__name__} is not correct.,'
                    f'\nGot: {output},\nexpected: {test.output_pandas}'
                )
            else:
                expected = test.output_pandas.to_numpy()
                output_np = output.to_numpy().astype('float')
                is_null = np.isnan(expected) & np.isnan(output_np)
                assert_almost_equal(expected[~is_null], output_np[~is_null])"
302;aligned/schemas/transformation.py;beta;"class SupportedTransformations:

    types: dict[str, type[Transformation]]

    _shared: SupportedTransformations | None = None

    def __init__(self) -> None:
        self.types = {}

        for tran_type in [
            Equals,
            NotEquals,
            NotNull,
            PandasLambdaTransformation,
            PandasFunctionTransformation,
            PolarsLambdaTransformation,
            Ratio,
            DivideDenumeratorValue,
            Contains,
            GreaterThen,
            GreaterThenValue,
            GreaterThenOrEqual,
            LowerThen,
            LowerThenOrEqual,
            DateComponent,
            Subtraction,
            Multiply,
            MultiplyValue,
            Addition,
            AdditionValue,
            TimeDifference,
            Logarithm,
            LogarithmOnePluss,
            ToNumerical,
            ReplaceStrings,
            IsIn,
            And,
            Or,
            Inverse,
            Ordinal,
            FillNaValues,
            Absolute,
            Round,
            Ceil,
            Floor,
            CopyTransformation,
            WordVectoriser,
            MapArgMax,
            LoadImageUrl,
            GrayscaleImage,
            Power,
            PowerFeature,
            AppendConstString,
            AppendStrings,
            ConcatStringAggregation,
            SumAggregation,
            MeanAggregation,
            MinAggregation,
            MaxAggregation,
            MedianAggregation,
            CountAggregation,
            CountDistinctAggregation,
            StdAggregation,
            VarianceAggregation,
            PercentileAggregation,
            Clip,
        ]:
            self.add(tran_type)

    def add(self, transformation: type[Transformation]) -> None:
        self.types[transformation.name] = transformation

    @classmethod
    def shared(cls) -> SupportedTransformations:
        if cls._shared:
            return cls._shared
        cls._shared = SupportedTransformations()
        return cls._shared"
303;aligned/schemas/transformation.py;beta;"class PandasFunctionTransformation(Transformation):
    """"""
    This will encode a custom method, that is not a lambda function
    Threfore, we will stort the actuall code, and dynamically load it on runtime.

    This is unsafe, but will remove the ModuleImportError for custom methods
    """"""

    code: str
    function_name: str
    dtype: FeatureType
    name: str = 'pandas_code_tran'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        if self.function_name not in locals():
            exec(self.code)

        loaded = locals()[self.function_name]
        if asyncio.iscoroutinefunction(loaded):
            return await loaded(df)
        else:
            return loaded(df)

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        pandas_df = df.collect().to_pandas()
        if self.function_name not in locals():
            exec(self.code)

        loaded = locals()[self.function_name]
        if asyncio.iscoroutinefunction(loaded):
            pandas_df[alias] = await loaded(pandas_df)
        else:
            pandas_df[alias] = loaded(pandas_df)

        return pl.from_pandas(pandas_df).lazy()

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            transformation=PandasFunctionTransformation(
                code='async def test(df):\n    return df[""a""] + df[""b""]',
                function_name='test',
                dtype=FeatureType('').int32,
            ),
            input={
                'a': [1, 2, 3, 4, 5],
                'b': [1, 2, 3, 4, 5],
            },
            output=[2, 4, 6, 8, 10],
        )"
304;aligned/schemas/transformation.py;beta;"class PandasLambdaTransformation(Transformation):

    method: bytes
    code: str
    dtype: FeatureType
    name: str = 'pandas_lambda_tran'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        import asyncio

        import dill

        loaded = dill.loads(self.method)
        if asyncio.iscoroutinefunction(loaded):
            return await loaded(df)
        else:
            return loaded(df)

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:

        import dill

        pandas_df = df.collect().to_pandas()
        loaded = dill.loads(self.method)
        if asyncio.iscoroutinefunction(loaded):
            pandas_df[alias] = await loaded(pandas_df)
        else:
            pandas_df[alias] = loaded(pandas_df)

        return pl.from_pandas(pandas_df).lazy()"
305;aligned/schemas/transformation.py;beta;"class PolarsFunctionTransformation(Transformation):
    """"""
    This will encode a custom method, that is not a lambda function
    Threfore, we will stort the actuall code, and dynamically load it on runtime.

    This is unsafe, but will remove the ModuleImportError for custom methods
    """"""

    code: str
    function_name: str
    dtype: FeatureType
    name: str = 'pandas_code_tran'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        polars_df = await self.transform_polars(pl.from_pandas(df).lazy(), self.function_name)
        return polars_df.collect().to_pandas()[self.function_name]

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        if self.function_name not in locals():
            exec(self.code)

        loaded = locals()[self.function_name]
        if asyncio.iscoroutinefunction(loaded):
            return await loaded(df, alias)
        else:
            return loaded(df, alias)"
306;aligned/schemas/transformation.py;beta;"class PolarsLambdaTransformation(Transformation):

    method: bytes
    code: str
    dtype: FeatureType
    name: str = 'polars_lambda_tran'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        import dill

        loaded: pl.Expr = dill.loads(self.method)
        pl_df = pl.from_pandas(df)
        pl_df = pl_df.with_column((loaded).alias('polars_tran_column'))
        return pl_df['polars_tran_column'].to_pandas()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        import dill

        tran: Callable[[pl.LazyFrame, str], pl.LazyFrame] = dill.loads(self.method)
        if isinstance(tran, pl.Expr):
            return tran
        else:
            return tran(df, alias)"
307;aligned/schemas/transformation.py;beta;"class NotNull(Transformation):

    key: str

    name: str = 'not_null'
    dtype: FeatureType = FeatureType('').bool

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key].notnull()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column(pl.col(self.key).is_not_null().alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            NotNull('x'),
            input={'x': ['Hello', None, None, 'test', None]},
            output=[True, False, False, True, False],
        )"
308;aligned/schemas/transformation.py;beta;"class Equals(Transformation):

    key: str
    value: LiteralValue

    name: str = 'equals'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, key: str, value: str) -> None:
        self.key = key
        self.value = value

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key] == self.value.python_value

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.key) == self.value.python_value).alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Equals('x', LiteralValue.from_value('Test')),
            input={'x': ['Hello', 'Test', 'nah', 'test', 'Test']},
            output=[False, True, False, False, True],
        )"
309;aligned/schemas/transformation.py;beta;"class And(Transformation):

    first_key: str
    second_key: str

    name: str = 'and'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, first_key: str, second_key: str) -> None:
        self.first_key = first_key
        self.second_key = second_key

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.first_key].isnull() | df[self.second_key].isnull()),
            transformation=lambda dfv: dfv[self.first_key] & dfv[self.second_key],
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column(
            (
                pl.when(pl.col(self.first_key).is_not_null() & pl.col(self.second_key).is_not_null())
                .then(pl.col(self.first_key) & pl.col(self.second_key))
                .otherwise(pl.lit(None))
            ).alias(alias)
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            And('x', 'y'),
            input={'x': [False, True, True, False, None], 'y': [True, False, True, False, False]},
            output=[False, False, True, False, np.nan],
        )"
310;aligned/schemas/transformation.py;beta;"class Or(Transformation):

    first_key: str
    second_key: str

    name: str = 'or'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, first_key: str, second_key: str) -> None:
        self.first_key = first_key
        self.second_key = second_key

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.first_key) | pl.col(self.second_key)).alias(alias))

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        df[self.first_key].__invert__
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.first_key].isnull() | df[self.second_key].isnull()),
            transformation=lambda dfv: dfv[self.first_key] | dfv[self.second_key],
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Or('x', 'y'),
            input={'x': [False, True, True, False, None], 'y': [True, False, True, False, False]},
            output=[True, True, True, False, np.nan],
        )"
311;aligned/schemas/transformation.py;beta;"class Inverse(Transformation):

    key: str

    name: str = 'inverse'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, key: str) -> None:
        self.key = key

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isnull()),
            transformation=lambda dfv: dfv[self.key] != True,  # noqa: E712
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((~pl.col(self.key)).alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Inverse('x'),
            input={'x': [False, True, True, False, None]},
            output=[True, False, False, True, np.nan],
        )"
312;aligned/schemas/transformation.py;beta;"class NotEquals(Transformation):

    key: str
    value: LiteralValue

    name: str = 'not-equals'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, key: str, value: str) -> None:
        self.key = key
        self.value = value

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key] != self.value.python_value

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.key) != self.value.python_value).alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            NotEquals('x', LiteralValue.from_value('Test')),
            input={'x': ['Hello', 'Test', 'nah', 'test', 'Test']},
            output=[True, False, True, True, False],
        )"
313;aligned/schemas/transformation.py;beta;"class GreaterThenValue(Transformation):

    key: str
    value: float

    name: str = 'gt'
    dtype: FeatureType = FeatureType('').bool

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key] > self.value

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.key) > self.value).alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            GreaterThenValue(key='x', value=2),
            input={'x': [1, 2, 3, nan]},
            output=[False, False, True, False],
        )"
314;aligned/schemas/transformation.py;beta;"class GreaterThen(Transformation):

    left_key: str
    right_key: str

    name: str = field(default='gtf')
    dtype: FeatureType = field(default=FeatureType('').bool)

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.left_key] > df[self.right_key]

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.left_key) > pl.col(self.right_key)).alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            GreaterThen(left_key='x', right_key='y'),
            input={'x': [1, 2, 3, nan, 5], 'y': [3, 2, 1, 5, nan]},
            output=[False, False, True, False, False],
        )"
315;aligned/schemas/transformation.py;beta;"class GreaterThenOrEqual(Transformation):

    key: str
    value: float

    name: str = 'gte'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, key: str, value: float) -> None:
        self.key = key
        self.value = value

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isna() | df[self.key].isnull()),
            transformation=lambda dfv: dfv[self.key] >= self.value,
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.key) >= self.value).alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            GreaterThenOrEqual(key='x', value=2),
            input={'x': [1, 2, 3, None]},
            output=[False, True, True, nan],
        )"
316;aligned/schemas/transformation.py;beta;"class LowerThen(Transformation):

    key: str
    value: float

    name: str = 'lt'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, key: str, value: float) -> None:
        self.key = key
        self.value = value

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isna() | df[self.key].isnull()),
            transformation=lambda dfv: dfv[self.key] < self.value,
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.key) < self.value).alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            LowerThen(key='x', value=2), input={'x': [1, 2, 3, None]}, output=[True, False, False, nan]
        )"
317;aligned/schemas/transformation.py;beta;"class LowerThenOrEqual(Transformation):

    key: str
    value: float

    name: str = 'lte'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, key: str, value: float) -> None:
        self.key = key
        self.value = value

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key) <= self.value

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isna() | df[self.key].isnull()),
            transformation=lambda dfv: dfv[self.key] <= self.value,
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            LowerThenOrEqual(key='x', value=2), input={'x': [1, 2, 3, None]}, output=[True, True, False, nan]
        )"
318;aligned/schemas/transformation.py;beta;"class Subtraction(Transformation, PsqlTransformation, RedshiftTransformation):

    front: str
    behind: str

    name: str = 'sub'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, front: str, behind: str) -> None:
        self.front = front
        self.behind = behind

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.front) - pl.col(self.behind)

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.front].isna() | df[self.behind].isna()),
            transformation=lambda dfv: dfv[self.front] - dfv[self.behind],
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            Subtraction(front='x', behind='y'),
            input={'x': [1, 2, 0, None, 1], 'y': [1, 0, 2, 1, None]},
            output=[0, 2, -2, nan, nan],
        )

    def as_psql(self) -> str:
        return f'{self.front} - {self.behind}'"
319;aligned/schemas/transformation.py;beta;"class AdditionValue(Transformation):

    feature: str
    value: LiteralValue

    name: str = 'add_value'
    dtype: FeatureType = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.feature] + self.value.python_value

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.feature) + pl.lit(self.value.python_value)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            AdditionValue(feature='x', value=LiteralValue.from_value(2)),
            input={'x': [1, 2, 0, None, 1], 'y': [1, 0, 2, 1, None]},
            output=[3, 4, 2, nan, 3],
        )"
320;aligned/schemas/transformation.py;beta;"class Multiply(Transformation, PsqlTransformation, RedshiftTransformation):

    front: str
    behind: str

    name: str = 'mul'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, front: str, behind: str) -> None:
        self.front = front
        self.behind = behind

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.front].isna() | df[self.behind].isna()),
            transformation=lambda dfv: dfv[self.front] * dfv[self.behind],
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.front) * pl.col(self.behind)

    def as_psql(self) -> str:
        return f'{self.front} * {self.behind}'"
321;aligned/schemas/transformation.py;beta;"class MultiplyValue(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    value: LiteralValue

    name: str = 'mul_val'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, key: str, value: LiteralValue) -> None:
        self.key = key
        self.value = value

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key) * pl.lit(self.value.python_value)

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key] * self.value.python_value

    def as_psql(self) -> str:
        return f""{self.key} * '{self.value.python_value}'"""
322;aligned/schemas/transformation.py;beta;"class Addition(Transformation, PsqlTransformation, RedshiftTransformation):

    front: str
    behind: str

    name: str = 'add'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, front: str, behind: str) -> None:
        self.front = front
        self.behind = behind

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.front].isna() | df[self.behind].isna()),
            transformation=lambda dfv: dfv[self.front] + dfv[self.behind],
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.front) + pl.col(self.behind)

    def as_psql(self) -> str:
        return f'{self.front} + {self.behind}'

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            Addition(front='x', behind='y'),
            input={'x': [1, 2, 0, None, 1], 'y': [1, 0, 2, 1, None]},
            output=[2, 2, 2, nan, nan],
        )"
323;aligned/schemas/transformation.py;beta;"class TimeDifference(Transformation, PsqlTransformation, RedshiftTransformation):

    front: str
    behind: str
    unit: str

    name: str = 'time-diff'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, front: str, behind: str, unit: str = 's') -> None:
        self.front = front
        self.behind = behind
        self.unit = unit

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.front].isna() | df[self.behind].isna()),
            transformation=lambda dfv: (dfv[self.front] - dfv[self.behind]) / np.timedelta64(1, self.unit),
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column((pl.col(self.front) - pl.col(self.behind)).dt.seconds().alias(alias))

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            TimeDifference(front='x', behind='y'),
            input={
                'x': [
                    datetime.fromtimestamp(1),
                    datetime.fromtimestamp(2),
                    datetime.fromtimestamp(0),
                    None,
                    datetime.fromtimestamp(1),
                ],
                'y': [
                    datetime.fromtimestamp(1),
                    datetime.fromtimestamp(0),
                    datetime.fromtimestamp(2),
                    datetime.fromtimestamp(1),
                    None,
                ],
            },
            output=[0, 2, -2, nan, nan],
        )

    def as_psql(self) -> str:
        return f""DATEDIFF('sec', {self.behind}, {self.front})"""
324;aligned/schemas/transformation.py;beta;"class Logarithm(Transformation):

    key: str

    name: str = 'log'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, key: str) -> None:
        self.key = key

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isna() | (df[self.key] <= 0)),
            transformation=lambda dfv: np.log(dfv[self.key]),
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column(
            (pl.when(pl.col(self.key) > 0).then(pl.col(self.key).log()).otherwise(pl.lit(None))).alias(alias)
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            Logarithm('x'), input={'x': [1, 0, np.e, None, -1]}, output=[0, nan, 1, nan, nan]
        )"
325;aligned/schemas/transformation.py;beta;"class LogarithmOnePluss(Transformation):

    key: str

    name: str = 'log1p'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, key: str) -> None:
        self.key = key

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isna() | (df[self.key] <= -1)),
            transformation=lambda dfv: np.log1p(dfv[self.key]),
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column(
            (pl.when(pl.col(self.key) > -1).then((pl.col(self.key) + 1).log()).otherwise(pl.lit(None))).alias(
                alias
            )
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            LogarithmOnePluss('x'),
            input={'x': [1, 0, np.e - 1, None, -1]},
            output=[0.6931471806, 0, 1, nan, nan],
        )"
326;aligned/schemas/transformation.py;beta;"class ToNumerical(Transformation):

    key: str

    name: str = 'to-num'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, key: str) -> None:
        self.key = key

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        from pandas import to_numeric

        return to_numeric(df[self.key], errors='coerce')

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).cast(pl.Float64)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            ToNumerical('x'),
            input={'x': ['1', '0', '10.5', None, nan, '-20']},
            output=[1, 0, 10.5, nan, nan, -20],
        )"
327;aligned/schemas/transformation.py;beta;"class DateComponent(Transformation):

    key: str
    component: str

    name: str = 'date-component'
    dtype: FeatureType = FeatureType('').int32

    def __init__(self, key: str, component: str) -> None:
        self.key = key
        self.component = component

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:

        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isna()),
            transformation=lambda dfv: getattr(dfv[self.key].dt, self.component),
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        pl.col(self.key).str.strptime(pl.Datetime, strict=False)
        pl.when(pl.col(self.key))

        col = pl.col(self.key).cast(pl.Datetime).dt
        match self.component:
            case 'day':
                expr = col.day()
            case 'days':
                expr = col.days()
            case 'epoch':
                expr = col.epoch()
            case 'hour':
                expr = col.hour()
            case 'hours':
                expr = col.hours()
            case 'iso_year':
                expr = col.iso_year()
            case 'microsecond':
                expr = col.microsecond()
            case 'microseconds':
                expr = col.microseconds()
            case 'millisecond':
                expr = col.millisecond()
            case 'milliseconds':
                expr = col.milliseconds()
            case 'minute':
                expr = col.minute()
            case 'minutes':
                expr = col.minutes()
            case 'month':
                expr = col.month()
            case 'nanosecond':
                expr = col.nanosecond()
            case 'nanoseconds':
                expr = col.nanoseconds()
            case 'ordinal_day':
                expr = col.ordinal_day()
            case 'quarter':
                expr = col.quarter()
            case 'second':
                expr = col.second()
            case 'seconds':
                expr = col.seconds()
            case 'week':
                expr = col.week()
            case 'weekday':
                expr = col.weekday()
            case 'year':
                expr = col.year()
            case 'dayofweek':
                expr = col.weekday()
            case _:
                raise NotImplementedError(
                    f'Date component {self.component} is not implemented. Maybe setup a PR and contribute?'
                )
        return expr

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            DateComponent(key='x', component='hour'),
            input={
                'x': [
                    datetime.fromisoformat(value) if value else None
                    for value in ['2022-04-02T20:20:50', None, '2022-02-20T23:20:50', '1993-04-02T01:20:50']
                ]
            },
            output=[20, nan, 23, 1],
        )"
328;aligned/schemas/transformation.py;beta;"class Contains(Transformation):
    """"""Checks if a string value contains another string

    some_string = String()
    contains_a_char = some_string.contains(""a"")
    """"""

    key: str
    value: str

    name: str = 'contains'
    dtype: FeatureType = FeatureType('').bool

    def __init__(self, key: str, value: str) -> None:
        self.key = key
        self.value = value

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.key].isna()),
            transformation=lambda dfv: dfv[self.key].astype('str').str.contains(self.value),
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).str.contains(self.value)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Contains('x', 'es'),
            input={'x': ['Hello', 'Test', 'nah', 'test', 'espania', None]},
            output=[False, True, False, True, True, None],
        )"
329;aligned/schemas/transformation.py;beta;"class Ordinal(Transformation):

    key: str
    orders: list[str]

    @property
    def orders_dict(self) -> dict[str, int]:
        return {key: index for index, key in enumerate(self.orders)}

    name: str = 'ordinal'
    dtype: FeatureType = FeatureType('').int32

    def __init__(self, key: str, orders: list[str]) -> None:
        self.key = key
        self.orders = orders

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key].map(self.orders_dict)

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        mapper = pl.DataFrame({self.key: list(self.orders), alias: list(range(0, len(self.orders)))})
        return df.join(mapper.lazy(), on=self.key, how='left')

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            Ordinal('x', ['a', 'b', 'c', 'd']),
            input={'x': ['a', 'b', 'a', None, 'd', 'p']},
            output=[0, 1, 0, nan, 3, nan],
        )"
330;aligned/schemas/transformation.py;beta;"class ReplaceStrings(Transformation):

    key: str
    values: dict[str, str]

    name: str = 'replace'
    dtype: FeatureType = FeatureType('').string

    def __init__(self, key: str, values: dict[str, str]) -> None:
        self.key = key
        self.values = values

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        temp_df = df[self.key].copy()
        mask = ~(df[self.key].isna() | df[self.key].isnull())
        temp_df.loc[~mask] = np.nan
        for k, v in self.values.items():
            temp_df.loc[mask] = temp_df.loc[mask].astype(str).str.replace(k, v)

        return temp_df

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        raise NotImplementedError()

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            ReplaceStrings('x', {r'20[\s]*-[\s]*10': '15', ' ': '', '.': '', '10-20': '15', '20\\+': '30'}),
            input={'x': [' 20', '10 - 20', '.yeah', '20+', None, '20   - 10']},
            output=['20', '15', 'yeah', '30', nan, '15'],
        )"
331;aligned/schemas/transformation.py;beta;"class Ratio(Transformation):

    numerator: str
    denumerator: str

    name: str = 'ratio'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, numerator: str, denumerator: str) -> None:
        self.numerator = numerator
        self.denumerator = denumerator

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(
                df[self.numerator].isna() | df[self.denumerator].isna() | df[self.denumerator] == 0
            ),
            transformation=lambda dfv: dfv[self.numerator].astype(float)
            / dfv[self.denumerator].astype(float),
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return (
            pl.when(pl.col(self.denumerator) != 0)
            .then(pl.col(self.numerator) / pl.col(self.denumerator))
            .otherwise(pl.lit(None))
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            Ratio('x', 'y'),
            input={'x': [1, 2, 0, 1, None, 9], 'y': [1, 0, 1, 4, 2, None]},
            output=[1, nan, 0, 0.25, nan, nan],
        )"
332;aligned/schemas/transformation.py;beta;"class DivideDenumeratorValue(Transformation):

    numerator: str
    denumerator: LiteralValue

    name: str = 'div_denum_val'
    dtype: FeatureType = FeatureType('').float

    def __init__(self, numerator: str, denumerator: LiteralValue) -> None:
        self.numerator = numerator
        self.denumerator = denumerator
        assert denumerator.python_value != 0

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return gracefull_transformation(
            df,
            is_valid_mask=~(df[self.numerator].isna()),
            transformation=lambda dfv: dfv[self.numerator].astype(float) / self.denumerator.python_value,
        )

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.numerator) / pl.lit(self.denumerator.python_value)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        from numpy import nan

        return TransformationTestDefinition(
            DivideDenumeratorValue('x', LiteralValue.from_value(2)),
            input={'x': [1, 2, 0, 1, None, 9]},
            output=[0.5, 1, 0, 0.5, nan, 4.5],
        )"
333;aligned/schemas/transformation.py;beta;"class IsIn(Transformation):

    values: list
    key: str

    name = 'isin'
    dtype = FeatureType('').bool

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key].isin(self.values)

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).is_in(self.values)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            IsIn(values=['hello', 'test'], key='x'),
            input={'x': ['No', 'Hello', 'hello', 'test', 'nah', 'nehtest']},
            output=[False, False, True, True, False, False],
        )"
334;aligned/schemas/transformation.py;beta;"class FillNaValues(Transformation):

    key: str
    value: LiteralValue
    dtype: FeatureType

    name: str = 'fill_missing'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key].fillna(self.value.python_value)

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        if self.dtype == FeatureType('').float:
            return pl.col(self.key).fill_nan(self.value.python_value).fill_null(self.value.python_value)

        else:
            return pl.col(self.key).fill_null(self.value.python_value)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            FillNaValues('x', LiteralValue.from_value(3), dtype=FeatureType('').int32),
            input={'x': [1, 1, None, None, 3, 3, None, 4, 5, None]},
            output=[1, 1, 3, 3, 3, 3, 3, 4, 5, 3],
        )"
335;aligned/schemas/transformation.py;beta;"class CopyTransformation(Transformation):
    key: str
    dtype: FeatureType

    name: str = 'nothing'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key]

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).alias(alias)"
336;aligned/schemas/transformation.py;beta;"class Floor(Transformation):

    key: str
    dtype: FeatureType = FeatureType('').int64

    name: str = 'floor'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        from numpy import floor

        return floor(df[self.key])

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).floor().alias(alias)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Floor('x'),
            input={'x': [1.3, 1.9, None]},
            output=[1, 1, None],
        )"
337;aligned/schemas/transformation.py;beta;"class Ceil(Transformation):

    key: str
    dtype: FeatureType = FeatureType('').int64

    name: str = 'ceil'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        from numpy import ceil

        return ceil(df[self.key])

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).ceil().alias(alias)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Ceil('x'),
            input={'x': [1.3, 1.9, None]},
            output=[2, 2, None],
        )"
338;aligned/schemas/transformation.py;beta;"class Round(Transformation):

    key: str
    dtype: FeatureType = FeatureType('').int64

    name: str = 'round'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        from numpy import round

        return round(df[self.key])

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).round(0).alias(alias)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Round('x'),
            input={'x': [1.3, 1.9, None]},
            output=[1, 2, None],
        )"
339;aligned/schemas/transformation.py;beta;"class Absolute(Transformation):

    key: str
    dtype: FeatureType = FeatureType('').float

    name: str = 'abs'

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        from numpy import abs

        return abs(df[self.key])

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).abs().alias(alias)

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            Absolute('x'),
            input={'x': [-13, 19, None]},
            output=[13, 19, None],
        )"
340;aligned/schemas/transformation.py;beta;"class MapArgMax(Transformation):

    column_mappings: dict[str, LiteralValue]
    name = 'map_arg_max'

    @property
    def dtype(self) -> FeatureType:
        return list(self.column_mappings.values())[0].dtype

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        pl_df = await self.transform_polars(pl.from_pandas(df).lazy(), 'feature')
        return pl_df.collect().to_pandas()['feature']

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        expr: pl.Expr = pl.lit(None)

        if len(self.column_mappings) == 1:
            key, value = list(self.column_mappings.items())[0]
            if self.dtype == FeatureType('').bool:
                expr = pl.when(pl.col(key) > 0.5).then(value.python_value).otherwise(not value.python_value)
            elif self.dtype == FeatureType('').string:
                expr = (
                    pl.when(pl.col(key) > 0.5).then(value.python_value).otherwise(f'not {value.python_value}')
                )
            else:
                expr = pl.when(pl.col(key) > 0.5).then(value.python_value).otherwise(pl.lit(None))
            return df.with_column(expr.alias(alias))
        else:
            features = list(self.column_mappings.keys())
            arg_max_alias = f'{alias}_arg_max'
            array_row_alias = f'{alias}_row'
            mapper = pl.DataFrame(
                {
                    alias: [self.column_mappings[feature].python_value for feature in features],
                    arg_max_alias: list(range(0, len(features))),
                }
            ).with_column(pl.col(arg_max_alias).cast(pl.UInt32))
            sub = df.with_column(pl.concat_list(pl.col(features)).alias(array_row_alias)).with_column(
                pl.col(array_row_alias).arr.arg_max().alias(arg_max_alias)
            )
            return sub.join(mapper.lazy(), on=arg_max_alias, how='left').select(
                pl.exclude([arg_max_alias, array_row_alias])
            )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            MapArgMax(
                {
                    'a_prob': LiteralValue.from_value('a'),
                    'b_prob': LiteralValue.from_value('b'),
                    'c_prob': LiteralValue.from_value('c'),
                }
            ),
            input={'a_prob': [0.01, 0.9, 0.25], 'b_prob': [0.9, 0.05, 0.15], 'c_prob': [0.09, 0.05, 0.6]},
            output=['b', 'a', 'c'],
        )"
341;aligned/schemas/transformation.py;beta;"class WordVectoriser(Transformation):
    key: str
    model: TextVectoriserModel

    name = 'word_vectoriser'
    dtype = FeatureType('').embedding

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return await self.model.vectorise_pandas(df[self.key])

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return await self.model.vectorise_polars(df, self.key, alias)"
342;aligned/schemas/transformation.py;beta;"class LoadImageUrl(Transformation):

    image_url_key: str

    name = 'load_image'
    dtype = FeatureType('').array

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        import asyncio
        from io import BytesIO

        import numpy as np
        from PIL import Image

        from aligned.sources.local import StorageFileSource

        urls = df.select(self.image_url_key).collect()[self.image_url_key]

        images = await asyncio.gather(*[StorageFileSource(url).read() for url in urls.to_list()])
        data = [np.asarray(Image.open(BytesIO(buffer))) for buffer in images]
        image_dfs = pl.DataFrame({alias: data})
        return df.with_context(image_dfs.lazy()).select(pl.all())"
343;aligned/schemas/transformation.py;beta;"class GrayscaleImage(Transformation):

    image_key: str

    name = 'grayscale_image'
    dtype = FeatureType('').array

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        import numpy as np

        def grayscale(images):
            return pl.Series(
                [np.mean(image, axis=2) if len(image.shape) == 3 else image for image in images.to_list()]
            )

        return pl.col(self.image_key).map(grayscale).alias(alias)"
344;aligned/schemas/transformation.py;beta;"class Power(Transformation):

    key: str
    power: LiteralValue
    name = 'power'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key] ** self.power.python_value

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).pow(self.power.python_value)"
345;aligned/schemas/transformation.py;beta;"class PowerFeature(Transformation):

    key: str
    power_key: float
    name = 'power_feat'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key] ** df[self.power_key]

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).pow(pl.col(self.power_key))"
346;aligned/schemas/transformation.py;beta;"class AppendConstString(Transformation):

    key: str
    string: str

    name = 'append_const_string'
    dtype = FeatureType('').string

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key] + self.string

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.concat_str([pl.col(self.key).fill_null(''), pl.lit(self.string)], sep='').alias(alias)"
347;aligned/schemas/transformation.py;beta;"class AppendStrings(Transformation):

    first_key: str
    second_key: str
    sep: str

    name = 'append_strings'
    dtype = FeatureType('').string

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.first_key] + self.sep + df[self.second_key]

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_column(
            pl.concat_str(
                [pl.col(self.first_key).fill_null(''), pl.col(self.second_key).fill_null('')], sep=self.sep
            ).alias(alias)
        )"
348;aligned/schemas/transformation.py;beta;"class ConcatStringAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]
    separator: str = field(default=' ')

    name = 'concat_string_agg'
    dtype = FeatureType('').string

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return (await self.transform_polars(pl.from_pandas(df).lazy())).collect().to_pandas()[self.name]

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return df.with_columns(
            pl.concat_str(pl.col(self.key), sep=self.separator).over(self.group_keys).alias(alias)
        )

    def as_psql(self) -> str:
        return f""array_to_string(array_agg({self.key}), '{self.separator}')""

    def as_redshift(self) -> str:
        return f""listagg(\""{self.key}\"", '{self.separator}') within group (order by \""{self.key}\"")"""
349;aligned/schemas/transformation.py;beta;"class SumAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'sum_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.sum(self.key)

    def as_psql(self) -> str:
        return f'SUM({self.key})'"
350;aligned/schemas/transformation.py;beta;"class MeanAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'mean_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).mean()

    def as_psql(self) -> str:
        return f'AVG({self.key})'"
351;aligned/schemas/transformation.py;beta;"class MinAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'min_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).min()

    def as_psql(self) -> str:
        return f'MIN({self.key})'"
352;aligned/schemas/transformation.py;beta;"class MaxAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'max_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).max()

    def as_psql(self) -> str:
        return f'MAX({self.key})'"
353;aligned/schemas/transformation.py;beta;"class CountAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'count_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).count()

    def as_psql(self) -> str:
        return f'COUNT({self.key})'"
354;aligned/schemas/transformation.py;beta;"class CountDistinctAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'count_distinct_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).unique_counts()

    def as_psql(self) -> str:
        return f'COUNT(DISTINCT {self.key})'"
355;aligned/schemas/transformation.py;beta;"class StdAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'std_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).std()

    def as_psql(self) -> str:
        return f'STDDEV({self.key})'"
356;aligned/schemas/transformation.py;beta;"class VarianceAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'var_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).var()

    def as_psql(self) -> str:
        return f'variance({self.key})'"
357;aligned/schemas/transformation.py;beta;"class MedianAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    group_keys: list[str]

    name = 'median_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).median()

    def as_psql(self) -> str:
        return f'PERCENTILE_CONT(0.5) WITHIN GROUP(ORDER BY {self.key})'"
358;aligned/schemas/transformation.py;beta;"class PercentileAggregation(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    percentile: float
    group_keys: list[str]

    name = 'percentile_agg'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        raise NotImplementedError()

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).quantile(self.percentile)

    def as_psql(self) -> str:
        return f'PERCENTILE_CONT({self.percentile}) WITHIN GROUP(ORDER BY {self.key})'"
359;aligned/schemas/transformation.py;beta;"class Clip(Transformation, PsqlTransformation, RedshiftTransformation):

    key: str
    lower: LiteralValue
    upper: LiteralValue

    name = 'clip'
    dtype = FeatureType('').float

    async def transform_pandas(self, df: pd.DataFrame) -> pd.Series:
        return df[self.key].clip(lower=self.lower.python_value, upper=self.upper.python_value)

    async def transform_polars(self, df: pl.LazyFrame, alias: str) -> pl.LazyFrame | pl.Expr:
        return pl.col(self.key).clip(min_val=self.lower.python_value, max_val=self.upper.python_value)

    def as_psql(self) -> str:
        return (
            f'CASE WHEN {self.key} < {self.lower} THEN {self.lower} WHEN '
            f'{self.key} > {self.upper} THEN {self.upper} ELSE {self.key} END'
        )

    @staticmethod
    def test_definition() -> TransformationTestDefinition:
        return TransformationTestDefinition(
            transformation=Clip(key='a', lower=LiteralValue.from_value(0), upper=LiteralValue.from_value(1)),
            input={'a': [-1, 0.1, 0.9, 2]},
            output=[0, 0.1, 0.9, 1],
        )"
360;aligned/schemas/vector_storage.py;beta;"class VectorStorageFactory:

    supported_storages: dict[str, type[VectorStorage]]

    _shared: VectorStorageFactory | None = None

    def __init__(self):
        self.supported_storages = {}

        for storage in VectorStorage.__subclasses__():
            self.supported_storages[storage.type_name] = storage

    @classmethod
    def shared(cls) -> VectorStorageFactory:
        if cls._shared is None:
            cls._shared = VectorStorageFactory()
        return cls._shared"
361;aligned/schemas/vector_storage.py;beta;"class VectorStorage(Codable, SerializableType):

    type_name: str

    def _serialize(self) -> dict:
        assert (
            self.type_name in VectorStorageFactory.shared().supported_storages
        ), f'VectorStorage {self.type_name} is not supported'
        return self.to_dict()

    @classmethod
    def _deserialize(cls, value: dict) -> VectorStorage:
        name = value['type_name']
        if name not in VectorStorageFactory.shared().supported_storages:
            raise ValueError(f'VectorStorage {name} is not supported')
        del value['type_name']
        return VectorStorageFactory.shared().supported_storages[name].from_dict(value)

    async def create_index(self, index: VectorIndex) -> None:
        raise NotImplementedError()

    async def upsert_polars(self, df: pl.LazyFrame, index: VectorIndex) -> None:
        raise NotImplementedError()"
362;aligned/schemas/vector_storage.py;beta;"class VectorIndex(Codable):

    location: FeatureLocation
    vector: Feature
    vector_dim: int
    metadata: list[Feature]
    storage: VectorStorage
    entities: list[Feature]

    def __pre_serialize__(self) -> VectorIndex:
        assert isinstance(self.vector_dim, int), f'got {self.vector_dim}, expected int'
        assert isinstance(self.storage, VectorStorage)
        assert isinstance(self.location, FeatureLocation)
        assert isinstance(self.vector, Feature)
        assert isinstance(self.metadata, list), f'metadata must be a list, got {type(self.metadata)}'
        assert isinstance(self.entities, list), f'entities must be a list, got {type(self.entities)}'

        return self"
363;aligned/server.py;beta;"class APIFeatureRequest(BaseModel):
    entities: dict[str, list]
    features: list[str]"
364;aligned/server.py;beta;"class TopicInfo:
    name: str
    views: list[CompiledFeatureView]
    mappings: dict[str, str]"
365;aligned/server.py;beta;"class FastAPIServer:
    @staticmethod
    def write_to_topic_path(topic: TopicInfo, feature_store: FeatureStore, app: FastAPI) -> None:

        required_features: set[Feature] = set()
        for view in topic.views:
            required_features.update(view.entities.union(view.features))

        view_names = [view.name for view in topic.views]
        mappings: dict[str, list[str]] = {
            output: input_path.split('.') for input_path, output in topic.mappings.items()
        }

        write_api_schema = {
            'requestBody': {
                'content': {
                    'application/json': {
                        'schema': {
                            'required': [feature.name for feature in required_features],
                            'type': 'object',
                            'properties': {
                                feature.name: {
                                    'type': 'array',
                                    'items': {'type': feature.dtype.name},
                                }
                                for feature in required_features
                            },
                        }
                    }
                },
                'required': True,
            },
        }

        @app.post(f'/topics/{topic.name}/write', openapi_extra=write_api_schema)
        async def write(feature_values: dict) -> None:

            for output_name, input_path in mappings.items():

                if output_name in feature_values:
                    continue

                if len(input_path) == 1:
                    feature_values[output_name] = feature_values[input_path[0]]
                else:
                    from functools import reduce

                    def find_path_variable(values: dict, key: str) -> Any:
                        return values.get(key, {})

                    values = [
                        # Select the value based on the key path given
                        reduce(find_path_variable, input_path[1:], value)  # initial value
                        for value in feature_values[input_path[0]]
                    ]
                    feature_values[output_name] = [value if value != {} else None for value in values]

            await asyncio.gather(
                *[feature_store.feature_view(view_name).write(feature_values) for view_name in view_names]
            )

    @staticmethod
    def feature_view_path(name: str, feature_store: FeatureStore, app: FastAPI) -> None:
        @app.post(f'/feature-views/{name}/all')
        async def all(limit: int | None = None) -> dict:
            df = await feature_store.feature_view(name).all(limit=limit).to_pandas()
            df.replace(nan, value=None, inplace=True)
            return df.to_dict('list')

    @staticmethod
    def model_path(name: str, feature_store: FeatureStore, app: FastAPI) -> None:
        from aligned.feature_store import RawStringFeatureRequest

        model = feature_store.models[name]
        features = {f'{feature.location.identifier}:{feature.name}' for feature in model.features}
        feature_request = feature_store.requests_for(RawStringFeatureRequest(features))

        entities: set[Feature] = set()
        for request in feature_request.needed_requests:
            entities.update(request.entities)

        required_features = entities.copy()
        for request in feature_request.needed_requests:
            required_features.update(request.all_required_features)

        properties = {
            entity.name: {
                'type': 'array',
                'items': {'type': entity.dtype.name},
            }
            for entity in entities
        }
        needs_event_timestamp = feature_request.needs_event_timestamp
        if needs_event_timestamp:
            properties['event_timestamp'] = {'type': 'array', 'items': {'type': 'string'}}

        featch_api_schema = {
            'requestBody': {
                'content': {
                    'application/json': {
                        'schema': {
                            'required': [entity.name for entity in entities] + ['event_timestamp'],
                            'type': 'object',
                            'properties': properties,
                        }
                    }
                },
                'required': True,
            },
        }

        # Using POST as this can have a body with the fact / entity table
        @app.post(f'/models/{name}', openapi_extra=featch_api_schema)
        async def get_model(entity_values: dict) -> str:
            missing_entities = {entity.name for entity in entities if entity.name not in entity_values}
            if missing_entities:
                raise HTTPException(status_code=400, detail=f'Missing entity values {missing_entities}')

            if needs_event_timestamp:
                entity_values['event_timestamp'] = [
                    datetime.fromtimestamp(value)
                    if isinstance(value, (float, int))
                    else datetime.fromisoformat(value)
                    for value in entity_values['event_timestamp']
                ]

            df = await feature_store.model(name).features_for(entity_values).to_polars()
            pandas_df = df.collect().to_pandas()
            orient = 'values'
            body = ','.join(
                [f'""{column}"":{pandas_df[column].to_json(orient=orient)}' for column in pandas_df.columns]
            )
            return Response(content=f'{{{body}}}', media_type='application/json')

    @staticmethod
    def app(feature_store: FeatureStore) -> FastAPI:
        from asgi_correlation_id import CorrelationIdMiddleware
        from fastapi import FastAPI
        from fastapi.middleware import Middleware
        from prometheus_fastapi_instrumentator import Instrumentator
        from starlette.status import HTTP_200_OK

        app = FastAPI(middleware=[Middleware(CorrelationIdMiddleware)])
        app.docs_url = '/docs'

        @app.on_event('startup')
        async def startup():
            Instrumentator().instrument(app).expose(app)

        @app.get('health', status_code=HTTP_200_OK)
        def health() -> None:
            return

        for model in feature_store.all_models:
            FastAPIServer.model_path(model, feature_store, app)

        can_write_to_store = isinstance(feature_store.feature_source, WritableFeatureSource)

        topics: dict[str, TopicInfo] = {}

        for feature_view in feature_store.feature_views.values():
            if not (stream_source := feature_view.stream_data_source):
                continue

            if isinstance(stream_source, HttpStreamSource):
                topic_name = stream_source.topic_name
                if topic_name not in topics:
                    topics[topic_name] = TopicInfo(
                        name=topic_name, views=[feature_view], mappings=stream_source.mappings
                    )
                else:
                    info = topics[topic_name]

                    topics[topic_name] = TopicInfo(
                        name=topic_name,
                        views=info.views + [feature_view],
                        mappings=info.mappings | stream_source.mappings,
                    )

        if can_write_to_store:
            for topic in topics.values():
                FastAPIServer.write_to_topic_path(topic, feature_store, app)
        else:
            logger.info(
                (
                    'Warning! The server is not using a WritableFeatureSource, ',
                    'and can therefore not setup stream sources',
                )
            )

        @app.get('/')
        async def root() -> RedirectResponse:
            return RedirectResponse('/docs')

        @app.post('/features')
        async def features(payload: APIFeatureRequest) -> dict:
            df = await feature_store.features_for(
                payload.entities,
                features=payload.features,
            ).to_pandas()
            orient = 'values'
            body = ','.join([f'""{column}"":{df[column].to_json(orient=orient)}' for column in df.columns])
            return Response(content=f'{{{body}}}', media_type='application/json')

        return app

    @staticmethod
    def run(
        feature_store: FeatureStore,
        host: str | None = None,
        port: int | None = None,
        workers: int | None = None,
    ) -> None:
        import uvicorn

        app = FastAPIServer.app(feature_store)

        uvicorn.run(app, host=host or '127.0.0.1', port=port or 8000, workers=workers or workers)"
366;aligned/sources/local.py;beta;"class StorageFileReference:
    """"""
    A reference to a file that can be loaded as bytes.

    The bytes can contain anything, potentially a FeatureStore definition
    """"""

    async def read(self) -> bytes:
        raise NotImplementedError()

    async def write(self, content: bytes) -> None:
        raise NotImplementedError()

    async def feature_store(self) -> FeatureStore:
        from aligned import FeatureStore
        from aligned.schemas.repo_definition import RepoDefinition

        file = await self.read()
        return FeatureStore.from_definition(RepoDefinition.from_json(file))"
367;aligned/sources/local.py;beta;"class CsvConfig(Codable):
    """"""
    A config for how a CSV file should be loaded
    """"""

    seperator: str = field(default=',')
    compression: Literal['infer', 'gzip', 'bz2', 'zip', 'xz', 'zstd'] = field(default='infer')
    should_write_index: bool = field(default=False)"
368;aligned/sources/local.py;beta;"class CsvFileSource(BatchDataSource, ColumnFeatureMappable, StatisticEricher, DataFileReference):
    """"""
    A source pointing to a CSV file
    """"""

    path: str
    mapping_keys: dict[str, str] = field(default_factory=dict)
    csv_config: CsvConfig = field(default_factory=CsvConfig)

    type_name: str = 'csv'

    def job_group_key(self) -> str:
        return f'{self.type_name}/{self.path}'

    def __hash__(self) -> int:
        return hash(self.job_group_key())

    async def read_pandas(self) -> pd.DataFrame:
        try:
            return pd.read_csv(
                self.path, sep=self.csv_config.seperator, compression=self.csv_config.compression
            )
        except FileNotFoundError:
            raise UnableToFindFileException()
        except HTTPStatusError:
            raise UnableToFindFileException()

    async def to_polars(self) -> pl.LazyFrame:

        if self.path.startswith('http'):
            from io import BytesIO

            buffer = await HttpStorage().read(self.path)
            io_buffer = BytesIO(buffer)
            io_buffer.seek(0)
            return pl.read_csv(io_buffer, sep=self.csv_config.seperator).lazy()

        return pl.scan_csv(self.path, sep=self.csv_config.seperator)

    async def write_pandas(self, df: pd.DataFrame) -> None:
        df.to_csv(
            self.path,
            sep=self.csv_config.seperator,
            compression=self.csv_config.compression,
            index=self.csv_config.should_write_index,
        )

    def std(
        self, columns: set[str], time: TimespanSelector | None = None, limit: int | None = None
    ) -> Enricher:
        return LoadedStatEnricher(
            stat='std',
            columns=list(columns),
            enricher=self.enricher().selector(time, limit),
            mapping_keys=self.mapping_keys,
        )

    def mean(
        self, columns: set[str], time: TimespanSelector | None = None, limit: int | None = None
    ) -> Enricher:
        return LoadedStatEnricher(
            stat='mean',
            columns=list(columns),
            enricher=self.enricher().selector(time, limit),
            mapping_keys=self.mapping_keys,
        )

    def enricher(self) -> CsvFileEnricher:
        return CsvFileEnricher(file=self.path)

    def all_data(self, request: RetrivalRequest, limit: int | None) -> FullExtractJob:
        return FileFullJob(self, request, limit)

    def all_between_dates(
        self, request: RetrivalRequest, start_date: datetime, end_date: datetime
    ) -> DateRangeJob:
        return FileDateJob(source=self, request=request, start_date=start_date, end_date=end_date)

    @classmethod
    def multi_source_features_for(
        cls, facts: RetrivalJob, requests: list[tuple[CsvFileSource, RetrivalRequest]]
    ) -> FactualRetrivalJob:
        sources = {source for source, _ in requests}
        if len(sources) != 1:
            raise ValueError(f'Only able to load one {requests} at a time')

        source = list(sources)[0]
        if not isinstance(source, cls):
            raise ValueError(f'Only {cls} is supported, recived: {source}')

        # Group based on config
        return FileFactualJob(
            source=source,
            requests=[request for _, request in requests],
            facts=facts,
        )"
369;aligned/sources/local.py;beta;"class ParquetConfig(Codable):
    """"""
    A config for how a CSV file should be loaded
    """"""

    engine: Literal['auto', 'pyarrow', 'fastparquet'] = field(default='auto')
    compression: Literal['snappy', 'gzip', 'brotli', None] = field(default='snappy')
    should_write_index: bool = field(default=False)"
370;aligned/sources/local.py;beta;"class ParquetFileSource(BatchDataSource, ColumnFeatureMappable, DataFileReference):
    """"""
    A source pointing to a Parquet file
    """"""

    path: str
    mapping_keys: dict[str, str] = field(default_factory=dict)
    config: ParquetConfig = field(default_factory=ParquetConfig)

    type_name: str = 'parquet'

    def job_group_key(self) -> str:
        return f'{self.type_name}/{self.path}'

    def __hash__(self) -> int:
        return hash(self.job_group_key())

    async def read_pandas(self) -> pd.DataFrame:
        try:
            return pd.read_parquet(self.path)
        except FileNotFoundError:
            raise UnableToFindFileException()
        except HTTPStatusError:
            raise UnableToFindFileException()

    async def write_pandas(self, df: pd.DataFrame) -> None:
        df.to_parquet(
            self.path,
            engine=self.config.engine,
            compression=self.config.compression,
            index=self.config.should_write_index,
        )

    async def to_polars(self) -> pl.LazyFrame:
        return pl.scan_parquet(self.path)

    async def write_polars(self, df: pl.LazyFrame) -> None:
        df.sink_parquet(self.path, compression=self.config.compression)

    def all_data(self, request: RetrivalRequest, limit: int | None) -> FullExtractJob:
        return FileFullJob(self, request, limit)

    def all_between_dates(
        self, request: RetrivalRequest, start_date: datetime, end_date: datetime
    ) -> DateRangeJob:
        return FileDateJob(source=self, request=request, start_date=start_date, end_date=end_date)

    @classmethod
    def multi_source_features_for(
        cls, facts: RetrivalJob, requests: list[tuple[ParquetFileSource, RetrivalRequest]]
    ) -> FactualRetrivalJob:
        if len(requests) != 1:
            raise ValueError(f'Only able to load one {requests} at a time')

        source = requests[0][0]
        if not isinstance(source, cls):
            raise ValueError(f'Only {cls} is supported, recived: {source}')

        # Group based on config
        return FileFactualJob(
            source=source,
            requests=[request for _, request in requests],
            facts=facts,
        )"
371;aligned/sources/local.py;beta;"class StorageFileSource(StorageFileReference):

    path: str

    @property
    def storage(self) -> Storage:
        if self.path.startswith('http'):
            return HttpStorage()
        else:
            return FileStorage()

    def __hash__(self) -> int:
        return hash(self.path)

    async def read(self) -> bytes:
        return await self.storage.read(self.path)

    async def write(self, content: bytes) -> None:
        await self.storage.write(self.path, content)"
372;aligned/sources/local.py;beta;"class FileSource:
    """"""
    A factory class, creating references to files.

    This therefore abstracts away the concrete classes the users wants.
    Therefore making them easier to discover.
    """"""

    @staticmethod
    def json_at(path: str) -> StorageFileSource:
        return StorageFileSource(path=path)

    @staticmethod
    def csv_at(
        path: str, mapping_keys: dict[str, str] | None = None, csv_config: CsvConfig | None = None
    ) -> CsvFileSource:
        return CsvFileSource(path, mapping_keys=mapping_keys or {}, csv_config=csv_config or CsvConfig())

    @staticmethod
    def parquet_at(
        path: str, mapping_keys: dict[str, str] | None = None, config: ParquetConfig | None = None
    ) -> ParquetFileSource:
        return ParquetFileSource(path=path, mapping_keys=mapping_keys or {}, config=config or ParquetConfig())

    @staticmethod
    def folder(path: str) -> Folder:
        return LocalFolder(base_path=Path(path))"
373;aligned/sources/local.py;beta;"class LocalFolder(Folder):

    base_path: Path
    name = 'local_folder'

    def file_at(self, path: Path) -> StorageFileReference:
        return StorageFileSource(path=str(self.base_path / path))"
374;aligned/sources/local.py;beta;"class LiteralReference(DataFileReference):
    """"""
    A"
375;aligned/sources/local.py;beta;"class containing a in mem pandas frame.

    This makes it easier standardise the interface when writing data.
    """"""

    file: pl.LazyFrame

    def __init__(self, file: pl.LazyFrame | pd.DataFrame) -> None:
        if isinstance(file, pd.DataFrame):
            self.file = pl.from_pandas(file).lazy()
        else:
            self.file = file

    def job_group_key(self) -> str:
        return str(uuid4())

    async def read_pandas(self) -> pd.DataFrame:
        return self.file.collect().to_pandas()

    async def to_polars(self) -> pl.LazyFrame:
        return self.file"
376;aligned/sources/psql.py;beta;"class PostgreSQLConfig(Codable):
    env_var: str
    schema: str | None = None

    @property
    def url(self) -> str:
        import os

        return os.environ[self.env_var]

    @staticmethod
    def from_url(url: str) -> PostgreSQLConfig:
        import os

        if 'PSQL_DATABASE' not in os.environ:
            os.environ['PSQL_DATABASE'] = url
        return PostgreSQLConfig(env_var='PSQL_DATABASE')

    @staticmethod
    def localhost(db: str, credentials: tuple[str, str] | None = None) -> PostgreSQLConfig:
        if credentials:
            return PostgreSQLConfig.from_url(f'postgresql://{credentials[0]}:{credentials[1]}@localhost/{db}')
        return PostgreSQLConfig.from_url(f'postgresql://localhost/{db}')

    def table(self, table: str, mapping_keys: dict[str, str] | None = None) -> PostgreSQLDataSource:
        return PostgreSQLDataSource(config=self, table=table, mapping_keys=mapping_keys or {})

    def data_enricher(self, sql: str, values: dict | None = None) -> Enricher:
        from aligned.enricher import SqlDatabaseEnricher

        return SqlDatabaseEnricher(self.env_var, sql, values)

    def entity_source(self, timestamp_column: str, sql: Callable[[str], str]) -> EntityDataSource:
        from aligned.compiler.model import SqlEntityDataSource

        return SqlEntityDataSource(sql, self.env_var, timestamp_column)

    def fetch(self, query: str) -> RetrivalJob:
        from aligned.psql.jobs import PostgreSqlJob

        return PostgreSqlJob(self, query)"
377;aligned/sources/psql.py;beta;"class PostgreSQLDataSource(BatchDataSource, ColumnFeatureMappable, StatisticEricher):

    config: PostgreSQLConfig
    table: str
    mapping_keys: dict[str, str]

    type_name = 'psql'

    def job_group_key(self) -> str:
        return self.config.env_var

    def __hash__(self) -> int:
        return hash(self.table)

    def mean(
        self, columns: set[str], time: TimespanSelector | None = None, limit: int | None = None
    ) -> Enricher:
        reverse_map = {value: key for key, value in self.mapping_keys.items()}
        sql_columns = ', '.join([f'AVG({reverse_map.get(column, column)}) AS {column}' for column in columns])

        query = f'SELECT {sql_columns} FROM {self.table}'
        if time:
            seconds = time.timespand.total_seconds()
            query += f' WHERE {time.time_column} >= NOW() - interval \'{seconds} seconds\''
        if limit and isinstance(limit, int):
            query += f' LIMIT {limit}'

        return SqlDatabaseEnricher(self.config.env_var, query)

    def std(
        self, columns: set[str], time: TimespanSelector | None = None, limit: int | None = None
    ) -> Enricher:
        reverse_map = {value: key for key, value in self.mapping_keys.items()}
        sql_columns = ', '.join(
            [f'STDDEV({reverse_map.get(column, column)}) AS {column}' for column in columns]
        )

        query = f'SELECT {sql_columns} FROM {self.table}'
        if time:
            seconds = time.timespand.total_seconds()
            query += f' WHERE {time.time_column} >= NOW() - interval \'{seconds} seconds\''
        if limit and isinstance(limit, int):
            query += f' LIMIT {limit}'

        return SqlDatabaseEnricher(self.config.env_var, query)

    def all_data(self, request: RetrivalRequest, limit: int | None) -> FullExtractJob:
        from aligned.psql.jobs import FullExtractPsqlJob

        return FullExtractPsqlJob(self, request, limit)

    def all_between_dates(
        self,
        request: RetrivalRequest,
        start_date: datetime,
        end_date: datetime,
    ) -> DateRangeJob:
        from aligned.psql.jobs import DateRangePsqlJob

        return DateRangePsqlJob(self, start_date, end_date, request)

    @classmethod
    def multi_source_features_for(
        cls, facts: RetrivalJob, requests: list[tuple[PostgreSQLDataSource, RetrivalRequest]]
    ) -> FactualRetrivalJob:
        # Group based on config
        from aligned.psql.jobs import FactPsqlJob

        return FactPsqlJob(
            sources={request.location: source for source, request in requests},
            requests=[request for _, request in requests],
            facts=facts,
        )"
378;aligned/sources/redis.py;beta;"class Redis:  # type: ignore
        pass

    StrictRedis = Redis
    ConnectionPool = Redis

from aligned.data_source.stream_data_source import SinkableDataSource, StreamDataSource
from aligned.feature_source import FeatureSource, FeatureSourceFactory, WritableFeatureSource
from aligned.request.retrival_request import FeatureRequest, RetrivalRequest
from aligned.retrival_job import RetrivalJob
from aligned.schemas.codable import Codable
from aligned.schemas.feature import Feature, FeatureType
from aligned.schemas.vector_storage import VectorIndex, VectorStorage

logger = logging.getLogger(__name__)


redis_manager: dict[str, ConnectionPool] = {}"
379;aligned/sources/redis.py;beta;"class RedisConfig(Codable, FeatureSourceFactory):

    env_var: str

    @property
    def url(self) -> str:
        import os

        return os.environ[self.env_var]

    @staticmethod
    def from_url(url: str) -> RedisConfig:
        import os

        os.environ['REDIS_URL'] = url
        return RedisConfig(env_var='REDIS_URL')

    @staticmethod
    def localhost() -> RedisConfig:
        import os

        if 'REDIS_URL' not in os.environ:
            os.environ['REDIS_URL'] = 'redis://localhost:6379'

        return RedisConfig(env_var='REDIS_URL')

    def feature_source(self) -> FeatureSource:
        return RedisSource(self)

    def redis(self) -> Redis:
        if self.env_var not in redis_manager:
            redis_manager[self.env_var] = ConnectionPool.from_url(self.url, decode_responses=True)

        return StrictRedis(connection_pool=redis_manager[self.env_var])

    def stream(self, topic: str) -> RedisStreamSource:
        return RedisStreamSource(topic_name=topic, config=self)

    def index(
        self,
        name: str,
        initial_cap: int | None = None,
        distance_metric: str | None = None,
        algorithm: str | None = None,
        embedding_type: str | None = None,
    ) -> RedisVectorIndex:
        return RedisVectorIndex(
            config=self,
            name=name,
            initial_cap=initial_cap or 10000,
            distance_metric=distance_metric or 'COSINE',
            index_alogrithm=algorithm or 'FLAT',
            embedding_type=embedding_type or 'FLOAT32',
        )"
380;aligned/sources/redis.py;beta;"class RedisVectorIndex(VectorStorage):

    config: RedisConfig

    name: str
    initial_cap: int
    distance_metric: str
    index_alogrithm: str
    embedding_type: str

    type_name: str = 'redis'

    async def create_index(self, index: VectorIndex) -> None:
        from redis.commands.search.indexDefinition import IndexDefinition, IndexType  # type: ignore

        redis = self.config.redis()
        try:
            info = await redis.ft(self.name).info()
            logger.info(f'Index {self.name} already exists: {info}')
        except Exception:
            logger.info(f'Creating index {self.name} with prefix {index.location.identifier}...')
            index_definition = IndexDefinition(prefix=[index.location.identifier], index_type=IndexType.HASH)
            fields = [
                self.index_schema(feature, index) for feature in set(index.metadata).union({index.vector})
            ]
            await redis.ft(self.name).create_index(
                fields=fields,
                definition=index_definition,
            )

    def index_schema(self, feature: Feature, index: VectorIndex) -> str:
        from redis.commands.search.field import NumericField, TagField, TextField, VectorField  # type: ignore

        if feature.dtype.is_numeric:
            return NumericField(name=feature.name)

        match feature.dtype.name:
            case 'string':
                return TextField(name=feature.name)
            case 'uuid':
                return TagField(name=feature.name)
            case 'embedding':
                if feature.name != index.vector.name:
                    raise ValueError(
                        f'The metadata can not contain a feature embedding. like: {feature.name}'
                    )

                return VectorField(
                    name=feature.name,
                    algorithm=self.index_alogrithm,
                    attributes={
                        'TYPE': self.embedding_type,
                        'DIM': index.vector_dim,
                        'DISTANCE_METRIC': self.distance_metric,
                        'INITIAL_CAP': self.initial_cap,
                    },
                )
        raise ValueError(f'Unsupported feature type {feature.dtype.name}')

    async def upsert_polars(self, df: pl.LazyFrame, index: VectorIndex) -> None:
        logger.info(f'Upserting {len(df.columns)} into index {self.name}...')"
381;aligned/sources/redis.py;beta;"class RedisSource(FeatureSource, WritableFeatureSource):

    config: RedisConfig
    batch_size = 1_000_000

    def all_for(self, request: FeatureRequest, limit: int | None = None) -> RetrivalJob:
        raise NotImplementedError()

    def features_for(self, facts: RetrivalJob, request: FeatureRequest) -> RetrivalJob:
        from aligned.redis.job import FactualRedisJob

        return FactualRedisJob(self.config, request.needed_requests, facts)

    async def write(self, job: RetrivalJob, requests: list[RetrivalRequest]) -> None:

        redis = self.config.redis()
        data = await job.to_polars()

        async with redis.pipeline(transaction=True) as pipe:

            for request in requests:
                # Run one query per row
                filter_entity_query: pl.Expr = pl.lit(True)
                for entity_name in request.entity_names:
                    filter_entity_query = filter_entity_query & (pl.col(entity_name).is_not_null())

                request_data = data.filter(filter_entity_query).with_column(
                    (
                        pl.lit(request.location.identifier)
                        + pl.lit(':')
                        + pl.concat_str(sorted(request.entity_names))
                    ).alias('id')
                )

                features = ['id']

                for feature in request.returned_features:

                    expr = pl.col(feature.name).cast(pl.Utf8).alias(feature.name)

                    if feature.dtype == FeatureType('').bool:
                        # Redis do not support bools
                        expr = (
                            pl.col(feature.name).cast(pl.Int8, strict=False).cast(pl.Utf8).alias(feature.name)
                        )
                    elif feature.dtype == FeatureType('').datetime:
                        expr = pl.col(feature.name).dt.timestamp('ms').cast(pl.Utf8).alias(feature.name)
                    elif feature.dtype == FeatureType('').embedding or feature.dtype == FeatureType('').array:
                        expr = pl.col(feature.name).apply(lambda x: x.to_numpy().tobytes())

                    request_data = request_data.with_column(expr)
                    features.append(feature.name)

                redis_frame = request_data.select(features).collect()

                for record in redis_frame.to_dicts():
                    pipe.hset(record['id'], mapping={key: value for key, value in record.items() if value})
                    for key, value in record.items():
                        if value is None:
                            logger.info(f""Deleting {key} from {record['id']}"")
                            pipe.hdel(record['id'], key)
                await pipe.execute()"
382;aligned/sources/redis.py;beta;"class RedisStreamSource(StreamDataSource, SinkableDataSource):

    topic_name: str
    config: RedisConfig

    mappings: dict[str, str] = field(default_factory=dict)

    name: str = 'redis'

    def map_values(self, mappings: dict[str, str]) -> RedisStreamSource:
        return RedisStreamSource(
            topic_name=self.topic_name, config=self.config, mappings=self.mappings | mappings
        )

    def make_redis_friendly(self, data: pl.LazyFrame, features: set[Feature]) -> pl.LazyFrame:
        # Run one query per row
        for feature in features:

            expr = pl.col(feature.name)

            if feature.dtype == FeatureType('').bool:
                # Redis do not support bools
                expr = pl.col(feature.name).cast(pl.Int8, strict=False)
            elif feature.dtype == FeatureType('').datetime:
                expr = pl.col(feature.name).dt.timestamp('ms')
            elif feature.dtype == FeatureType('').embedding or feature.dtype == FeatureType('').array:

                expr = pl.col(feature.name).apply(lambda x: x.to_numpy().tobytes())

            data = data.with_column(expr.cast(pl.Utf8).alias(feature.name))

        return data

    async def write_to_stream(self, job: RetrivalJob) -> None:
        redis = self.config.redis()
        df = await job.to_polars()

        df = self.make_redis_friendly(df, job.request_result.features.union(job.request_result.entities))
        values = df.collect()

        async with redis.pipeline(transaction=True) as pipe:
            _ = [
                pipe.xadd(
                    self.topic_name,
                    {key: value for key, value in record.items() if value},
                )
                for record in values.to_dicts()
            ]
            await pipe.execute()"
383;aligned/sources/redshift.py;beta;"class RedshiftSQLConfig(Codable):
    env_var: str
    schema: str | None = None

    @property
    def url(self) -> str:
        import os

        return os.environ[self.env_var]

    @property
    def psql_config(self) -> PostgreSQLConfig:

        return PostgreSQLConfig(self.env_var, self.schema)

    @staticmethod
    def from_url(url: str) -> RedshiftSQLConfig:
        import os

        os.environ['REDSHIFT_DATABASE'] = url.replace('redshift:', 'postgresql:')
        return RedshiftSQLConfig(env_var='REDSHIFT_DATABASE')

    def table(self, table: str, mapping_keys: dict[str, str] | None = None) -> RedshiftSQLDataSource:
        return RedshiftSQLDataSource(config=self, table=table, mapping_keys=mapping_keys or {})

    def data_enricher(
        self, name: str, sql: str, redis: RedisConfig, values: dict | None = None, lock_timeout: int = 60
    ) -> Enricher:
        from pathlib import Path

        from aligned.enricher import FileCacheEnricher, RedisLockEnricher, SqlDatabaseEnricher

        return FileCacheEnricher(
            timedelta(days=1),
            file=Path(f'./cache/{name}.parquet'),
            enricher=RedisLockEnricher(
                name, SqlDatabaseEnricher(self.url, sql, values), redis, timeout=lock_timeout
            ),
        )

    def entity_source(self, timestamp_column: str, sql: Callable[[str], str]) -> EntityDataSource:
        return SqlEntityDataSource(sql, self.url, timestamp_column)"
384;aligned/sources/redshift.py;beta;"class RedshiftSQLDataSource(BatchDataSource, ColumnFeatureMappable):

    config: RedshiftSQLConfig
    table: str
    mapping_keys: dict[str, str]

    type_name = 'redshift'

    def to_psql_source(self) -> PostgreSQLDataSource:
        return PostgreSQLDataSource(self.config.psql_config, self.table, self.mapping_keys)

    def job_group_key(self) -> str:
        return self.config.env_var

    def __hash__(self) -> int:
        return hash(self.table)

    def all_data(self, request: RetrivalRequest, limit: int | None) -> FullExtractJob:
        from aligned.psql.jobs import FullExtractPsqlJob

        return FullExtractPsqlJob(self, request, limit)

    def all_between_dates(
        self, request: RetrivalRequest, start_date: datetime, end_date: datetime
    ) -> DateRangeJob:
        from aligned.psql.jobs import DateRangePsqlJob, PostgreSQLDataSource

        source = PostgreSQLDataSource(self.config.psql_config, self.table, self.mapping_keys)

        return DateRangePsqlJob(source, start_date, end_date, request)

    @classmethod
    def multi_source_features_for(
        cls: type[RedshiftSQLDataSource],
        facts: RetrivalJob,
        requests: list[tuple[RedshiftSQLDataSource, RetrivalRequest]],
    ) -> RetrivalJob:
        from aligned.redshift.jobs import FactRedshiftJob

        return FactRedshiftJob(
            sources={request.location: source.to_psql_source() for source, request in requests},
            requests=[request for _, request in requests],
            facts=facts,
        )"
385;aligned/sources/s3.py;beta;"class S3Config:  # type: ignore[no-redef]
        pass"
386;aligned/sources/s3.py;beta;"class AwsS3Config(Codable):

    access_token_env: str
    secret_token_env: str
    bucket_env: str
    region_env: str

    @property
    def s3_config(self) -> S3Config:
        import os

        return S3Config(
            aws_access_key=os.environ[self.access_token_env],
            aws_secret_key=os.environ[self.secret_token_env],
            aws_region=os.environ[self.region_env],
            aws_s3_bucket=os.environ[self.bucket_env],
        )

    @property
    def url(self) -> str:
        import os

        region = os.environ[self.bucket_env]
        bucket = os.environ[self.bucket_env]
        return f'https://{region}.amazoneaws.com/{bucket}/'

    def json_at(self, path: str, mapping_keys: dict[str, str] | None = None) -> 'AwsS3DataSource':
        return AwsS3DataSource(config=self, path=path)

    def csv_at(
        self, path: str, mapping_keys: dict[str, str] | None = None, csv_config: CsvConfig | None = None
    ) -> 'AwsS3CsvDataSource':
        return AwsS3CsvDataSource(
            config=self, path=path, mapping_keys=mapping_keys or {}, csv_config=csv_config or CsvConfig()
        )

    def parquet_at(
        self, path: str, mapping_keys: dict[str, str] | None = None, config: ParquetConfig | None = None
    ) -> 'AwsS3ParquetDataSource':
        return AwsS3ParquetDataSource(
            config=self, path=path, mapping_keys=mapping_keys or {}, parquet_config=config or ParquetConfig()
        )

    @property
    def storage(self) -> Storage:
        return AwsS3Storage(self)"
387;aligned/sources/s3.py;beta;"class AwsS3DataSource(StorageFileReference, ColumnFeatureMappable):

    config: AwsS3Config
    path: str

    type_name: str = 'aws_s3'

    def job_group_key(self) -> str:
        return f'{self.type_name}/{self.path}'

    @property
    def storage(self) -> Storage:
        return self.config.storage

    @property
    def url(self) -> str:
        return f'{self.config.url}{self.path}'

    async def read(self) -> bytes:
        return await self.storage.read(self.path)

    async def write(self, content: bytes) -> None:
        return await self.storage.write(self.path, content)"
388;aligned/sources/s3.py;beta;"class AwsS3CsvDataSource(BatchDataSource, DataFileReference, ColumnFeatureMappable):

    config: AwsS3Config
    path: str
    mapping_keys: dict[str, str]
    csv_config: CsvConfig

    type_name: str = 'aws_s3_csv'

    def job_group_key(self) -> str:
        return f'{self.type_name}/{self.path}'

    @property
    def storage(self) -> Storage:
        return self.config.storage

    @property
    def url(self) -> str:
        return f'{self.config.url}{self.path}'

    async def read_pandas(self) -> pd.DataFrame:
        try:
            data = await self.storage.read(self.path)
            buffer = BytesIO(data)
            return pd.read_csv(buffer, sep=self.csv_config.seperator, compression=self.csv_config.compression)
        except FileNotFoundError:
            raise UnableToFindFileException()
        except HTTPStatusError:
            raise UnableToFindFileException()

    async def write_pandas(self, df: pd.DataFrame) -> None:
        buffer = BytesIO()
        df.to_csv(
            buffer,
            sep=self.csv_config.seperator,
            index=self.csv_config.should_write_index,
            compression=self.csv_config.compression,
        )
        buffer.seek(0)
        await self.storage.write(self.path, buffer.read())

    async def write_polars(self, df: pl.LazyFrame) -> None:
        buffer = BytesIO()
        df.collect().write_csv(
            buffer,
            sep=self.csv_config.seperator,
        )
        buffer.seek(0)
        await self.storage.write(self.path, buffer.read())

    def all_data(self, request: RetrivalRequest, limit: int | None) -> FullExtractJob:
        return FileFullJob(self, request=request, limit=limit)

    def all_between_dates(
        self, request: RetrivalRequest, start_date: datetime, end_date: datetime
    ) -> DateRangeJob:
        return FileDateJob(self, request, start_date, end_date)"
389;aligned/sources/s3.py;beta;"class AwsS3ParquetDataSource(BatchDataSource, DataFileReference, ColumnFeatureMappable):

    config: AwsS3Config
    path: str
    mapping_keys: dict[str, str]

    parquet_config: ParquetConfig
    type_name: str = 'aws_s3_parquet'

    def job_group_key(self) -> str:
        return f'{self.type_name}/{self.path}'

    @property
    def storage(self) -> Storage:
        return self.config.storage

    @property
    def url(self) -> str:
        return f'{self.config.url}{self.path}'

    async def read_pandas(self) -> pd.DataFrame:
        try:
            data = await self.storage.read(self.path)
            buffer = BytesIO(data)
            return pd.read_parquet(buffer)
        except FileNotFoundError:
            raise UnableToFindFileException()
        except HTTPStatusError:
            raise UnableToFindFileException()

    async def to_polars(self) -> pl.LazyFrame:
        try:
            data = await self.storage.read(self.path)
            buffer = BytesIO(data)
            return pl.read_parquet(buffer).lazy()
        except FileNotFoundError:
            raise UnableToFindFileException()
        except HTTPStatusError:
            raise UnableToFindFileException()

    async def write_pandas(self, df: pd.DataFrame) -> None:
        buffer = BytesIO()
        df.to_parquet(buffer, compression=self.parquet_config.compression, engine=self.parquet_config.engine)
        buffer.seek(0)
        await self.storage.write(self.path, buffer.read())

    async def write_polars(self, df: pl.LazyFrame) -> None:
        buffer = BytesIO()
        df.collect().write_parquet(buffer, compression=self.parquet_config.compression)
        buffer.seek(0)
        await self.storage.write(self.path, buffer.read())"
390;aligned/split_strategy.py;beta;"class TrainTestSet(Generic[DatasetType]):

    data: DatasetType

    entity_columns: set[str]
    features: set[str]
    target_columns: set[str]

    train_index: Index
    test_index: Index
    event_timestamp_column: str | None

    def __init__(
        self,
        data: DatasetType,
        entity_columns: set[str],
        features: set[str],
        target_columns: set[str],
        train_index: Index,
        test_index: Index,
        event_timestamp_column: str | None,
    ):
        if isinstance(data, pl.LazyFrame):
            raise ValueError('The dataframe need to be a DataFrame, not a LazyFrame when using Polars.')
        self.data = data
        self.entity_columns = entity_columns
        self.features = features
        self.target_columns = target_columns
        self.train_index = train_index
        self.test_index = test_index
        self.event_timestamp_column = event_timestamp_column

    @property
    def sorted_features(self) -> list[str]:
        return sorted(self.features)

    @property
    def train(self) -> DatasetType:
        if isinstance(self.data, pl.DataFrame):
            return self.data[self.train_index.to_list(), :]
        return self.data.iloc[self.train_index]

    @property
    def train_input(self) -> DatasetType:
        return self.train[self.sorted_features]

    @property
    def train_target(self) -> DatasetType:
        return self.train[list(self.target_columns)]

    @property
    def test(self) -> DatasetType:
        if isinstance(self.data, pl.DataFrame):
            return self.data[self.test_index.to_list(), :]
        return self.data.iloc[self.test_index]

    @property
    def test_input(self) -> DatasetType:
        return self.test[self.sorted_features]

    @property
    def test_target(self) -> DatasetType:
        return self.test[list(self.target_columns)]"
391;aligned/split_strategy.py;beta;"class SupervisedDataSet(Generic[DatasetType]):
    data: DatasetType

    entity_columns: set[str]
    features: set[str]
    target_columns: set[str]
    event_timestamp_column: str | None

    @property
    def sorted_features(self) -> list[str]:
        return sorted(self.features)

    def __init__(
        self,
        data: DatasetType,
        entity_columns: set[str],
        features: set[str],
        target: set[str],
        event_timestamp_column: str | None,
    ):
        self.data = data
        self.entity_columns = entity_columns
        self.features = features
        self.target_columns = target
        self.event_timestamp_column = event_timestamp_column

    @property
    def entities(self) -> DatasetType:
        if isinstance(self.data, (pl.LazyFrame, pl.DataFrame)):
            return self.data.select(list(self.entity_columns))
        return self.data[list(self.entity_columns)]

    @property
    def input(self) -> DatasetType:
        if isinstance(self.data, (pl.LazyFrame, pl.DataFrame)):
            return self.data.select(self.sorted_features)
        return self.data[self.sorted_features]

    @property
    def target(self) -> DatasetType:
        if isinstance(self.data, (pl.LazyFrame, pl.DataFrame)):
            return self.data.select(list(self.target_columns))
        return self.data[list(self.target_columns)]"
392;aligned/split_strategy.py;beta;"class TrainTestValidateSet(Generic[DatasetType]):

    data: DatasetType

    entity_columns: set[str]
    features: set[str]
    target_columns: set[str]

    train_index: Index
    test_index: Index
    validate_index: Index
    event_timestamp_column: str | None

    def __init__(
        self,
        data: DatasetType,
        entity_columns: set[str],
        features: set[str],
        target: set[str],
        train_index: Index,
        test_index: Index,
        validate_index: Index,
        event_timestamp_column: str | None,
    ):
        self.data = data
        self.entity_columns = entity_columns
        self.features = features
        self.target_columns = target
        self.train_index = train_index
        self.test_index = test_index
        self.validate_index = validate_index
        self.event_timestamp_column = event_timestamp_column

    @property
    def sorted_features(self) -> list[str]:
        return sorted(self.features)

    @property
    def input(self) -> DatasetType:
        if isinstance(self.data, pl.LazyFrame):
            return self.data.select(self.sorted_features)
        return self.data[self.sorted_features]

    @property
    def target(self) -> DatasetType:
        if isinstance(self.data, pl.LazyFrame):
            return self.data.select(sorted(self.target_columns))
        return self.data[sorted(self.target_columns)]

    @property
    def train(self) -> SupervisedDataSet[DatasetType]:
        if isinstance(self.data, pl.DataFrame):
            data = self.data[self.train_index.to_list(), :]
        else:
            data = self.data.iloc[self.train_index]

        return SupervisedDataSet(
            data,
            self.entity_columns,
            self.features,
            self.target_columns,
            self.event_timestamp_column,
        )

    @property
    def train_input(self) -> DatasetType:
        return self.train.input

    @property
    def train_target(self) -> DatasetType:
        return self.train.target

    @property
    def test(self) -> SupervisedDataSet[DatasetType]:

        if isinstance(self.data, pl.DataFrame):
            data = self.data[self.test_index.to_list(), :]
        else:
            data = self.data.iloc[self.test_index]

        return SupervisedDataSet(
            data,
            set(self.entity_columns),
            set(self.features),
            self.target_columns,
            self.event_timestamp_column,
        )

    @property
    def test_input(self) -> DatasetType:
        return self.test.input

    @property
    def test_target(self) -> DatasetType:
        return self.test.target

    @property
    def validate(self) -> SupervisedDataSet[DatasetType]:
        if isinstance(self.data, pl.DataFrame):
            data = self.data[self.validate_index.to_list(), :]
        else:
            data = self.data.iloc[self.validate_index]
        return SupervisedDataSet(
            data,
            self.entity_columns,
            set(self.features),
            self.target_columns,
            self.event_timestamp_column,
        )

    @property
    def validate_input(self) -> DatasetType:
        return self.validate.input

    @property
    def validate_target(self) -> DatasetType:
        return self.validate.target"
393;aligned/split_strategy.py;beta;"class SplitDataSet(Generic[DatasetType]):

    train_input: DatasetType
    train_target: DatasetType

    develop_input: DatasetType
    develop_target: DatasetType

    test_input: DatasetType
    test_target: DatasetType

    def __init__(
        self,
        train_input: DatasetType,
        train_target: DatasetType,
        develop_input: DatasetType,
        develop_target: DatasetType,
        test_input: DatasetType,
        test_target: DatasetType,
    ):
        self.train_input = train_input
        self.train_target = train_target
        self.develop_input = develop_input
        self.develop_target = develop_target
        self.test_input = test_input
        self.test_target = test_target"
394;aligned/split_strategy.py;beta;"class SplitStrategy:
    def split_pandas(self, data: DataFrame, target_column: str) -> SplitDataSet[DataFrame]:
        pass"
395;aligned/split_strategy.py;beta;"class StrategicSplitStrategy(SplitStrategy):

    train_size_percentage: float
    test_size_percentage: float

    def __init__(self, train_size_percentage: float, test_size_percentage: float):
        assert train_size_percentage + test_size_percentage <= 1
        self.train_size_percentage = train_size_percentage
        self.test_size_percentage = test_size_percentage

    def split_pandas(self, data: DataFrame, target_column: str) -> SplitDataSet[DataFrame]:
        train = DataFrame(columns=data.columns)
        test = DataFrame(columns=data.columns)
        develop = DataFrame(columns=data.columns)

        target_data = data[target_column]

        def split(data: DataFrame, start_ratio: float, end_ratio: float) -> DataFrame:
            group_size = data.shape[0]
            start_index = math.floor(group_size * start_ratio)
            end_index = math.floor(group_size * end_ratio)
            return data.iloc[start_index:end_index]

        for target in target_data.unique():
            sub_group = data.loc[target_data == target]

            train = concat([train, split(sub_group, 0, self.train_size_percentage)], axis=0)
            test = concat(
                [
                    test,
                    split(
                        sub_group,
                        self.train_size_percentage,
                        self.train_size_percentage + self.test_size_percentage,
                    ),
                ],
                axis=0,
            )
            develop = concat(
                [develop, split(sub_group, self.train_size_percentage + self.test_size_percentage, 1)], axis=0
            )

        return SplitDataSet(
            train_input=train.drop(columns=[target_column]),
            train_target=train[target_column],
            develop_input=develop.drop(columns=[target_column]),
            develop_target=develop[target_column],
            test_input=test.drop(columns=[target_column]),
            test_target=test[target_column],
        )"
396;aligned/storage.py;beta;"class Storage:
    async def read(self, path: str) -> bytes:
        raise NotImplementedError()

    async def write(self, path: str, content: bytes) -> None:
        raise NotImplementedError()"
397;aligned/tests/test_models_as_feature.py;beta;"class View(FeatureView):

    metadata = FeatureView.metadata_with('view', 'test', FileSource.csv_at(''))

    view_id = Int32().as_entity()

    feature_a = String()"
398;aligned/tests/test_models_as_feature.py;beta;"class OtherView(FeatureView):

    metadata = FeatureView.metadata_with('other', 'test', FileSource.csv_at(''))

    other_id = Int32().as_entity()

    feature_b = Int32()
    is_true = Bool()"
399;aligned/tests/test_models_as_feature.py;beta;"class First(Model):

    view = View()
    other = OtherView()

    metadata = Model.metadata_with('test_model', '', features=[view.feature_a, other.feature_b])

    target = other.is_true.as_classification_target()"
400;aligned/tests/test_models_as_feature.py;beta;"class Second(Model):

    first = First()

    metadata = Model.metadata_with('second_model', '', features=[first.target])


def test_model_referenced_as_feature() -> None:
    model = Second.compile()

    feature = list(model.features)[0]

    assert feature.location == FeatureLocation.model('test_model')
    assert feature.name == 'target'
    assert len(model.predictions_view.entities) == 2


def test_model_request() -> None:
    store = FeatureStore.experimental()
    store.add_feature_view(View())
    store.add_feature_view(OtherView())
    store.add_model(First())

    assert len(store.feature_views) == 2

    model_request = store.model('test_model').request
    assert model_request.features_to_include == {'feature_a', 'feature_b', 'view_id', 'other_id'}"
401;aligned/validation/interface.py;beta;"class Validator:
    async def validate_pandas(self, features: list[Feature], df: pd.DataFrame) -> pd.DataFrame:
        pass

    async def validate_polars(self, features: list[Feature], df: pl.LazyFrame) -> pl.LazyFrame:
        pass"
402;aligned/validation/pandera.py;beta;"class PanderaValidator(Validator):

    check_map: dict[str, Callable[[Constraint], Check]] = {
        'lower_bound': lambda constraint: Check.greater_than(constraint.value),
        'lower_bound_inc': lambda constraint: Check.greater_than_or_equal_to(constraint.value),
        'upper_bound': lambda constraint: Check.less_than(constraint.value),
        'upper_bound_inc': lambda constraint: Check.less_than_or_equal_to(constraint.value),
        'in_domain': lambda domain: Check.isin(domain.values),
        'min_length': lambda constraint: Check.str_length(min_value=constraint.value),
        'max_length': lambda constraint: Check.str_length(max_value=constraint.value),
        'regex': lambda constraint: Check.str_matches(constraint.value),
        'ends_with': lambda constraint: Check.str_endswith(constraint.value),
        'starts_with': lambda constraint: Check.str_startswith(constraint.value),
    }

    datatype_check = {
        FeatureType('').bool,
        FeatureType('').string,
        FeatureType('').uuid,
        FeatureType('').date,
    }

    def _column_for(self, feature: Feature) -> Column:
        Check.str_matches

        if feature.constraints is None:
            return Column(
                feature.dtype.pandas_type if feature.dtype in self.datatype_check else None, nullable=True
            )

        is_nullable = Required() not in feature.constraints

        checks = [
            self.check_map[constraint.name](constraint)
            for constraint in feature.constraints
            if constraint.name in self.check_map
        ]

        return Column(
            dtype=feature.dtype.pandas_type if feature.dtype in self.datatype_check else None,
            checks=checks,
            nullable=is_nullable,
            required=not is_nullable,
        )

    def _build_schema(self, features: list[Feature]) -> DataFrameSchema:
        return DataFrameSchema(columns={feature.name: self._column_for(feature) for feature in features})

    async def validate_pandas(self, features: list[Feature], df: pd.DataFrame) -> pd.DataFrame:
        from pandera.errors import SchemaError

        schema = self._build_schema(features)
        try:
            return schema.validate(df)
        except SchemaError as error:
            # Will only return one error at a time, so will remove
            # errors and then run it recrusive

            if error.failure_cases.shape[0] == df.shape[0]:
                raise ValueError('Validation is removing all the data.')

            if error.failure_cases['index'].iloc[0] is None:
                raise ValueError(error)

            return await self.validate_pandas(
                features, df.loc[df.index.delete(error.failure_cases['index'])].reset_index()
            )

    async def validate_polars(self, features: list[Feature], df: pl.LazyFrame) -> pl.LazyFrame:
        input_df = df.collect().to_pandas()
        return pl.from_pandas(await self.validate_pandas(features, input_df)).lazy()"
403;aligned/worker.py;beta;"class ActiveLearningConfig:
    metric: ActiveLearningMetric
    selection: ActiveLearningSelection
    write_policy: ActiveLearningWritePolicy
    model_names: list[str] | None = None"
404;aligned/worker.py;beta;"class StreamWorker:

    feature_store_reference: StorageFileReference
    sink_source: WritableFeatureSource
    views_to_process: set[str]
    should_prune_unused_features: bool = field(default=False)
    active_learning_configs: list[ActiveLearningConfig] = field(default_factory=list)
    metric_logging_port: int | None = field(default=None)

    @staticmethod
    def from_reference(
        source: StorageFileReference,
        sink_source: WritableFeatureSource,
        views_to_process: set[str] | None = None,
    ) -> StreamWorker:
        """"""
        Creates a stream worker.

        This object can start a background worker process of streaming data.

        Args:
            source (StorageFileReference): The storage of the feature store file
            sink_source (WritableFeatureSource): Where to store the processed features
            views_to_process (set[str] | None, optional): The views to process.
                Defaults to None aka, all streaming views.

        Returns:
            StreamWorker | None: A worker that can start processing
        """"""

        return StreamWorker(source, sink_source, views_to_process)

    @staticmethod
    def from_object(repo: Path, file: Path, obj: str) -> StreamWorker:
        from aligned.compiler.repo_reader import import_module, path_to_py_module
        from aligned.exceptions import StreamWorkerNotFound

        module_path = path_to_py_module(file, repo)

        try:
            module = import_module(module_path)

            worker = getattr(module, obj)
            if isinstance(worker, StreamWorker):
                return worker
            raise ValueError('No reference found')
        except AttributeError:
            raise ValueError('No reference found')
        except ModuleNotFoundError:
            raise StreamWorkerNotFound(module_path)

    def generate_active_learning_dataset(
        self,
        metric: ActiveLearningMetric | None = None,
        selection: ActiveLearningSelection | None = None,
        write_policy: ActiveLearningWritePolicy | None = None,
        model_names: list[str] | None = None,
    ) -> StreamWorker:
        self.active_learning_configs.append(
            ActiveLearningConfig(
                metric=metric or ActiveLearningMetric.max_probability(),
                selection=selection or ActiveLearningSelection.under_threshold(0.5),
                write_policy=write_policy or ActiveLearningWritePolicy.sample_size(10, 1000),
                model_names=model_names,
            )
        )
        return self

    def metrics_port(self, port: int) -> StreamWorker:
        self.metric_logging_port = port
        return self

    async def start(self, should_prune_unused_features: bool) -> None:
        from prometheus_client import start_http_server

        from aligned.data_source.stream_data_source import HttpStreamSource

        store = await self.feature_store_reference.feature_store()
        store = store.with_source(self.sink_source)

        views = self.views_to_process or set()
        if not self.views_to_process:
            views = [
                view.name
                for view in store.feature_views.values()
                if view.stream_data_source is not None
                and not isinstance(view.stream_data_source, HttpStreamSource)
            ]
        if not views:
            raise ValueError('No feature views with streaming source to process')

        feature_views_to_process = views

        streams: list[StreamDataSource] = []
        feature_views: dict[str, list[FeatureViewStore]] = {}

        for view in store.feature_views.values():
            if view.name not in feature_views_to_process:
                continue

            if not view.stream_data_source:
                continue
            source = view.stream_data_source

            if not streams:
                streams.append(source)
                feature_views[source.topic_name] = [store.feature_view(view.name)]
            elif isinstance(source, type(streams[0])):
                streams.append(source)
                if source.topic_name in feature_views:
                    feature_views[source.topic_name] = feature_views[source.topic_name] + [
                        store.feature_view(view.name)
                    ]
                else:
                    feature_views[source.topic_name] = [store.feature_view(view.name)]
            else:
                raise ValueError(
                    'The grouped stream sources is not of the same type.'
                    f'{view.name} was as {type(source)} expected {streams[0]}'
                )

        if not isinstance(streams[0], RedisStreamSource):
            raise ValueError(
                'Only supporting Redis Streams for worker nodes as of now. '
                f'Please contribute to the repo in order to support {type(streams[0])} on a worker node'
            )

        redis_streams: list[RedisStreamSource] = streams
        for stream in redis_streams:
            if not stream.config == redis_streams[0].config:
                raise ValueError(f'Not all stream configs for {feature_views_to_process}, is equal.')

        redis_stream = RedisStream(redis_streams[0].config.redis())
        processes = []
        for topic_name, views in feature_views.items():
            process_views = views
            if should_prune_unused_features:
                process_views = [view.with_optimised_write() for view in process_views]
            processes.append(process(redis_stream, topic_name, process_views))

        for active_learning_config in self.active_learning_configs:
            for model_name in set(active_learning_config.model_names):
                processes.append(
                    process_predictions(redis_stream, store.model(model_name), active_learning_config)
                )

        if self.metric_logging_port:
            start_http_server(self.metric_logging_port)

        await asyncio.gather(*processes)


async def process_predictions(
    stream_source: RedisStream, model: ModelFeatureStore, active_learning_config: ActiveLearningConfig | None
) -> None:
    from aligned.active_learning.job import ActiveLearningJob

    if not active_learning_config:
        logger.info('No active learning config found, will not listen to predictions')
        return

    topic_name = model.model.predictions_view.stream_source.topic_name
    last_id = '$'
    logger.info(f'Started listning to {topic_name}')

    while True:
        stream_values = await stream_source.read_from_timestamp({topic_name: last_id})

        if not stream_values:
            continue
        start_time = timeit.default_timer()
        _, values = stream_values[0]
        last_id = values[-1][0]
        records = [value for _, value in values]

        request = model.model.request_all_predictions.needed_requests[0]
        job = RetrivalJob.from_dict(records, request).ensure_types([request])
        job = ActiveLearningJob(
            job,
            model.model,
            active_learning_config.metric,
            active_learning_config.selection,
            active_learning_config.write_policy,
        )
        _ = await job.to_polars()

        logger.info(f'Processing {len(records)} predictions in {timeit.default_timer() - start_time} seconds')


async def single_processing(
    stream_source: RedisStream, topic_name: str, feature_view: FeatureViewStore
) -> None:
    last_id = '$'
    logger.info(f'Started listning to {topic_name}')
    # request = feature_view.view.request_all.needed_requests[0]
    # aggregations = request.aggregate_over()
    # checkpoints = {
    #     window: FileSource.parquet_at(f'{feature_view.view.name}_agg_{window.time_window.total_seconds()}')
    #     for window in aggregations.keys()
    # }
    while True:
        stream_values = await stream_source.read_from_timestamp({topic_name: last_id})

        if not stream_values:
            continue
        start_time = timeit.default_timer()
        _, values = stream_values[0]
        last_id = values[-1][0]
        records = [record for _, record in values]

        job = stream_job(records, feature_view)

        await feature_view.batch_write(job)  # type: ignore [arg-type]
        elapsed = timeit.default_timer() - start_time

        process_time.labels(feature_view.view.name).observe(elapsed)
        processed_rows_count.labels(feature_view.view.name).inc(len(values))

        logger.info(f'Wrote {len(values)} records in {elapsed} seconds')


def stream_job(values: list[dict], feature_view: FeatureViewStore) -> RetrivalJob:
    from aligned import FileSource

    request = feature_view.request
    job = (
        RetrivalJob.from_dict(values, request)
        .validate_entites()
        .fill_missing_columns()
        .ensure_types([request])
    )

    aggregations = request.aggregate_over()
    if not aggregations:
        return job

    checkpoints = {}

    for aggregation in aggregations.keys():
        name = f'{feature_view.view.name}_agg'
        if aggregation.window:
            time_window = aggregation.window
            name += f'_{time_window.time_window.total_seconds()}'
        checkpoints[aggregation] = FileSource.parquet_at(name)

    return StreamAggregationJob(job, checkpoints)


async def monitor_process(values: list[dict], view: FeatureViewStore):
    start_time = timeit.default_timer()
    await view.batch_write(stream_job(values, view))
    elapsed = timeit.default_timer() - start_time
    process_time.labels(view.view.name).observe(elapsed)
    processed_rows_count.labels(view.view.name).inc(len(values))


async def multi_processing(
    stream_source: RedisStream, topic_name: str, feature_views: list[FeatureViewStore]
) -> None:
    last_id = '$'
    logger.info(f'Started listning to {topic_name}')
    while True:
        stream_values = await stream_source.read_from_timestamp({topic_name: last_id})

        if not stream_values:
            continue

        _, values = stream_values[0]
        last_id = values[-1][0]
        mapped_values = [record for _, record in values]

        await asyncio.gather(*[monitor_process(mapped_values, view) for view in feature_views])


async def process(stream_source: RedisStream, topic_name: str, feature_views: list[FeatureViewStore]) -> None:
    if len(feature_views) == 1:
        await single_processing(stream_source, topic_name, feature_views[0])
    else:
        await multi_processing(stream_source, topic_name, feature_views)"
